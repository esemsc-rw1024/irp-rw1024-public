{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "extract_sustainability_kpi.py\n",
    "==================================\n",
    "Automatically extract KPI sentences/table rows from Sustainability Report PDF\n",
    "and compare with manual KPI annotations\n",
    "--------------------------------------------------\n",
    "1. pdfplumber extracts text + tables\n",
    "2. Camelot supplements complex table parsing (optional)\n",
    "3. Chunking to control tokens\n",
    "4. OpenAI ChatCompletion API call (GPT-4o / GPT-4 / GPT-3.5)\n",
    "5. Aggregate, deduplicate, and export to auto_kpi.xlsx\n",
    "6. Compare with manual_kpi.xlsx for differences\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "eDB9oRTEZoZw",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "outputId": "f273aa1b-fadc-47c5-b1ac-615372845665"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nextract_sustainability_kpi.py\\n==================================\\nAutomatically extract KPI sentences/table rows from Sustainability Report PDF\\nand compare with manual KPI annotations\\n--------------------------------------------------\\n1. pdfplumber extracts text + tables\\n2. Camelot supplements complex table parsing (optional)\\n3. Chunking to control tokens\\n4. OpenAI ChatCompletion API call (GPT-4o / GPT-4 / GPT-3.5)\\n5. Aggregate, deduplicate, and export to auto_kpi.xlsx\\n6. Compare with manual_kpi.xlsx for differences\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 1
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q openai"
   ],
   "metadata": {
    "id": "LaGnafXLZxfg"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import openai"
   ],
   "metadata": {
    "id": "3w2Bya8SZob3"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install openai python-dotenv pdfplumber tiktoken pandas\n",
    "!sudo apt-get update -y\n",
    "!sudo apt-get install -y ghostscript\n",
    "!pip install \"camelot-py[cv]\"\n",
    "!pip install PyMuPDF Pillow\n",
    "!pip install -q transformers pillow torchvision"
   ],
   "metadata": {
    "id": "JSakj9TyZodt",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b8ede858-0b74-450e-d886-f62f2c7bebdd"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.97.1)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
      "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
      "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: python-dotenv, pypdfium2, pdfminer.six, pdfplumber\n",
      "Successfully installed pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.0 python-dotenv-1.1.1\n",
      "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
      "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,772 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,471 kB]\n",
      "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,152 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,574 kB]\n",
      "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,160 kB]\n",
      "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,269 kB]\n",
      "Fetched 21.8 MB in 4s (4,922 kB/s)\n",
      "Reading package lists... Done\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  fonts-droid-fallback fonts-noto-mono fonts-urw-base35 libgs9 libgs9-common\n",
      "  libidn12 libijs-0.35 libjbig2dec0 poppler-data\n",
      "Suggested packages:\n",
      "  fonts-noto fonts-freefont-otf | fonts-freefont-ttf fonts-texgyre\n",
      "  ghostscript-x poppler-utils fonts-japanese-mincho | fonts-ipafont-mincho\n",
      "  fonts-japanese-gothic | fonts-ipafont-gothic fonts-arphic-ukai\n",
      "  fonts-arphic-uming fonts-nanum\n",
      "The following NEW packages will be installed:\n",
      "  fonts-droid-fallback fonts-noto-mono fonts-urw-base35 ghostscript libgs9\n",
      "  libgs9-common libidn12 libijs-0.35 libjbig2dec0 poppler-data\n",
      "0 upgraded, 10 newly installed, 0 to remove and 42 not upgraded.\n",
      "Need to get 16.7 MB of archives.\n",
      "After this operation, 63.0 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1build1 [1,805 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 poppler-data all 0.4.11-1 [2,171 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-mono all 20201225-1build1 [397 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-urw-base35 all 20200910-1 [6,367 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9-common all 9.55.0~dfsg1-0ubuntu5.12 [753 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libidn12 amd64 1.38-4ubuntu1 [60.0 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libijs-0.35 amd64 0.35-15build2 [16.5 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjbig2dec0 amd64 0.19-3build2 [64.7 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9 amd64 9.55.0~dfsg1-0ubuntu5.12 [5,031 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ghostscript amd64 9.55.0~dfsg1-0ubuntu5.12 [49.4 kB]\n",
      "Fetched 16.7 MB in 1s (14.6 MB/s)\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 10.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Selecting previously unselected package fonts-droid-fallback.\n",
      "(Reading database ... 126284 files and directories currently installed.)\n",
      "Preparing to unpack .../0-fonts-droid-fallback_1%3a6.0.1r16-1.1build1_all.deb ...\n",
      "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
      "Selecting previously unselected package poppler-data.\n",
      "Preparing to unpack .../1-poppler-data_0.4.11-1_all.deb ...\n",
      "Unpacking poppler-data (0.4.11-1) ...\n",
      "Selecting previously unselected package fonts-noto-mono.\n",
      "Preparing to unpack .../2-fonts-noto-mono_20201225-1build1_all.deb ...\n",
      "Unpacking fonts-noto-mono (20201225-1build1) ...\n",
      "Selecting previously unselected package fonts-urw-base35.\n",
      "Preparing to unpack .../3-fonts-urw-base35_20200910-1_all.deb ...\n",
      "Unpacking fonts-urw-base35 (20200910-1) ...\n",
      "Selecting previously unselected package libgs9-common.\n",
      "Preparing to unpack .../4-libgs9-common_9.55.0~dfsg1-0ubuntu5.12_all.deb ...\n",
      "Unpacking libgs9-common (9.55.0~dfsg1-0ubuntu5.12) ...\n",
      "Selecting previously unselected package libidn12:amd64.\n",
      "Preparing to unpack .../5-libidn12_1.38-4ubuntu1_amd64.deb ...\n",
      "Unpacking libidn12:amd64 (1.38-4ubuntu1) ...\n",
      "Selecting previously unselected package libijs-0.35:amd64.\n",
      "Preparing to unpack .../6-libijs-0.35_0.35-15build2_amd64.deb ...\n",
      "Unpacking libijs-0.35:amd64 (0.35-15build2) ...\n",
      "Selecting previously unselected package libjbig2dec0:amd64.\n",
      "Preparing to unpack .../7-libjbig2dec0_0.19-3build2_amd64.deb ...\n",
      "Unpacking libjbig2dec0:amd64 (0.19-3build2) ...\n",
      "Selecting previously unselected package libgs9:amd64.\n",
      "Preparing to unpack .../8-libgs9_9.55.0~dfsg1-0ubuntu5.12_amd64.deb ...\n",
      "Unpacking libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.12) ...\n",
      "Selecting previously unselected package ghostscript.\n",
      "Preparing to unpack .../9-ghostscript_9.55.0~dfsg1-0ubuntu5.12_amd64.deb ...\n",
      "Unpacking ghostscript (9.55.0~dfsg1-0ubuntu5.12) ...\n",
      "Setting up fonts-noto-mono (20201225-1build1) ...\n",
      "Setting up libijs-0.35:amd64 (0.35-15build2) ...\n",
      "Setting up fonts-urw-base35 (20200910-1) ...\n",
      "Setting up poppler-data (0.4.11-1) ...\n",
      "Setting up libjbig2dec0:amd64 (0.19-3build2) ...\n",
      "Setting up libidn12:amd64 (1.38-4ubuntu1) ...\n",
      "Setting up fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
      "Setting up libgs9-common (9.55.0~dfsg1-0ubuntu5.12) ...\n",
      "Setting up libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.12) ...\n",
      "Setting up ghostscript (9.55.0~dfsg1-0ubuntu5.12) ...\n",
      "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
      "\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "Collecting camelot-py[cv]\n",
      "  Downloading camelot_py-1.0.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "\u001b[33mWARNING: camelot-py 1.0.0 does not provide the extra 'cv'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (8.2.1)\n",
      "Requirement already satisfied: chardet>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (5.2.0)\n",
      "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (2.0.2)\n",
      "Requirement already satisfied: openpyxl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (3.1.5)\n",
      "Requirement already satisfied: pdfminer-six>=20240706 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (20250506)\n",
      "Collecting pypdf<4.0,>=3.17 (from camelot-py[cv])\n",
      "  Downloading pypdf-3.17.4-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: pandas>=2.2.2 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (2.2.2)\n",
      "Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (0.9.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.7.0.68 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (4.12.0.88)\n",
      "Requirement already satisfied: pypdfium2>=4 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (4.30.0)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl>=3.1.0->camelot-py[cv]) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.2->camelot-py[cv]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.2->camelot-py[cv]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.2->camelot-py[cv]) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer-six>=20240706->camelot-py[cv]) (3.4.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer-six>=20240706->camelot-py[cv]) (43.0.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[cv]) (1.17.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.2->camelot-py[cv]) (1.17.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[cv]) (2.22)\n",
      "Downloading pypdf-3.17.4-py3-none-any.whl (278 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading camelot_py-1.0.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf, camelot-py\n",
      "Successfully installed camelot-py-1.0.0 pypdf-3.17.4\n",
      "Collecting PyMuPDF\n",
      "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
      "Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.26.3\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os, re, json, time, textwrap, argparse, logging\n",
    "import pdfplumber, pandas as pd, tiktoken\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Optional, Set, Tuple\n",
    "from pathlib import Path\n",
    "from difflib import SequenceMatcher\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import concurrent.futures\n",
    "import hashlib\n",
    "import pickle\n",
    "# 在现有的导入语句后添加这些新的导入\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime"
   ],
   "metadata": {
    "id": "rNQHu6_fZofh"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ----------------------------- Configuration -----------------------------\n",
    "PDF_PATH          = \"/content/Test_Unknown_northwest-sustainability-report-2022_fbqow68f-60-74.pdf\"\n",
    "MANUAL_XLSX       = \"manual_kpi.xlsx\"   # Leave empty if not available\n",
    "EXPORT_AUTO_XLSX  = \"auto_kpi.xlsx\"\n",
    "MODEL_NAME        = \"gpt-4o\"       # Adjust based on account availability\n",
    "MAX_TOKENS_CHUNK  = 1500               # Token limit per chunk\n",
    "SLEEP_SEC         = 0.6                # Rate limiting\n",
    "ENABLE_QUALITY_VALIDATION = True       # Enable additional quality checks\n",
    "# -----------------------------------------------------------------"
   ],
   "metadata": {
    "id": "L9B6ORg1Zohn"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============ Fixed initialization part ============\n",
    "def initialize_environment():\n",
    "    \"\"\"Initialize the environment and API client\"\"\"\n",
    "    # Load environment variables\n",
    "    load_dotenv(\"ruojia_api_key.env\")\n",
    "\n",
    "    # Initialize OpenAI client\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"OPENAI_API_KEY not found in environment variables!\")\n",
    "\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    return client, enc\n",
    "\n",
    "# Initialize global variables\n",
    "client, enc = initialize_environment()"
   ],
   "metadata": {
    "id": "mi52QoSbZoja"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============ Fixed PDF text extraction ============\n",
    "def pdf_to_text_and_tables(path: str) -> str:\n",
    "    \"\"\"Extract text paragraphs and tables using pdfplumber.\"\"\"\n",
    "    all_chunks = []\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"PDF file not found: {path}\")\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(path) as pdf:\n",
    "            logging.info(f\"Processing PDF with {len(pdf.pages)} pages...\")\n",
    "\n",
    "            for page_num, page in enumerate(pdf.pages, 1):\n",
    "                try:\n",
    "                    # Extract text\n",
    "                    text = page.extract_text() or \"\"\n",
    "                    if text.strip():\n",
    "                        all_chunks.append(f\"PAGE_{page_num}_TEXT:\\n{text}\")\n",
    "\n",
    "                    # Extract tables\n",
    "                    tables = page.extract_tables()\n",
    "                    for table_num, tb in enumerate(tables):\n",
    "                        if tb and len(tb) > 0:\n",
    "                            try:\n",
    "                                # Handle table headers safely\n",
    "                                if tb[0]:\n",
    "                                    headers = tb[0]\n",
    "                                else:\n",
    "                                    headers = [f\"Col_{i}\" for i in range(len(tb[1]) if len(tb) > 1 else 1)]\n",
    "\n",
    "                                rows = tb[1:] if len(tb) > 1 else []\n",
    "\n",
    "                                if rows:\n",
    "                                    df = pd.DataFrame(rows, columns=headers)\n",
    "                                    # Clean DataFrame\n",
    "                                    df = df.dropna(how='all')  # Remove empty rows\n",
    "                                    if not df.empty:\n",
    "                                        table_txt = f\"TABLE_START_PAGE_{page_num}_{table_num}\\n\" + df.to_csv(index=False) + \"\\nTABLE_END\"\n",
    "                                        all_chunks.append(table_txt)\n",
    "                            except Exception as e:\n",
    "                                logging.warning(f\"Error processing table on page {page_num}: {e}\")\n",
    "                                continue\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error processing page {page_num}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        return \"\\n\\n\".join(all_chunks)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error opening PDF file: {e}\")\n",
    "        raise"
   ],
   "metadata": {
    "id": "IkSbr5pqaGsO"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============ Fixed Camelot table extraction ============\n",
    "def generate_table_fingerprint(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Generate table fingerprint for deduplication\"\"\"\n",
    "    try:\n",
    "        fingerprint_parts = []\n",
    "        fingerprint_parts.append(f\"shape_{df.shape[0]}x{df.shape[1]}\")\n",
    "\n",
    "        if not df.columns.empty:\n",
    "            col_names = [str(col).strip().lower().replace(' ', '') for col in df.columns]\n",
    "            col_fingerprint = '_'.join(sorted(col_names))\n",
    "            fingerprint_parts.append(f\"cols_{hash(col_fingerprint)}\")\n",
    "\n",
    "        if df.shape[0] > 0:\n",
    "            numeric_values = []\n",
    "            for col in df.columns:\n",
    "                for val in df[col].head(3):\n",
    "                    if pd.notna(val):\n",
    "                        numbers = re.findall(r'\\d+\\.?\\d*', str(val))\n",
    "                        numeric_values.extend(numbers)\n",
    "\n",
    "            if numeric_values:\n",
    "                numeric_fingerprint = hash('_'.join(sorted(numeric_values[:10])))\n",
    "                fingerprint_parts.append(f\"nums_{numeric_fingerprint}\")\n",
    "\n",
    "        return '_'.join(fingerprint_parts)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error generating table fingerprint: {e}\")\n",
    "        return str(hash(df.to_csv()))\n",
    "\n",
    "def clean_table_data_improved(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Improved table data cleaning\"\"\"\n",
    "    try:\n",
    "        cleaned_df = df.copy()\n",
    "        cleaned_df = cleaned_df.dropna(how='all')\n",
    "        cleaned_df = cleaned_df.dropna(axis=1, how='all')\n",
    "\n",
    "        for col in cleaned_df.columns:\n",
    "            if cleaned_df[col].dtype == 'object':\n",
    "                cleaned_df[col] = cleaned_df[col].astype(str).str.strip()\n",
    "                cleaned_df[col] = cleaned_df[col].replace(['nan', 'NaN', 'None'], '')\n",
    "\n",
    "        if not cleaned_df.empty:\n",
    "            new_columns = []\n",
    "            for i, col in enumerate(cleaned_df.columns):\n",
    "                col_str = str(col).strip()\n",
    "                if col_str in ['nan', 'NaN', 'None', ''] or pd.isna(col):\n",
    "                    new_columns.append(f'Column_{i}')\n",
    "                else:\n",
    "                    new_columns.append(col_str)\n",
    "            cleaned_df.columns = new_columns\n",
    "\n",
    "        cleaned_df = cleaned_df.reset_index(drop=True)\n",
    "        return cleaned_df\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error in table cleaning: {e}\")\n",
    "        return df\n",
    "\n",
    "def is_valid_table_improved(df: pd.DataFrame) -> bool:\n",
    "    \"\"\"Improved table validation\"\"\"\n",
    "    try:\n",
    "        if df.empty or df.shape[0] < 1 or df.shape[1] < 1:\n",
    "            return False\n",
    "\n",
    "        non_null_cells = 0\n",
    "        total_cells = df.shape[0] * df.shape[1]\n",
    "\n",
    "        for col in df.columns:\n",
    "            for val in df[col]:\n",
    "                if pd.notna(val) and str(val).strip() not in ['', 'nan', 'NaN', 'None']:\n",
    "                    non_null_cells += 1\n",
    "\n",
    "        if non_null_cells / total_cells < 0.2:\n",
    "            return False\n",
    "\n",
    "        has_meaningful_content = False\n",
    "        for col in df.columns:\n",
    "            text_content = ' '.join(df[col].dropna().astype(str))\n",
    "            if (any(char.isdigit() for char in text_content) or\n",
    "                '%' in text_content or\n",
    "                any(keyword in text_content.lower() for keyword in [\n",
    "                    'rate', 'percentage', 'total', 'number', 'emission', 'energy',\n",
    "                    'water', 'waste', 'employee', 'year', '2020', '2021', '2022', '2023'\n",
    "                ])):\n",
    "                has_meaningful_content = True\n",
    "                break\n",
    "\n",
    "        return has_meaningful_content\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error validating table: {e}\")\n",
    "        return True\n",
    "\n",
    "def format_table_output_improved(df: pd.DataFrame, table_id: str, parsing_report=None) -> str:\n",
    "    \"\"\"Improved table output formatting\"\"\"\n",
    "    try:\n",
    "        table_info = f\"TABLE_START_{table_id}\\n\"\n",
    "        table_info += f\"DIMENSIONS: {df.shape[0]} rows × {df.shape[1]} columns\\n\"\n",
    "\n",
    "        col_info = \"COLUMNS: \" + \" | \".join([f\"{i}:{col}\" for i, col in enumerate(df.columns)])\n",
    "        table_info += col_info + \"\\n\"\n",
    "\n",
    "        if df.shape[0] > 0:\n",
    "            preview_rows = min(2, df.shape[0])\n",
    "            table_info += f\"PREVIEW_FIRST_{preview_rows}_ROWS:\\n\"\n",
    "            for i in range(preview_rows):\n",
    "                row_preview = \" | \".join([str(df.iloc[i, j])[:20] for j in range(min(5, df.shape[1]))])\n",
    "                table_info += f\"  Row_{i}: {row_preview}\\n\"\n",
    "\n",
    "        if parsing_report:\n",
    "            try:\n",
    "                accuracy = getattr(parsing_report, 'accuracy', 'N/A')\n",
    "                if accuracy != 'N/A':\n",
    "                    table_info += f\"EXTRACTION_ACCURACY: {accuracy:.2f}\\n\"\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        table_info += \"TABLE_DATA_START\\n\"\n",
    "        table_csv = df.to_csv(index=False, na_rep='', quoting=1, escapechar='\\\\')\n",
    "        table_end = f\"TABLE_DATA_END\\nTABLE_END_{table_id}\\n\"\n",
    "\n",
    "        return table_info + table_csv + table_end\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error formatting table output: {e}\")\n",
    "        return f\"TABLE_START_{table_id}\\n{df.to_csv(index=False)}\\nTABLE_END_{table_id}\\n\"\n",
    "\n",
    "def camelot_extra_tables_enhanced(path: str) -> List[str]:\n",
    "    \"\"\"Enhanced table extraction using Camelot with better error handling\"\"\"\n",
    "    try:\n",
    "        import camelot\n",
    "    except ImportError:\n",
    "        logging.warning(\"Camelot not installed, skipping Camelot table parsing.\")\n",
    "        return []\n",
    "\n",
    "    extra_chunks = []\n",
    "    extracted_tables_fingerprints = set()\n",
    "\n",
    "    try:\n",
    "        logging.info(\"Starting Camelot table extraction...\")\n",
    "\n",
    "        # Stream mode extraction\n",
    "        try:\n",
    "            stream_tables = camelot.read_pdf(\n",
    "                path,\n",
    "                pages=\"all\",\n",
    "                flavor=\"stream\",\n",
    "                edge_tol=50,\n",
    "                row_tol=2,\n",
    "                column_tol=0\n",
    "            )\n",
    "\n",
    "            stream_count = 0\n",
    "            for i, table in enumerate(stream_tables):\n",
    "                if not table.df.empty and table.df.shape[0] > 0:\n",
    "                    table_fingerprint = generate_table_fingerprint(table.df)\n",
    "\n",
    "                    if table_fingerprint not in extracted_tables_fingerprints:\n",
    "                        cleaned_df = clean_table_data_improved(table.df)\n",
    "\n",
    "                        if is_valid_table_improved(cleaned_df):\n",
    "                            table_txt = format_table_output_improved(cleaned_df, f\"STREAM_{i}\", table.parsing_report)\n",
    "                            extra_chunks.append(table_txt)\n",
    "                            extracted_tables_fingerprints.add(table_fingerprint)\n",
    "                            stream_count += 1\n",
    "\n",
    "            logging.info(f\"Stream mode extracted {stream_count} valid tables\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Stream mode extraction failed: {e}\")\n",
    "\n",
    "        # Lattice mode extraction\n",
    "        try:\n",
    "            lattice_tables = camelot.read_pdf(\n",
    "                path,\n",
    "                pages=\"all\",\n",
    "                flavor=\"lattice\",\n",
    "                line_scale=15,\n",
    "                line_tol=2,\n",
    "                joint_tol=2\n",
    "            )\n",
    "\n",
    "            lattice_count = 0\n",
    "            for i, table in enumerate(lattice_tables):\n",
    "                if not table.df.empty and table.df.shape[0] > 0:\n",
    "                    table_fingerprint = generate_table_fingerprint(table.df)\n",
    "\n",
    "                    if table_fingerprint not in extracted_tables_fingerprints:\n",
    "                        cleaned_df = clean_table_data_improved(table.df)\n",
    "\n",
    "                        if is_valid_table_improved(cleaned_df):\n",
    "                            table_txt = format_table_output_improved(cleaned_df, f\"LATTICE_{i}\", table.parsing_report)\n",
    "                            extra_chunks.append(table_txt)\n",
    "                            extracted_tables_fingerprints.add(table_fingerprint)\n",
    "                            lattice_count += 1\n",
    "\n",
    "            logging.info(f\"Lattice mode extracted {lattice_count} additional unique tables\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Lattice mode extraction failed: {e}\")\n",
    "\n",
    "        total_extracted = len(extra_chunks)\n",
    "        logging.info(f\"Camelot extraction completed: {total_extracted} total unique tables extracted\")\n",
    "        return extra_chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Camelot table extraction failed: {e}\")\n",
    "        return []"
   ],
   "metadata": {
    "id": "amKWHBAHaGuk"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============ Text Chunking ============\n",
    "def split_into_chunks(full_text: str, max_tokens: int) -> List[str]:\n",
    "    \"\"\"Split text into chunks based on token limit\"\"\"\n",
    "    paragraphs = [p for p in full_text.split(\"\\n\") if p.strip()]\n",
    "    chunks, current = [], []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        para_tokens = len(enc.encode(paragraph))\n",
    "\n",
    "        if current_tokens + para_tokens > max_tokens and current:\n",
    "            chunks.append(\"\\n\".join(current))\n",
    "            current = [paragraph]\n",
    "            current_tokens = para_tokens\n",
    "        else:\n",
    "            current.append(paragraph)\n",
    "            current_tokens += para_tokens\n",
    "\n",
    "    if current:\n",
    "        chunks.append(\"\\n\".join(current))\n",
    "\n",
    "    return chunks"
   ],
   "metadata": {
    "id": "ZSLAWykuaGxs"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============ System prompt words ============\n",
    "UNIVERSAL_SYSTEM_PROMPT = textwrap.dedent(\"\"\"\n",
    "    You are a professional ESG data analyst specializing in extracting Key Performance Indicators (KPIs) from sustainability reports.\n",
    "\n",
    "    ## CRITICAL: What is a KPI?\n",
    "    A KPI MUST contain SPECIFIC NUMBERS, PERCENTAGES, or MEASURABLE QUANTITIES that demonstrate actual performance or concrete targets.\n",
    "\n",
    "    ## IMPORTANT: Table Data Processing Rules\n",
    "    When processing table data:\n",
    "    1. Pay close attention to column headers to identify the correct time periods\n",
    "    2. Match data values with their corresponding year columns\n",
    "    3. If you see table format like \"Metric, 2021, 2022\" - the first number after metric belongs to 2021, second to 2022\n",
    "    4. Look for table headers that indicate year columns (e.g., \"2020\", \"2021\", \"2022\")\n",
    "    5. Extract each year's data as separate KPIs\n",
    "    6. Avoid extracting the same KPI multiple times - consolidate similar metrics\n",
    "\n",
    "    ## ENHANCED: Advanced Table Processing\n",
    "    7. **EXTRACT ALL DATA POINTS**: For each table cell containing a number, create a separate KPI\n",
    "    8. **REGIONAL/LOCATION DATA**: Pay special attention to location-specific data (countries, regions, cities)\n",
    "    9. **WORKFORCE DATA**: Extract all employee numbers, headcount data, and demographic information\n",
    "    10. **INCOMPLETE DATA**: Extract available data even if some cells are empty or missing\n",
    "    11. **TOTALS AND SUBTOTALS**: Always extract total values and aggregated numbers\n",
    "\n",
    "    ## ✅ VALID KPI EXAMPLES:\n",
    "    - \"Achieved 89.4% reuse and recycle rate for cloud hardware in 2023\"\n",
    "    - \"Diverted over 18,537 metric tons of waste from landfills in 2023\"\n",
    "    - \"Reduced single-use plastics in product packaging to 2.7%\"\n",
    "    - \"Contracted 19 GW of new renewable energy across 16 countries in 2024\"\n",
    "    - \"Provided clean water access to over 1.5 million people in 2023\"\n",
    "    - \"Protected 15,849 acres of land—exceeding target by more than 30%\"\n",
    "    - \"Allocated 761 million toward innovative climate technologies\"\n",
    "    - \"Achieved 80% renewable energy operations by 2024\"\n",
    "    - \"Water replenishment projects estimated to provide over 25 million cubic meters\"\n",
    "    - \"Exceeded annual target to divert 75% of construction waste by reaching 85%\"\n",
    "    - \"Board independence: 78% of directors\"\n",
    "    - \"Women in senior leadership increased to 35% in 2023\"\n",
    "    - \"Employee engagement score: 87% in annual survey\"\n",
    "    - \"Reduced greenhouse gas emissions by 50% compared to 2019 baseline\"\n",
    "    - \"Zero workplace fatalities achieved for third consecutive year\"\n",
    "    - \"Training completion rate: 98% for mandatory compliance courses\"\n",
    "    - \"Supplier ESG assessments completed for 95% of tier-1 suppliers\"\n",
    "    - \"Customer satisfaction rating: 4.6 out of 5.0\"\n",
    "    - \"Data breach incidents: 0 material breaches in 2023\"\n",
    "\n",
    "    ## ❌ NOT KPIs (DO NOT EXTRACT):\n",
    "    - \"Microsoft will require select suppliers to use carbon-free electricity by 2030\"\n",
    "    - \"The company plans to expand Sustainability Manager capabilities\"\n",
    "    - \"We are launching two new Circular Centers in 2023\"\n",
    "    - \"The organization established a new climate innovation fund\"\n",
    "    - \"Microsoft introduced enhanced data governance solutions\"\n",
    "    - \"Updated guidebook to include guidance on corporate responsibility\"\n",
    "    - \"Plans to publish new ESG strategy\"\n",
    "    - \"Implemented a new recycling program\"\n",
    "    - \"Conducted sustainability training sessions\"\n",
    "    - \"Launched employee wellness programs\"\n",
    "    - \"Committed to reducing emissions\"\n",
    "    - \"Focusing on environmental performance\"\n",
    "    - \"Established sustainability committee\"\n",
    "    - \"The company operates facilities in multiple regions\"\n",
    "    - \"Our supply chain includes thousands of vendors globally\"\n",
    "    - Any text without specific numbers, percentages, or quantifiable metrics\n",
    "    - Duplicate or repeated metrics (extract only once per time period)\n",
    "    - Any statement that describes business operations rather than performance outcomes\n",
    "\n",
    "    ## KPI Categories:\n",
    "    ### Environmental:\n",
    "    - **Carbon_Climate**: GHG emissions, carbon footprint, emission reductions, climate targets, scope 1/2/3 emissions, carbon intensity, carbon offsets, TCFD alignment\n",
    "    - **Energy**: Energy consumption, renewable energy percentage, energy efficiency, energy intensity, MWh, GWh, energy savings, fossil fuel usage\n",
    "    - **Water**: Water withdrawal, water consumption, water intensity, water recycling, water reuse, water stress, water discharge quality\n",
    "    - **Waste**: Waste generation, recycling rates, diversion percentages, hazardous waste, non-hazardous waste, zero waste to landfill, e-waste, incineration\n",
    "    - **Biodiversity**: Protected areas, species conservation, habitat restoration, biodiversity impact assessments, land use, ecosystem restoration\n",
    "    - **Circular_Economy**: Recycling rates, material recovery, circular design, raw materials usage, renewable materials, packaging waste\n",
    "    - **Materials**: Raw materials consumption, recycled content, sustainable materials, material intensity, sustainable sourcing\n",
    "\n",
    "    ### Social:\n",
    "    - **Workforce_Diversity**: Employee demographics, gender diversity, age diversity, ethnic diversity, disability inclusion, LGBTQ+ inclusion, workforce composition\n",
    "    - **Gender_Equality**: Women in leadership, gender pay ratio, parental leave return rates, gender representation, female employees percentage\n",
    "    - **Disability_Inclusion**: Employees with disabilities, accessibility compliance, inclusive workplace design, disability support programs\n",
    "    - **Health_Safety**: Lost Time Injury Frequency Rate (LTIFR), Total Recordable Incident Rate (TRIR), fatalities, workplace illness, safety training hours, PPE compliance, emergency drills\n",
    "    - **Employee_Wellbeing**: Employee satisfaction, retention rates, turnover rates, training hours, wellness programs, mental health services, work-life balance\n",
    "    - **Community_Engagement**: Corporate volunteering, social investment, community impact assessments, local hiring, stakeholder engagement activities\n",
    "    - **Human_Rights**: Child labor incidents, forced labor, human rights due diligence, freedom of association, grievance mechanisms, labor audits\n",
    "    - **Labor_Rights**: Collective bargaining coverage, labor complaints resolution, supplier labor audits, working conditions, fair wages\n",
    "    - **Customer_Safety**: Product safety incidents, customer satisfaction, accessibility features, safety recalls, quality metrics\n",
    "    - **Supply_Chain_Social**: Supplier assessments, sustainable sourcing, supplier code compliance, supply chain audits\n",
    "\n",
    "    ### Governance:\n",
    "    - **Board_Governance**: Board independence, board diversity, CEO-chair separation, board ESG expertise, board composition, director tenure\n",
    "    - **Executive_Compensation**: ESG-linked compensation, executive pay ratios, compensation disclosure, incentive structures\n",
    "    - **Ethics_Compliance**: Code of conduct training, corruption incidents, bribery cases, fines and penalties, whistleblower reports, anti-corruption assessments\n",
    "    - **Transparency_Disclosure**: ESG reporting coverage, third-party assurance, political contributions disclosure, GRI/SASB/TCFD compliance\n",
    "    - **Risk_Management**: Risk assessments, mitigation measures, climate risk disclosure, operational risk management\n",
    "    - **Cybersecurity_Data**: Cybersecurity breaches, data privacy policies, cybersecurity training, GDPR compliance, data protection measures\n",
    "    - **Supply_Chain_Governance**: Supplier ESG screening, supplier audits, procurement ESG clauses, vendor compliance rates\n",
    "\n",
    "    ## MANDATORY Requirements:\n",
    "    1. MUST contain specific numbers (e.g., 25%, 15,000, 2.5M, 8.5%, 0.3 per million hours)\n",
    "    2. MUST relate to measurable sustainability outcomes\n",
    "    3. MUST have time reference (year, period, or deadline)\n",
    "    4. MUST be performance-focused (results, not activities or descriptions)\n",
    "    5. MUST NOT be future plans or operational descriptions\n",
    "\n",
    "    ## Output Format:\n",
    "    Return a JSON array. Each KPI must contain:\n",
    "    ```json\n",
    "    {\n",
    "        \"kpi_text\": \"Complete original sentence with the quantifiable metric\",\n",
    "        \"kpi_theme\": \"Environmental/Social/Governance\",\n",
    "        \"kpi_category\": \"Specific category from above list\",\n",
    "        \"quantitative_value\": \"The specific number/percentage extracted\",\n",
    "        \"unit\": \"Unit of measurement (%, tonnes, employees, etc.)\",\n",
    "        \"time_period\": \"Time reference (2023, annual, by 2030, etc.)\",\n",
    "        \"target_or_actual\": \"Target/Actual/Both\"\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    ## Additional Instructions:\n",
    "    - If a sentence includes a comparison value, such as a baseline, previous year, or other historical/target data (e.g., \"Compared to 32,395 MWh in 2020\"), extract it as a **separate KPI**.\n",
    "    - Do NOT store the comparison in any other field — just create another valid KPI from it.\n",
    "    - Avoid merging multiple numerical values into one KPI unless they are clearly part of the same metric (e.g., male: X, female: Y).\n",
    "\n",
    "    ## STRICT FILTERING:\n",
    "    - Return empty array [] if no quantifiable KPIs found\n",
    "    - Only extract text that contains specific measurable values\n",
    "    - Ignore all qualitative statements, plans, and descriptions\n",
    "    - Focus only on numerical performance data\n",
    "\n",
    "    Now analyze the following text for sustainability KPIs:\n",
    "\"\"\").strip()\n",
    "\n",
    "# 🔥 新增：增强的图像分析Prompt\n",
    "ENHANCED_IMAGE_KPI_SYSTEM_PROMPT = textwrap.dedent(\"\"\"\n",
    "    You are an expert data analyst specializing in extracting quantifiable KPI data from charts, graphs, and data visualizations in sustainability reports.\n",
    "\n",
    "    ## CRITICAL INSTRUCTION: ALWAYS EXTRACT NUMERICAL VALUES\n",
    "\n",
    "    **Your primary task is to extract the ACTUAL NUMBERS and PERCENTAGES visible in charts, not just descriptions.**\n",
    "\n",
    "    ## MISSION:\n",
    "    Extract ALL quantifiable data points from charts and graphs, including:\n",
    "    - Bar charts (vertical/horizontal)\n",
    "    - Pie charts and donut charts\n",
    "    - Line charts and trend graphs\n",
    "    - Stacked charts and combo charts\n",
    "    - Tables with numerical data\n",
    "    - Infographics with statistics\n",
    "    - Gauge charts and dashboards\n",
    "\n",
    "    ## DETAILED ANALYSIS INSTRUCTIONS:\n",
    "\n",
    "    ### For PIE CHARTS:\n",
    "    1. Read percentage labels on each slice\n",
    "    2. If no labels visible, estimate based on slice size\n",
    "    3. Identify what each slice represents (categories)\n",
    "    4. Extract each slice as separate KPI\n",
    "    5. **MUST read the percentage labels on each slice** - Look for numbers like 64%, 33%, 68%, 30%, etc.\n",
    "    6. **If percentages are visible on the chart, extract them exactly**\n",
    "    7. **If no labels visible, estimate based on slice size using these guidelines:**\n",
    "       - 90° slice = 25%\n",
    "       - 180° slice = 50%\n",
    "       - 270° slice = 75%\n",
    "       - Full circle = 100%\n",
    "    8. **Each slice MUST have a specific percentage value in the final output**\n",
    "\n",
    "    ### For BAR CHARTS:\n",
    "    1. Read Y-axis scale carefully (units, increments)\n",
    "    2. Estimate bar heights using grid lines and scale\n",
    "    3. Read X-axis labels (years, categories, regions)\n",
    "    4. Extract each bar as separate KPI\n",
    "    5. Pay attention to grouped/stacked bars\n",
    "\n",
    "    ### For LINE CHARTS:\n",
    "    1. Read data points at intersection of grid lines\n",
    "    2. Follow trend lines to extract values for each time period\n",
    "    3. Use Y-axis scale for value estimation\n",
    "    4. Extract each data point as separate KPI\n",
    "\n",
    "    ### For TABLES:\n",
    "    1. Read all numerical values in cells\n",
    "    2. Match values with row and column headers\n",
    "    3. Extract each cell with numerical data as KPI\n",
    "\n",
    "    ## MANDATORY VALUE EXTRACTION RULES:\n",
    "\n",
    "    **RULE 1**: Every KPI MUST contain a specific numerical value (percentage, amount, count, etc.)\n",
    "    **RULE 2**: For charts with categories, you MUST find and extract the quantitative values for each category\n",
    "    **RULE 3**: Never create KPIs without specific numbers - descriptions alone are incomplete\n",
    "    **RULE 4**: Include complete context: what + how much + when/where if available\n",
    "\n",
    "\n",
    "    ## VALUE ESTIMATION GUIDELINES:\n",
    "    - Use proportional analysis: if a bar reaches 80% of scale maximum, calculate 80% of max value\n",
    "    - For pie charts: estimate slice angles (90° = 25%, 180° = 50%, etc.)\n",
    "    - Cross-reference with any visible data labels or legends\n",
    "    - Be conservative but reasonably accurate in estimates\n",
    "\n",
    "    ## CHART IDENTIFICATION:\n",
    "    First identify the chart type, then apply appropriate extraction method.\n",
    "    Look for:\n",
    "    - Axes and scales\n",
    "    - Data labels and legends\n",
    "    - Grid lines for reference\n",
    "    - Color coding and patterns\n",
    "    - Title and subtitle information\n",
    "\n",
    "    ## OUTPUT FORMAT:\n",
    "    Return a JSON array. For each data point found:\n",
    "    ```json\n",
    "    {\n",
    "        \"kpi_text\": \"Complete description with the ACTUAL NUMERICAL VALUE included\",\n",
    "        \"kpi_theme\": \"Environmental/Social/Governance\",\n",
    "        \"kpi_category\": \"Specific category based on content\",\n",
    "        \"quantitative_value\": \"The exact number/percentage (e.g., '64', '33.5', '68')\",\n",
    "        \"unit\": \"% / tonnes / employees / MWh / USD / etc.\",\n",
    "        \"time_period\": \"2021/2020/2022/Year/period/etc if identifiable\",\n",
    "        \"target_or_actual\": \"Actual\",\n",
    "        \"chart_type\": \"pie_chart/bar_chart/line_chart/table/etc\",\n",
    "        \"estimation_confidence\": \"High/Medium/Low\",\n",
    "        \"chart_title\": \"Chart title if visible\",\n",
    "        \"data_source\": \"Legend or source if visible\"\n",
    "    }\n",
    "\n",
    "    ```\n",
    "    ## EXAMPLES of CORRECT vs INCORRECT extraction:\n",
    "\n",
    "    ### ❌ INCORRECT (incomplete - missing numerical values):\n",
    "    ```json\n",
    "    {\n",
    "        \"kpi_text\": \"Energy consumption by facility type\",\n",
    "        \"quantitative_value\": \"\",\n",
    "        \"unit\": \"%\"\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    ### ✅ CORRECT (complete with specific values):\n",
    "    ```json\n",
    "    {\n",
    "        \"kpi_text\": \"Office buildings account for 45% of total energy consumption\",\n",
    "        \"quantitative_value\": \"45\",\n",
    "        \"unit\": \"%\"\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    ### ❌ INCORRECT (category without value):\n",
    "    ```json\n",
    "    {\n",
    "        \"kpi_text\": \"Renewable energy percentage by region\",\n",
    "        \"quantitative_value\": \"\",\n",
    "        \"unit\": \"%\"\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    ### ✅ CORRECT (specific regional data):\n",
    "    ```json\n",
    "    {\n",
    "        \"kpi_text\": \"North America achieved 78% renewable energy usage\",\n",
    "        \"quantitative_value\": \"78\",\n",
    "        \"unit\": \"%\"\n",
    "    }\n",
    "    ```\n",
    "    ## QUALITY ASSURANCE CHECKLIST:\n",
    "    Before returning results, verify:\n",
    "    - ✅ Every KPI contains a specific numerical value\n",
    "    - ✅ Chart categories are paired with their quantitative data\n",
    "    - ✅ KPI descriptions are complete and self-explanatory\n",
    "    - ✅ Units are correctly identified and specified\n",
    "    - ✅ Context (time, location, category) is preserved when available\n",
    "    - Each KPI must have a specific numerical value\n",
    "    - Context must be clear and self-contained\n",
    "    - Avoid extracting the same data point multiple times\n",
    "    - Focus on sustainability/ESG metrics when possible\n",
    "\n",
    "    ## VALUE ESTIMATION GUIDELINES:\n",
    "    - **High confidence**: Numbers clearly visible in image\n",
    "    - **Medium confidence**: Numbers estimated using chart scales/grid lines\n",
    "    - **Low confidence**: Values approximated from proportional analysis\n",
    "    - **If no numerical data is visible, return empty array []**\n",
    "\n",
    "    ## IMPORTANT NOTES:\n",
    "    - Extract ALL visible data points, not just main highlights\n",
    "    - Include context in descriptions (e.g., \"According to pie chart showing emission sources\")\n",
    "    - If values are not clearly visible, make reasonable estimates and mark confidence as \"Low\"\n",
    "    - Return empty array [] ONLY if image contains no charts/graphs with quantifiable data\n",
    "    - For multi-year data, create separate KPIs for each year\n",
    "    - Pay special attention to small text and numbers\n",
    "    - Focus on extracting actual performance data, not just identifying chart elements\n",
    "    - If you can see numbers in the image, you MUST extract them\n",
    "    - Pie chart percentages are usually the most important data points\n",
    "    - Return empty array [] ONLY if no numerical data is visible\n",
    "\n",
    "    Now analyze the provided image and extract ALL quantifiable KPI data points:\n",
    "\"\"\").strip()"
   ],
   "metadata": {
    "id": "IonQ4PK2aGzy"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============ KPI Extraction Function ============\n",
    "def extract_page_from_chunk(chunk: str) -> str:\n",
    "    \"\"\"Extract page information from chunk\"\"\"\n",
    "    # Look for PAGE_X_TEXT: format\n",
    "    page_matches = re.findall(r'PAGE_(\\d+)_TEXT:', chunk)\n",
    "    if page_matches:\n",
    "        pages = [int(p) for p in page_matches]\n",
    "        if len(pages) == 1:\n",
    "            return str(pages[0])\n",
    "        else:\n",
    "            return f\"{min(pages)}-{max(pages)}\"\n",
    "\n",
    "    # Look for TABLE_START_PAGE_X_\n",
    "    table_matches = re.findall(r'TABLE_START_PAGE_(\\d+)_', chunk)\n",
    "    if table_matches:\n",
    "        pages = [int(p) for p in table_matches]\n",
    "        if len(pages) == 1:\n",
    "            return str(pages[0])\n",
    "        else:\n",
    "            return f\"{min(pages)}-{max(pages)}\"\n",
    "\n",
    "    return \"Unknown\"\n",
    "\n",
    "def contains_procedural_language(text: str) -> bool:\n",
    "    \"\"\"Check if text contains procedural language\"\"\"\n",
    "    procedural_words = [\n",
    "        'introduced', 'established', 'set up', 'implemented', 'created',\n",
    "        'launched', 'formed', 'built', 'installed', 'deployed',\n",
    "        'additionally introduced', 'procedure for', 'standardization management'\n",
    "    ]\n",
    "    text_lower = text.lower()\n",
    "    return any(word in text_lower for word in procedural_words)\n",
    "\n",
    "def is_data_fragment(kpi_text: str) -> bool:\n",
    "    \"\"\"Check if text is a meaningless data fragment\"\"\"\n",
    "    text = kpi_text.strip()\n",
    "\n",
    "    # Filter pure numbers or simple percentages without context\n",
    "    if re.match(r'^\\d+\\.?\\d*%?$', text):\n",
    "        return True\n",
    "\n",
    "    # Filter very short text (less than 4 meaningful words)\n",
    "    meaningful_words = [word for word in text.split() if len(word) > 2 and not word.isdigit()]\n",
    "    if len(meaningful_words) < 3:\n",
    "        return True\n",
    "\n",
    "    # Filter text with only numbers and common connecting words\n",
    "    words = text.lower().split()\n",
    "    non_functional_words = [word for word in words if word not in ['in', 'of', 'the', 'and', 'or', 'to', 'for', 'with', 'by']]\n",
    "    if len(non_functional_words) < 3:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def standardize_kpi_universal(kpi_item: Dict) -> Dict:\n",
    "    \"\"\"Universal KPI data standardization\"\"\"\n",
    "    standardized = kpi_item.copy()\n",
    "\n",
    "    # Standardize numerical formats\n",
    "    quantitative_value = str(standardized.get('quantitative_value', '')).strip()\n",
    "    kpi_text = standardized.get('kpi_text', '').lower()\n",
    "\n",
    "    # Smart handling of percentage formats\n",
    "    if quantitative_value and quantitative_value.replace('.', '').replace('-', '').replace(',', '').isdigit():\n",
    "        # Check if original text suggests this is a percentage\n",
    "        percentage_indicators = ['percent', 'percentage', '%', 'rate', 'ratio', 'proportion', 'share']\n",
    "        if any(indicator in kpi_text for indicator in percentage_indicators):\n",
    "            if not quantitative_value.endswith('%'):\n",
    "                standardized['quantitative_value'] = quantitative_value + '%'\n",
    "                if not standardized.get('unit'):\n",
    "                    standardized['unit'] = '%'\n",
    "\n",
    "    # Ensure unit field consistency\n",
    "    if '%' in str(standardized.get('quantitative_value', '')):\n",
    "        standardized['unit'] = '%'\n",
    "\n",
    "    # Clean and normalize KPI text\n",
    "    kpi_text_original = standardized.get('kpi_text', '').strip()\n",
    "    # Remove extra spaces and newlines\n",
    "    kpi_text_cleaned = ' '.join(kpi_text_original.split())\n",
    "    standardized['kpi_text'] = kpi_text_cleaned\n",
    "\n",
    "    return standardized\n",
    "\n",
    "def generate_universal_metric_key(kpi_item: Dict) -> str:\n",
    "    \"\"\"Generate universal metric key for deduplication\"\"\"\n",
    "    try:\n",
    "        # Extract core elements\n",
    "        category = kpi_item.get('kpi_category', '').lower().strip()\n",
    "        value = str(kpi_item.get('quantitative_value', '')).replace('%', '').replace(',', '').strip()\n",
    "        time_period = kpi_item.get('time_period', '').lower().strip()\n",
    "        unit = kpi_item.get('unit', '').lower().strip()\n",
    "\n",
    "        # Extract key semantic information from KPI text\n",
    "        kpi_text = kpi_item.get('kpi_text', '').lower()\n",
    "\n",
    "        # Extract primary number (for more precise matching)\n",
    "        numbers_in_text = re.findall(r'\\d+\\.?\\d*', kpi_text)\n",
    "        primary_number = numbers_in_text[0] if numbers_in_text else value\n",
    "\n",
    "        # Generate semantic signature: extract keywords from text\n",
    "        # Remove common stop words\n",
    "        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'}\n",
    "\n",
    "        # Extract keywords (length>2 and not stop words)\n",
    "        words = re.findall(r'\\b\\w+\\b', kpi_text)\n",
    "        key_words = [word for word in words if len(word) > 2 and word not in stop_words and not word.isdigit()]\n",
    "\n",
    "        # Sort keywords to ensure consistency\n",
    "        key_words = sorted(set(key_words))[:5]  # Take at most 5 keywords\n",
    "        semantic_signature = '_'.join(key_words)\n",
    "\n",
    "        # Build universal metric key\n",
    "        key_components = []\n",
    "\n",
    "        if category:\n",
    "            key_components.append(f\"cat:{category}\")\n",
    "        if primary_number:\n",
    "            key_components.append(f\"val:{primary_number}\")\n",
    "        if time_period:\n",
    "            key_components.append(f\"time:{time_period}\")\n",
    "        if unit:\n",
    "            key_components.append(f\"unit:{unit}\")\n",
    "        if semantic_signature:\n",
    "            key_components.append(f\"sem:{semantic_signature}\")\n",
    "\n",
    "        # Generate final key\n",
    "        metric_key = \"|\".join(key_components)\n",
    "\n",
    "        # If all components are empty, use text hash\n",
    "        if not metric_key:\n",
    "            metric_key = f\"hash:{hash(kpi_text)}\"\n",
    "\n",
    "        return metric_key\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error generating universal metric key: {e}\")\n",
    "        # Fallback to text hash\n",
    "        return f\"fallback:{hash(kpi_item.get('kpi_text', ''))}\"\n",
    "\n",
    "def extract_kpi_from_chunk_universal(chunk: str) -> List[Dict]:\n",
    "    \"\"\"Universal KPI extraction function for various sustainability reports\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": UNIVERSAL_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"Extract ALL KPIs from this text. Requirements:\n",
    "\n",
    "1. Create COMPLETE, MEANINGFUL KPI descriptions with full context\n",
    "2. DO NOT extract standalone numbers without explanatory text\n",
    "3. Include all relevant context (time, location, metric type, etc.)\n",
    "4. Use consistent formatting for similar metrics\n",
    "5. Ensure each KPI is self-explanatory\n",
    "\n",
    "Text to analyze:\n",
    "{chunk}\"\"\"}\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            max_tokens=4000,\n",
    "            timeout=60\n",
    "        )\n",
    "\n",
    "        content = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Clean potential markdown formatting\n",
    "        if content.startswith('```json'):\n",
    "            content = content[7:]\n",
    "        if content.endswith('```'):\n",
    "            content = content[:-3]\n",
    "\n",
    "        if not content.strip().startswith(\"[\"):\n",
    "            logging.warning(f\"API response not JSON list: {content[:100]}...\")\n",
    "            return []\n",
    "\n",
    "        result = json.loads(content)\n",
    "\n",
    "        if not isinstance(result, list):\n",
    "            logging.warning(\"API response is not a list format\")\n",
    "            return []\n",
    "\n",
    "        # Extract page information\n",
    "        page_number = extract_page_from_chunk(chunk)\n",
    "\n",
    "        # Universal validation and deduplication logic\n",
    "        validated_result = []\n",
    "        seen_metrics = set()\n",
    "\n",
    "        for item in result:\n",
    "            if isinstance(item, dict) and 'kpi_text' in item and 'kpi_theme' in item:\n",
    "                if item['kpi_text'].strip() and item['kpi_theme'].strip():\n",
    "\n",
    "                    # Check procedural language\n",
    "                    if contains_procedural_language(item['kpi_text']):\n",
    "                        logging.debug(f\"Procedural statement filtered: {item['kpi_text'][:50]}...\")\n",
    "                        continue\n",
    "\n",
    "                    # Filter meaningless data fragments\n",
    "                    if is_data_fragment(item['kpi_text']):\n",
    "                        logging.debug(f\"Data fragment filtered: {item['kpi_text']}\")\n",
    "                        continue\n",
    "\n",
    "                    # Standardize KPI data\n",
    "                    standardized_item = standardize_kpi_universal(item)\n",
    "\n",
    "                    # Add page information\n",
    "                    standardized_item['source_page'] = page_number\n",
    "                    standardized_item['source_type'] = 'text'\n",
    "\n",
    "                    # Universal deduplication mechanism\n",
    "                    metric_key = generate_universal_metric_key(standardized_item)\n",
    "\n",
    "                    if metric_key not in seen_metrics:\n",
    "                        validated_result.append(standardized_item)\n",
    "                        seen_metrics.add(metric_key)\n",
    "                        logging.debug(f\"KPI extracted: {standardized_item['kpi_text'][:80]}...\")\n",
    "                    else:\n",
    "                        logging.debug(f\"Duplicate metric filtered: {standardized_item['kpi_text'][:50]}...\")\n",
    "\n",
    "        logging.info(f\"Chunk processed: {len(validated_result)} unique KPIs extracted\")\n",
    "        return validated_result\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        logging.warning(f\"JSON parsing failed: {e}\\nContent: {content[:300]}...\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        logging.error(f\"API call failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def post_process_kpis_universal(kpis: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Universal KPI post-processing for various report types\"\"\"\n",
    "    if not kpis:\n",
    "        return kpis\n",
    "\n",
    "    # Step 1: Deduplication based on metric keys\n",
    "    unique_kpis_dict = {}\n",
    "\n",
    "    for kpi in kpis:\n",
    "        metric_key = generate_universal_metric_key(kpi)\n",
    "\n",
    "        if metric_key not in unique_kpis_dict:\n",
    "            unique_kpis_dict[metric_key] = kpi\n",
    "        else:\n",
    "            # If duplicate, keep the more complete KPI description\n",
    "            existing_kpi = unique_kpis_dict[metric_key]\n",
    "            current_kpi = kpi\n",
    "\n",
    "            # Compare KPI text completeness\n",
    "            if len(current_kpi.get('kpi_text', '')) > len(existing_kpi.get('kpi_text', '')):\n",
    "                unique_kpis_dict[metric_key] = current_kpi\n",
    "                logging.debug(f\"Replaced with more complete KPI: {current_kpi.get('kpi_text', '')[:50]}...\")\n",
    "            else:\n",
    "                logging.debug(f\"Kept existing KPI: {existing_kpi.get('kpi_text', '')[:50]}...\")\n",
    "\n",
    "    # Step 2: Text similarity-based secondary deduplication\n",
    "    final_kpis = list(unique_kpis_dict.values())\n",
    "\n",
    "    # Use text similarity to check remaining potential duplicates\n",
    "    final_unique_kpis = []\n",
    "\n",
    "    for current_kpi in final_kpis:\n",
    "        is_duplicate = False\n",
    "        current_text = current_kpi.get('kpi_text', '')\n",
    "\n",
    "        for existing_kpi in final_unique_kpis:\n",
    "            existing_text = existing_kpi.get('kpi_text', '')\n",
    "\n",
    "            # Calculate text similarity\n",
    "            similarity = calculate_text_similarity(current_text, existing_text)\n",
    "\n",
    "            # If similarity is very high, consider it duplicate\n",
    "            if similarity > 0.8:\n",
    "                is_duplicate = True\n",
    "                logging.debug(f\"Text similarity duplicate filtered: {current_text[:50]}...\")\n",
    "                break\n",
    "\n",
    "        if not is_duplicate:\n",
    "            final_unique_kpis.append(current_kpi)\n",
    "\n",
    "    logging.info(f\"Universal post-processing: {len(final_unique_kpis)}/{len(kpis)} KPIs retained\")\n",
    "    return final_unique_kpis\n",
    "\n",
    "def calculate_text_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"Calculate similarity between two texts\"\"\"\n",
    "    # Normalize texts\n",
    "    norm1 = ' '.join(text1.lower().split())\n",
    "    norm2 = ' '.join(text2.lower().split())\n",
    "\n",
    "    # Word sets\n",
    "    words1 = set(norm1.split())\n",
    "    words2 = set(norm2.split())\n",
    "\n",
    "    if len(words1) == 0 or len(words2) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate intersection and union\n",
    "    intersection = len(words1.intersection(words2))\n",
    "    union = len(words1.union(words2))\n",
    "\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def validate_kpi_quality(kpis: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Additional quality validation for extracted KPIs with relaxed filtering\"\"\"\n",
    "    if not ENABLE_QUALITY_VALIDATION:\n",
    "        return kpis\n",
    "\n",
    "    quality_kpis = []\n",
    "\n",
    "    for kpi in kpis:\n",
    "        kpi_text = kpi.get('kpi_text', '').lower()\n",
    "\n",
    "        # Exclude \"planned tone\" KPIs (not actual performance)\n",
    "        is_future_statement = any(word in kpi_text for word in [\n",
    "            'will', 'aim to', 'plan to', 'planning to', 'intend to',\n",
    "            'is expected to', 'is scheduled to', 'expects to', 'expected to',\n",
    "            'targeting', 'propose to', 'going to', 'shall', 'to be installed'\n",
    "        ])\n",
    "        if is_future_statement:\n",
    "            logging.debug(f\"KPI rejected (future plan): {kpi_text[:100]}...\")\n",
    "            continue\n",
    "\n",
    "        # Filter procedural language\n",
    "        if contains_procedural_language(kpi_text):\n",
    "            logging.debug(f\"KPI rejected (procedural language): {kpi_text[:100]}...\")\n",
    "            continue\n",
    "\n",
    "        # Filter for phrases like \"place name + percentage\" (not ESG KPIs, but distribution descriptions)\n",
    "        geo_percent_pattern = re.compile(r\"^[a-z\\s,:%-]+(?:\\s)?\\d{1,3}%$\")\n",
    "        if geo_percent_pattern.match(kpi_text.strip()) and len(kpi_text.strip().split()) <= 6:\n",
    "            logging.debug(f\"KPI rejected (geo+percent short form): {kpi_text}\")\n",
    "            continue\n",
    "\n",
    "        # Verb whitelist: must include action verbs\n",
    "        allowed_kpi_verbs = [\n",
    "            'reduce', 'reduced', 'achieve', 'achieved', 'improve', 'improved',\n",
    "            'diverted', 'trained', 'invested', 'decreased', 'increased',\n",
    "            'consumed', 'emitted', 'saved', 'reached', 'attained', 'completed',\n",
    "            'recorded', 'cut', 'lowered', 'targeted', 'complied', 'avoided',\n",
    "            'used', 'recycled', 'sourced', 'returned', 'measured', 'maintained',\n",
    "            'reported', 'accounted', 'utilized', 'were', 'was'  # Add state verbs\n",
    "        ]\n",
    "        if not any(verb in kpi_text for verb in allowed_kpi_verbs):\n",
    "            logging.debug(f\"KPI rejected (no action verb): {kpi_text[:100]}...\")\n",
    "            continue\n",
    "\n",
    "        # Greylist verbs (action words but not necessarily performance words) - remove problematic words\n",
    "        graylist_verbs = [\n",
    "            'launched',  # Keep some potentially useful words, but remove obvious procedural words\n",
    "            'formed', 'opened', 'started'\n",
    "        ]\n",
    "\n",
    "        contains_graylist = any(verb in kpi_text for verb in graylist_verbs)\n",
    "\n",
    "        # Check for quantitative indicators\n",
    "        has_numbers = any(char.isdigit() for char in kpi_text)\n",
    "        has_percentage = '%' in kpi_text\n",
    "\n",
    "        # Extended units and measurement indicators\n",
    "        has_units = any(unit in kpi_text for unit in [\n",
    "            'tonnes', 'tons', 'kg', 'mwh', 'kwh', 'gwh', 'litres', 'liters', 'gallons',\n",
    "            'employees', 'hours', 'million', 'billion', 'thousand', 'm³', 'co2e', 'tco2e',\n",
    "            'dollars', 'usd', 'eur', 'gbp', 'incidents', 'rate', 'ratio', 'intensity',\n",
    "            'frequency', 'recordable', 'fatalities', 'injuries', 'directors', 'board',\n",
    "            'workforce', 'leadership', 'diversity', 'inclusion', 'satisfaction', 'retention',\n",
    "            'turnover', 'training', 'safety', 'ltifr', 'trir', 'compliance', 'audit',\n",
    "            'assessment', 'screening', 'supplier', 'breach', 'violation', 'disclosure',\n",
    "            'assurance', 'coverage', 'participation', 'completion', 'investment',\n",
    "            'volunteering', 'engagement', 'grievance', 'whistleblower', 'compensation',\n",
    "            'people', 'staff', 'workers', 'positions', 'roles', 'headcount', 'fte',\n",
    "            'performance', 'score', 'index', 'metric', 'level', 'amount', 'value',\n",
    "            'average', 'median', 'total', 'sum', 'count', 'number', 'quantity'\n",
    "        ])\n",
    "\n",
    "        # More flexible time reference detection\n",
    "        has_time_ref = any(time_word in kpi_text for time_word in [\n",
    "            '2019', '2020', '2021', '2022', '2023', '2024', '2025', '2026', '2027', '2028', '2029', '2030',\n",
    "            '2031', '2032', '2033', '2034', '2035', '2040', '2045', '2050',\n",
    "            'annual', 'yearly', 'year', 'quarter', 'month', 'by', 'target', 'baseline', 'fy',\n",
    "            'per year', 'per annum', 'quarterly', 'monthly', 'daily', 'future', 'deadline',\n",
    "            'period', 'reporting', 'current', 'previous', 'next', 'last', 'this'\n",
    "        ])\n",
    "\n",
    "        # Enhanced sustainability context detection\n",
    "        has_sustainability_context = any(sus_word in kpi_text for sus_word in [\n",
    "            # Environmental keywords\n",
    "            'emission', 'carbon', 'energy', 'renewable', 'waste', 'water', 'recycl',\n",
    "            'environmental', 'ghg', 'scope', 'climate', 'biodiversity', 'circular',\n",
    "            'materials', 'intensity', 'consumption', 'efficiency', 'footprint',\n",
    "            'sustainable', 'sustainability', 'green', 'clean', 'eco', 'offset',\n",
    "            'tcfd', 'nature', 'habitat', 'ecosystem', 'pollution', 'discharge',\n",
    "            'electricity', 'gas', 'fuel', 'solar', 'wind', 'hydro', 'nuclear',\n",
    "\n",
    "            # Social keywords\n",
    "            'safety', 'training', 'employee', 'diversity', 'community', 'social',\n",
    "            'workforce', 'gender', 'women', 'female', 'male', 'disability', 'disabled',\n",
    "            'inclusion', 'equity', 'equality', 'lgbtq', 'minorities', 'ethnic',\n",
    "            'health', 'wellbeing', 'wellness', 'satisfaction', 'retention', 'turnover',\n",
    "            'injury', 'incident', 'fatality', 'ltifr', 'trir', 'recordable',\n",
    "            'human rights', 'labor', 'child labor', 'forced labor', 'slavery',\n",
    "            'freedom', 'association', 'collective bargaining', 'grievance',\n",
    "            'volunteering', 'investment', 'hiring', 'local', 'stakeholder',\n",
    "            'customer', 'supplier', 'supply chain', 'accessibility', 'parental',\n",
    "            'mental health', 'ppe', 'emergency', 'drill', 'compliance',\n",
    "            'people', 'staff', 'workers', 'employment', 'job', 'career',\n",
    "            'leadership', 'management', 'senior', 'executive', 'promotion',\n",
    "\n",
    "            # Governance keywords\n",
    "            'governance', 'board', 'director', 'independent', 'chair', 'ceo',\n",
    "            'executive', 'compensation', 'pay', 'ethics', 'compliance', 'corruption',\n",
    "            'bribery', 'code of conduct', 'whistleblower', 'transparency',\n",
    "            'disclosure', 'reporting', 'assurance', 'audit', 'risk', 'management',\n",
    "            'cybersecurity', 'data', 'privacy', 'gdpr', 'breach', 'policy',\n",
    "            'screening', 'assessment', 'due diligence', 'political', 'contribution',\n",
    "            'gri', 'sasb', 'oversight', 'expertise', 'separation', 'incentive',\n",
    "            'fine', 'penalty', 'violation', 'resolution', 'anti-corruption',\n",
    "\n",
    "            # General business performance that could be sustainability-related\n",
    "            'performance', 'quality', 'delivery', 'customer', 'service', 'product',\n",
    "            'operation', 'facility', 'site', 'location', 'region', 'business'\n",
    "        ])\n",
    "\n",
    "        # If it is a greylist verb sentence, but there is no performance content such as numbers, units, time, etc. → delete\n",
    "        if contains_graylist and not (has_numbers or has_units or has_percentage or has_time_ref or has_sustainability_context):\n",
    "            logging.debug(f\"KPI rejected (graylist verb, no quantitative data): {kpi_text[:100]}...\")\n",
    "            continue\n",
    "\n",
    "        # More lenient quality scoring - only require numbers and either units/percentage OR time reference OR sustainability context\n",
    "        basic_requirements = has_numbers and (has_percentage or has_units or has_time_ref or has_sustainability_context)\n",
    "\n",
    "        # Additional check for obvious ESG relevance\n",
    "        is_esg_relevant = any(esg_word in kpi_text for esg_word in [\n",
    "            'emission', 'carbon', 'energy', 'waste', 'water', 'renewable', 'employee',\n",
    "            'safety', 'training', 'diversity', 'governance', 'board', 'compliance',\n",
    "            'sustainability', 'environmental', 'social', 'ghg', 'co2', 'workforce',\n",
    "            'gender', 'health', 'injury', 'incident', 'ethics', 'transparency'\n",
    "        ])\n",
    "\n",
    "        if basic_requirements or is_esg_relevant:\n",
    "            quality_kpis.append(kpi)\n",
    "            logging.debug(f\"KPI accepted: {kpi_text[:100]}...\")\n",
    "        else:\n",
    "            logging.debug(f\"KPI filtered out for quality: {kpi_text[:100]}...\")\n",
    "\n",
    "    logging.info(f\"Quality validation: {len(quality_kpis)}/{len(kpis)} KPIs passed\")\n",
    "    return quality_kpis"
   ],
   "metadata": {
    "id": "sB_kDyLZaG1o"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============ Image processing functions ============\n",
    "def extract_numeric_spans(page):\n",
    "    text_dict = page.get_text(\"dict\")\n",
    "    nums = []\n",
    "    for block in text_dict[\"blocks\"]:\n",
    "        for line in block.get(\"lines\", []):\n",
    "            for span in line.get(\"spans\", []):\n",
    "                s = span[\"text\"].strip()\n",
    "                if re.match(r\"[\\d,.]+%?$\", s):          # Pure number or number + %\n",
    "                    nums.append({\n",
    "                        \"text\": s,\n",
    "                        \"bbox\": span[\"bbox\"],           # (x0,y0,x1,y1)\n",
    "                        \"font\": span[\"size\"]\n",
    "                    })\n",
    "    return nums\n",
    "\n",
    "def extract_images_from_pdf_fixed(pdf_path: str) -> List[Dict]:\n",
    "    \"\"\"Extract images from PDF using PyMuPDF\"\"\"\n",
    "    images = []\n",
    "\n",
    "    try:\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "        for page_num in range(len(pdf_document)):\n",
    "            page = pdf_document[page_num]\n",
    "            image_list = page.get_images()\n",
    "\n",
    "            # 🔥 New: Extract page screenshots as an alternative\n",
    "            page_pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # high resolution\n",
    "            page_img = Image.frombytes(\"RGB\", [page_pix.width, page_pix.height], page_pix.samples)\n",
    "\n",
    "            # Add full page screenshot\n",
    "            images.append({\n",
    "                'image': page_img,\n",
    "                'page_number': page_num + 1,\n",
    "                'width': page_img.width,\n",
    "                'height': page_img.height,\n",
    "                'image_index': 'full_page',\n",
    "                'type': 'full_page'\n",
    "            })\n",
    "\n",
    "\n",
    "            for img_index, img in enumerate(image_list):\n",
    "                try:\n",
    "                    xref = img[0]\n",
    "                    base_image = pdf_document.extract_image(xref)\n",
    "                    image_bytes = base_image[\"image\"]\n",
    "\n",
    "                    image = Image.open(BytesIO(image_bytes))\n",
    "\n",
    "                    # Convert to RGB if needed\n",
    "                    if image.mode in ['RGBA', 'LA']:\n",
    "                        background = Image.new('RGB', image.size, (255, 255, 255))\n",
    "                        if image.mode == 'RGBA':\n",
    "                            background.paste(image, mask=image.split()[-1])\n",
    "                        else:\n",
    "                            background.paste(image)\n",
    "                        image = background\n",
    "                    elif image.mode != 'RGB':\n",
    "                        image = image.convert('RGB')\n",
    "\n",
    "                    # Filter small images\n",
    "                    if image.width >= 50 and image.height >= 50:\n",
    "                        images.append({\n",
    "                            'image': image,\n",
    "                            'page_number': page_num + 1,\n",
    "                            'width': image.width,\n",
    "                            'height': image.height,\n",
    "                            'image_index': img_index,\n",
    "                            'type': 'extracted'  # 🔥 Added type identifier\n",
    "                        })\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error extracting image {img_index} from page {page_num + 1}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        pdf_document.close()\n",
    "        logging.info(f\"Extracted {len(images)} images from PDF\")\n",
    "        return images\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting images from PDF: {e}\")\n",
    "        return []\n",
    "\n",
    "def image_to_base64_fixed(image: Image.Image) -> str:\n",
    "    \"\"\"Convert image to base64 with error handling\"\"\"\n",
    "    try:\n",
    "        if image.mode not in ['RGB', 'L']:\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        # Resize large images\n",
    "        max_size = (1536, 1536)\n",
    "        if image.width > max_size[0] or image.height > max_size[1]:\n",
    "            # Calculate scaling to maintain aspect ratio\n",
    "            ratio = min(max_size[0]/image.width, max_size[1]/image.height)\n",
    "            new_size = (int(image.width * ratio), int(image.height * ratio))\n",
    "            image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\", quality=95)\n",
    "        img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "\n",
    "        return img_str\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error converting image to base64: {e}\")\n",
    "        return \"\""
   ],
   "metadata": {
    "id": "Vfegv4osaG3b"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ------------------------------------------------------------\n",
    "# Multi-crop / multi-resolution generator (supports crop parameter 0)\n",
    "# ------------------------------------------------------------\n",
    "from itertools import product\n",
    "\n",
    "def generate_image_variants(img: Image.Image,\n",
    "                            max_side_full: int = 1200,\n",
    "                            crop_size: int = 768,\n",
    "                            stride: int = 512) -> List[Tuple[Image.Image, str]]:\n",
    "    \"\"\"\n",
    "    Returns [(variant_image, variant_tag), ...]\n",
    "    variant_tag value: original / resized / crop_{row}_{col}\n",
    "    \"\"\"\n",
    "    variants = []\n",
    "\n",
    "    # 0) Original image\n",
    "    variants.append((img, \"original\"))\n",
    "\n",
    "    # 1) Zoom (if the original image is too large)\n",
    "    w, h = img.size\n",
    "    if max(w, h) > max_side_full:\n",
    "        scale = max_side_full / float(max(w, h))\n",
    "        resized = img.resize((int(w * scale), int(h * scale)), Image.Resampling.LANCZOS)\n",
    "        variants.append((resized, \"resized\"))\n",
    "    else:\n",
    "        resized = img  # Keep the original image without scaling\n",
    "        variants.append((resized, \"resized\"))  # Unified plus resized version\n",
    "\n",
    "    # 2) Sliding window cropping (skipped when cropping size or step size is 0)\n",
    "    if crop_size > 0 and stride > 0:\n",
    "        base_img = variants[-1][0]\n",
    "        bw, bh = base_img.size\n",
    "        if bw > crop_size or bh > crop_size:\n",
    "            xs = list(range(0, max(bw - crop_size, 1), stride)) + [bw - crop_size]\n",
    "            ys = list(range(0, max(bh - crop_size, 1), stride)) + [bh - crop_size]\n",
    "            for r, c in product(range(len(ys)), range(len(xs))):\n",
    "                x, y = xs[c], ys[r]\n",
    "                crop = base_img.crop((x, y, x + crop_size, y + crop_size))\n",
    "                # Filter solid color areas\n",
    "                if np.array(crop.convert('L')).std() < 5:\n",
    "                    continue\n",
    "                variants.append((crop, f\"crop_{r}_{c}\"))\n",
    "\n",
    "    return variants"
   ],
   "metadata": {
    "id": "ma9D4crJaG5z"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------\n",
    "# 📊 A chart recognition function that replaces plotclassifier (Hugging Face model)\n",
    "# ---------------------------------------------\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "import torch\n",
    "# 🔧 Fix: Chart recognition with CLIP model\n",
    "def setup_chart_classifier():\n",
    "    \"\"\"Setting up the chart classifier\"\"\"\n",
    "    try:\n",
    "        from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "        # Loading CLIP Model\n",
    "        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "        def is_chart_image_clip(image: Image.Image) -> bool:\n",
    "            \"\"\"Use CLIP to determine whether it is a chart\"\"\"\n",
    "            try:\n",
    "                # Defines text description related to the chart\n",
    "                chart_labels = [\n",
    "                    \"a chart\", \"a graph\", \"a bar chart\", \"a pie chart\",\n",
    "                    \"a line graph\", \"a table\", \"data visualization\",\n",
    "                    \"statistics\", \"a diagram\", \"an infographic\"\n",
    "                ]\n",
    "\n",
    "                # Processing Input\n",
    "                inputs = processor(\n",
    "                    text=chart_labels,\n",
    "                    images=image,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True\n",
    "                )\n",
    "\n",
    "                # Get prediction results\n",
    "                outputs = model(**inputs)\n",
    "                logits_per_image = outputs.logits_per_image\n",
    "                probs = logits_per_image.softmax(dim=1)\n",
    "\n",
    "                # If the probability of any chart label is greater than 0.25, it is considered to be a chart\n",
    "                max_prob = probs.max().item()\n",
    "                is_chart = max_prob > 0.25\n",
    "\n",
    "                logging.debug(f\"CLIP chart recognition: maximum probability = {max_prob:.3f}, result = {is_chart}\")\n",
    "                return is_chart\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"CLIP chart recognition failed: {e}\")\n",
    "                # Downgrade to statistical methods\n",
    "                gray = image.convert('L')\n",
    "                return np.array(gray).std() > 15\n",
    "\n",
    "        logging.info(\"✅ Graph recognition using CLIP model\")\n",
    "        return is_chart_image_clip\n",
    "\n",
    "    except ImportError:\n",
    "        logging.warning(\"CLIP model is not available, use statistical methods\")\n",
    "        def is_chart_image_stats(image: Image.Image) -> bool:\n",
    "            \"\"\"Statistical method to determine whether it is a chart\"\"\"\n",
    "            try:\n",
    "                gray = image.convert('L')\n",
    "                std_dev = np.array(gray).std()\n",
    "                return std_dev > 15\n",
    "            except:\n",
    "                return True\n",
    "\n",
    "        return is_chart_image_stats\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to set chart classifier: {e}\")\n",
    "        def is_chart_image_fallback(image: Image.Image) -> bool:\n",
    "            return True  # Conservative Strategy: When in Doubt, Analyze\n",
    "        return is_chart_image_fallback\n",
    "\n",
    "# Initialize the graph classifier\n",
    "is_chart_image = setup_chart_classifier()\n",
    "\n",
    "\n",
    "def extract_kpi_from_image_fixed(image: Image.Image, page_number: int, image_type: str = 'extracted') -> List[Dict]:\n",
    "    \"\"\"Extract KPIs from image with improved error handling\"\"\"\n",
    "    try:\n",
    "        # 🔥 New: Pre-filter: Check if it might be a chart\n",
    "        if not is_chart_image(image):\n",
    "            logging.debug(f\"Image on page {page_number} filtered out (not likely a chart)\")\n",
    "            return []\n",
    "\n",
    "        base64_image = image_to_base64_fixed(image)\n",
    "        if not base64_image:\n",
    "            return []\n",
    "\n",
    "        # 🔥 Change: Use enhanced prompt\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": ENHANCED_IMAGE_KPI_SYSTEM_PROMPT  # 🔥 Using the new prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            # 🔥 New: Detailed user instructions\n",
    "                            \"text\": \"\"\"Analyze this image carefully for quantifiable performance data.\n",
    "\n",
    "IMPORTANT ANALYSIS PRINCIPLES:\n",
    "\n",
    "1. **Chart Type Recognition**:\n",
    "   - Stacked charts: Multiple colors/patterns layered in same position\n",
    "   - Grouped charts: Multiple elements side by side at same position\n",
    "   - Simple charts: One data point per position\n",
    "\n",
    "2. **Value Extraction Rules**:\n",
    "   - For STACKED charts: Read each layer separately, NOT the total height\n",
    "   - For GROUPED charts: Read each element individually\n",
    "   - For SIMPLE charts: Read data point values directly\n",
    "\n",
    "3. **Data Relevance Filter**:\n",
    "   ✅ EXTRACT: Performance outcomes, efficiency metrics, reduction rates, satisfaction scores, compliance rates\n",
    "   ❌ SKIP: Certification counts, project timelines, implementation schedules, organizational charts, process flows\n",
    "\n",
    "4. **Quality Standards**:\n",
    "   - Only extract clear, quantifiable performance indicators\n",
    "   - Each data point must have complete context\n",
    "   - If uncertain about values, don't estimate\n",
    "   - If chart shows mainly operational/administrative data, return empty array\n",
    "\n",
    "Please analyze this chart step by step:\n",
    "- First identify the chart type\n",
    "- Then determine if it contains performance KPIs\n",
    "- Finally extract all relevant performance data points\n",
    "\n",
    "Focus on measurable outcomes and achievements, not counts or processes.\"\"\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                                \"detail\": \"high\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=4000,\n",
    "            timeout=60\n",
    "        )\n",
    "\n",
    "        content = response.choices[0].message.content.strip()\n",
    "\n",
    "        if not content:\n",
    "            return []\n",
    "\n",
    "        # Clean formatting\n",
    "        if content.startswith('```json'):\n",
    "            content = content[7:]\n",
    "        if content.endswith('```'):\n",
    "            content = content[:-3]\n",
    "\n",
    "        content = content.strip()\n",
    "\n",
    "        if not content.startswith(\"[\"):\n",
    "            logging.warning(f\"Image analysis response not JSON list: {content[:100]}...\")\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            result = json.loads(content)\n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.warning(f\"JSON parsing failed for image analysis: {e}\")\n",
    "            return []\n",
    "\n",
    "        if not isinstance(result, list):\n",
    "            return []\n",
    "\n",
    "        # Process results\n",
    "        processed_result = []\n",
    "        for item in result:\n",
    "            if isinstance(item, dict) and 'kpi_text' in item:\n",
    "                if not item.get('kpi_text', '').strip():\n",
    "                    continue\n",
    "\n",
    "                item['source_page'] = page_number\n",
    "                item['source_type'] = 'image'\n",
    "                item['image_type'] = image_type  # 🔥 新增字段\n",
    "\n",
    "                # 🔥 更改：确保有chart标识\n",
    "                kpi_text = item['kpi_text']\n",
    "                if not any(marker in kpi_text.lower() for marker in ['chart', 'graph', 'table', 'figure']):\n",
    "                    chart_type = item.get('chart_type', 'chart')\n",
    "                    item['kpi_text'] = f\"[{chart_type.title()}] {kpi_text}\"\n",
    "\n",
    "                processed_result.append(item)\n",
    "\n",
    "        if processed_result:\n",
    "            logging.info(f\"✅ Extracted {len(processed_result)} KPIs from {image_type} on page {page_number}\")\n",
    "        else:\n",
    "            logging.debug(f\"❌ No KPIs found in {image_type} on page {page_number}\")\n",
    "\n",
    "        return processed_result\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting KPIs from image: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def process_pdf_images_for_kpis_fixed(pdf_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Traverse each page of the PDF:\n",
    "        • Perform multiple cropping + Vision on all ‘extracted’ images on the page\n",
    "        • If the page has not captured the KPI, perform Vision on the entire page screenshot\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting page-by-page image KPI extraction …\")\n",
    "\n",
    "    images = extract_images_from_pdf_fixed(pdf_path)\n",
    "    if not images:\n",
    "        return []\n",
    "\n",
    "    # Aggregate images by page\n",
    "    page_dict = {}\n",
    "    for info in images:\n",
    "        pg = info[\"page_number\"]\n",
    "        page_dict.setdefault(pg, {\"extracted\": [], \"full\": None})\n",
    "        if info[\"type\"] == \"extracted\":\n",
    "            page_dict[pg][\"extracted\"].append(info[\"image\"])\n",
    "        else:                    # full_page\n",
    "            page_dict[pg][\"full\"] = info[\"image\"]\n",
    "\n",
    "    all_image_kpis: List[Dict] = []\n",
    "\n",
    "    # —— Page by page processing ——\n",
    "    for pg in sorted(page_dict.keys()):\n",
    "        logging.info(f\"\\n=== Page {pg} ===\")\n",
    "        page_kpis: List[Dict] = []\n",
    "\n",
    "        # ① Individually extracted images\n",
    "        for idx, img in enumerate(page_dict[pg][\"extracted\"]):\n",
    "            for var_img, var_tag in generate_image_variants(img, 1200, 768, 512):\n",
    "                kpis = extract_kpi_from_image_fixed(\n",
    "                    var_img, pg, f\"extracted_{var_tag}\"\n",
    "                )\n",
    "                for k in kpis:\n",
    "                    key = generate_universal_metric_key(k)\n",
    "                    if key not in {generate_universal_metric_key(x) for x in page_kpis}:\n",
    "                        page_kpis.append(k)\n",
    "                time.sleep(0.8)\n",
    "\n",
    "        # ② If it is still empty, analyze the entire page again\n",
    "        if not page_kpis and page_dict[pg][\"full\"] is not None:\n",
    "            for var_img, var_tag in generate_image_variants(\n",
    "                    page_dict[pg][\"full\"], 1200, 0, 0):   # 只做 original/resized\n",
    "                kpis = extract_kpi_from_image_fixed(\n",
    "                    var_img, pg, f\"full_{var_tag}\"\n",
    "                )\n",
    "                for k in kpis:\n",
    "                    key = generate_universal_metric_key(k)\n",
    "                    if key not in {generate_universal_metric_key(x) for x in page_kpis}:\n",
    "                        page_kpis.append(k)\n",
    "                time.sleep(1.0)\n",
    "\n",
    "        logging.info(f\"  → Page {pg} KPI count: {len(page_kpis)}\")\n",
    "        all_image_kpis.extend(page_kpis)\n",
    "\n",
    "    logging.info(f\"Image KPI extraction finished: {len(all_image_kpis)} KPIs from {len(page_dict)} pages\")\n",
    "    return all_image_kpis"
   ],
   "metadata": {
    "id": "F5ZLpwL8Zok_",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343,
     "referenced_widgets": [
      "0ccce40b728940e084ed44759e6f6a3c",
      "1a172779d818401d9e5f0a214815f483",
      "8d2c6c2957554af89331c72491000a60",
      "a635c12c3dba492dae47dbb1505d4672",
      "008e4c8201424c8cb2f398dc22e529da",
      "b2440ae5c97a4fcb92c0d4f8c89a5aeb",
      "22d5a2876f2f44c29243d7b96f432861",
      "5b2a3c0a87f94b8898c83024220d1e11",
      "c31a22c8653147eda676a7dc391e6cab",
      "c2aa8b8f0df94d5daa9970e077c79993",
      "98bfd7b5b8804088a9acdee84db1d0a9",
      "83fc87aef0e44f1c9efa556852a50535",
      "4fb7b631141d4dc386a395446a222d26",
      "f45e3c6571a5479685e1a5ff0de7baee",
      "00206bf0a5d149bdbb534e917db1bef2",
      "9501f3fedfdb4d159f3c3b892432d729",
      "e943538241e6457ba970352a6f96059f",
      "db5e00d2fef84840a9cacefe3fc7c1d9",
      "e6c84fd09471495fa3631ecd76e85d97",
      "2441d4481cc14e14828e852d3a1a153e",
      "a490041783ef45cc98a54630bc9cd393",
      "9b02fcdba7c44d5f9d8dcb862e30a82d",
      "ad004cb9ee8f4a4e96b022f585a0907a",
      "b56b6c4b69ac4cf28e2febb0531d2b92",
      "0bc9f1efaab5417c86bcb20ea56ee8b8",
      "e9535287c0e54cf692bfa24b702b6475",
      "f40b7ec0ba034f0aa8b9094503918b3d",
      "6dc7bf1d30354c00a0ba6aa1d14908d6",
      "1f9ab98197d643ca80ed16ad28a52da2",
      "c6096b1b211d40adaca08c2fc635a7a1",
      "fc1a53207c8e4286a3c0399449ffdc4b",
      "e7995bd155434285abe5622772ae529f",
      "739e450e933e4b9597a02486adbd6cb9",
      "6984cb1a9323453c80c386e066abb72f",
      "aa7b609f6ce746459777798f8cad4e7c",
      "b3b19a69907c41a5987d4003e5778e6b",
      "587539bde2ef4a15bcd1f3964af9ea10",
      "c493417e34204b04a4a3b4401d2c397a",
      "0587c5ccd53146309278915ff67d5e3c",
      "beb304462d8149ed94cae4554a0d2b0d",
      "59b3c0904f1949c8ad7d8ebe85ab9699",
      "b1aa3a529f0f4d02b8d804ab3fe6833d",
      "06902734c8d74980a8ab896a8a4e75ca",
      "fc7096c1a41e4485987f3bff71879990",
      "0c2a41c3cfd348cc909ada49d30fbf53",
      "37c52b64dd224bb7a8211a1bc9127045",
      "d67e417be4604dee918b05f20229d49f",
      "d470b6ef7a874108bce384ea882aa9bc",
      "70905b5292514b73aad50ecf57071621",
      "7c848e102d0c4a559fa5959307eca19f",
      "8866fca2dba142959336ca621822ed57",
      "d40bdd0b6dfa408fad567ae182586ca2",
      "abd9ea4a8caa47629630c649601af91f",
      "2e27e8476ce94485a341cb2912787687",
      "0cfb5ab6124c465896ae7efa919ab888",
      "3ab4f790ec134fac8bb8c7e520e24fda",
      "a62e35740c9c4be8a33fdc15577fea13",
      "e642eab9e2834d8f9b2a979bf9ac2a51",
      "8048e7d6e1a94c9eb65b08b05602ed1c",
      "cc92a407708046e48b0a73427c889ccd",
      "bef94210926b49539c34807397ec6115",
      "dc39303ab58a4313b0bc72b13bbddbac",
      "9ac180c8a52f4679ac04df996aec2c31",
      "59d358dafacd45f395c4a83b54274d1f",
      "90b3410ad0864b41894e816220b0a006",
      "0c97ad7a8e9242e1af4efc874c5c8c24",
      "e718b9af3ce1406989c267de242e42ab",
      "c58f091283354f698beac0a8f4ceda9e",
      "cedb9014073f4564823a9ae25cbd94ba",
      "89382a88b3804ccdb5d707ead82231fe",
      "e55fc8ecc872430799f7578b416e2197",
      "884438185fe643a6a5f0c106121e8378",
      "0b4e3b2ed9354a49b5cce768ed95b757",
      "4365b450bf4648808faa4f158767a270",
      "9e4201d4a3b643a0b19ae2b3bcc334d5",
      "35af810e53514a6db976c4c89720f7b8",
      "a7a44700821e4a56ba54f01ee1eeaefa",
      "6fc9ae0453fd4fe09d393e92ec6b7289",
      "417dea0e987540c0b7ade5e03cde85f4",
      "64a250f4e69347d48d822b934abf134e",
      "2b4617e352874e2fb7317f7ae1f660f3",
      "8c4ac8a99b2f42c0b11e924c321fcca2",
      "0c19b4a646864f348b72a2507871c01a",
      "1ca96e705e964c61af7622291d7624e8",
      "1146807e77bc4fdfbcd074bf04f18d0f",
      "f7f8c1af66e24d309bb3ed5c4d2f90a7",
      "ba8cc3ceac6f4500a99d5409c78d87ae",
      "43aa4639221f4a81b688010bc4c2e20b",
      "957c2f3e8e214a56894f191bbee07ee4",
      "86bad43bd7334e1aade785e368095a2d",
      "a9f2ea6fa7dc42edb8db0f4dcb154c4a",
      "6926dc6ab1ab4e62af882354b0cc357b",
      "14a9c4052f674f5386584a266c2c4dff",
      "faeb39a8529c4de198a2acd641aba786",
      "b3e5f2841bca45429e0b03eb58e415b1",
      "60ef1db8886c45c58d46319e9e65f3f1",
      "a309b6bee65d481085d2ff28ea2ec7c9",
      "50040d9132fa49baac87ce120d69471c",
      "c247c3bec87f4ade9939b18d226bc661"
     ]
    },
    "outputId": "58254de4-2efb-4663-e98e-7fa01e6ba95a"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ccce40b728940e084ed44759e6f6a3c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "83fc87aef0e44f1c9efa556852a50535"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad004cb9ee8f4a4e96b022f585a0907a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6984cb1a9323453c80c386e066abb72f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c2a41c3cfd348cc909ada49d30fbf53"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3ab4f790ec134fac8bb8c7e520e24fda"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e718b9af3ce1406989c267de242e42ab"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6fc9ae0453fd4fe09d393e92ec6b7289"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "957c2f3e8e214a56894f191bbee07ee4"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 1. Conservative image filter\n",
    "def conservative_image_filter(image: Image.Image) -> Tuple[bool, str]:\n",
    "    \"\"\"Conservative image filtering - only filters obviously useless images\"\"\"\n",
    "    try:\n",
    "        # Only filter very small images (maybe logos, icons)\n",
    "        if image.width < 40 or image.height < 40:\n",
    "            return False, \"too_small_icon\"\n",
    "\n",
    "        # Filter only images of almost pure colors (decorative elements)\n",
    "        gray = np.array(image.convert('L'))\n",
    "        std_dev = gray.std()\n",
    "\n",
    "        # Very conservative threshold - only images with completely pure colors are filtered\n",
    "        if std_dev < 3:\n",
    "            return False, \"pure_color\"\n",
    "\n",
    "        # Check if it is a pure white background (blank area)\n",
    "        mean_val = gray.mean()\n",
    "        if mean_val > 250 and std_dev < 8:\n",
    "            return False, \"blank_white\"\n",
    "\n",
    "        # Default: Process all other images to ensure integrity\n",
    "        return True, \"keep_for_analysis\"\n",
    "    except Exception:\n",
    "        return True, \"filter_error_keep\"\n",
    "\n",
    "# 2. Cache mechanism\n",
    "class FastKPICache:\n",
    "    def __init__(self, cache_dir: str = \"fast_kpi_cache\"):\n",
    "        self.cache_dir = cache_dir\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        self.hit_count = 0\n",
    "        self.miss_count = 0\n",
    "\n",
    "    def get_image_hash(self, image: Image.Image) -> str:\n",
    "        \"\"\"Fast image fingerprint generation\"\"\"\n",
    "        width, height = image.size\n",
    "        if width > 100 and height > 100:\n",
    "            center_crop = image.crop((\n",
    "                width//4, height//4,\n",
    "                3*width//4, 3*height//4\n",
    "            )).resize((32, 32))\n",
    "            img_bytes = BytesIO()\n",
    "            center_crop.save(img_bytes, format='JPEG', quality=50)\n",
    "            sample_hash = hashlib.md5(img_bytes.getvalue()).hexdigest()[:16]\n",
    "        else:\n",
    "            sample_hash = hashlib.md5(str(width * height).encode()).hexdigest()[:16]\n",
    "\n",
    "        return f\"{width}x{height}_{sample_hash}\"\n",
    "\n",
    "    def get_cached_kpis(self, image_hash: str) -> Optional[List[Dict]]:\n",
    "        cache_file = os.path.join(self.cache_dir, f\"{image_hash}.pkl\")\n",
    "        if os.path.exists(cache_file):\n",
    "            try:\n",
    "                with open(cache_file, 'rb') as f:\n",
    "                    self.hit_count += 1\n",
    "                    return pickle.load(f)\n",
    "            except:\n",
    "                pass\n",
    "        self.miss_count += 1\n",
    "        return None\n",
    "\n",
    "    def cache_kpis(self, image_hash: str, kpis: List[Dict]):\n",
    "        cache_file = os.path.join(self.cache_dir, f\"{image_hash}.pkl\")\n",
    "        try:\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump(kpis, f)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def get_stats(self):\n",
    "        total = self.hit_count + self.miss_count\n",
    "        hit_rate = self.hit_count / total if total > 0 else 0\n",
    "        return f\"Cache: {self.hit_count} hits, {self.miss_count} misses (hit rate: {hit_rate:.1%})\"\n",
    "\n",
    "# Initialize the cache\n",
    "fast_cache = FastKPICache()\n",
    "\n",
    "# 3. Optimized API calls\n",
    "COMPREHENSIVE_EXTRACTION_PROMPT = \"\"\"\n",
    "You are an expert data analyst. Extract ALL quantifiable performance indicators from this image.\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "1. Extract EVERY visible number, percentage, and metric\n",
    "2. Include ALL data points from charts, graphs, and tables\n",
    "3. Do not skip any quantifiable information\n",
    "\n",
    "Return complete JSON array:\n",
    "[\n",
    "  {\n",
    "    \"kpi_text\": \"Complete contextual description with the specific number\",\n",
    "    \"quantitative_value\": \"exact number only\",\n",
    "    \"unit\": \"unit of measurement\",\n",
    "    \"kpi_theme\": \"Environmental/Social/Governance\",\n",
    "    \"kpi_category\": \"specific category\",\n",
    "    \"time_period\": \"year/period if visible\"\n",
    "  }\n",
    "]\n",
    "\n",
    "COMPLETENESS IS CRITICAL - Extract everything quantifiable.\n",
    "\"\"\"\n",
    "\n",
    "def extract_kpi_optimized(image: Image.Image, page_number: int) -> List[Dict]:\n",
    "    \"\"\"Optimized KPI extraction\"\"\"\n",
    "    try:\n",
    "        # Check the cache\n",
    "        image_hash = fast_cache.get_image_hash(image)\n",
    "        cached_kpis = fast_cache.get_cached_kpis(image_hash)\n",
    "        if cached_kpis is not None:\n",
    "            for kpi in cached_kpis:\n",
    "                kpi['source_page'] = page_number\n",
    "            return cached_kpis\n",
    "\n",
    "        # Optimizing image encoding\n",
    "        base64_image = image_to_base64_optimized(image)\n",
    "        if not base64_image:\n",
    "            return []\n",
    "\n",
    "        # API Calls\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": COMPREHENSIVE_EXTRACTION_PROMPT},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                        \"detail\": \"high\"\n",
    "                    }}\n",
    "                ]\n",
    "            }],\n",
    "            temperature=0.0,\n",
    "            max_tokens=2000,\n",
    "            timeout=60\n",
    "        )\n",
    "\n",
    "        # Parsing results\n",
    "        kpis = parse_optimized_response(response, page_number)\n",
    "\n",
    "        # Caching results\n",
    "        fast_cache.cache_kpis(image_hash, kpis)\n",
    "\n",
    "        return kpis\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Optimized KPI extraction failed for page {page_number}: {e}\")\n",
    "        return []\n",
    "\n",
    "def image_to_base64_optimized(image: Image.Image) -> str:\n",
    "    \"\"\"Optimized image encoding\"\"\"\n",
    "    try:\n",
    "        max_dimension = 1400  # Maintain high quality\n",
    "        width, height = image.size\n",
    "\n",
    "        if max(width, height) > max_dimension:\n",
    "            scale = max_dimension / max(width, height)\n",
    "            new_size = (int(width * scale), int(height * scale))\n",
    "            image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "        if image.mode != 'RGB':\n",
    "            if image.mode in ['RGBA', 'LA']:\n",
    "                background = Image.new('RGB', image.size, (255, 255, 255))\n",
    "                if image.mode == 'RGBA':\n",
    "                    background.paste(image, mask=image.split()[-1])\n",
    "                else:\n",
    "                    background.paste(image)\n",
    "                image = background\n",
    "            else:\n",
    "                image = image.convert('RGB')\n",
    "\n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\", quality=92, optimize=True)\n",
    "        return base64.b64encode(buffered.getvalue()).decode()\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Optimized image encoding failed: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def parse_optimized_response(response, page_number: int) -> List[Dict]:\n",
    "    \"\"\"Optimized response parsing\"\"\"\n",
    "    try:\n",
    "        content = response.choices[0].message.content.strip()\n",
    "\n",
    "        if content.startswith('```json'):\n",
    "            content = content[7:]\n",
    "        if content.endswith('```'):\n",
    "            content = content[:-3]\n",
    "        content = content.strip()\n",
    "\n",
    "        if not content.startswith('['):\n",
    "            return []\n",
    "\n",
    "        result = json.loads(content)\n",
    "        if not isinstance(result, list):\n",
    "            return []\n",
    "\n",
    "        validated_kpis = []\n",
    "        for item in result:\n",
    "            if (isinstance(item, dict) and\n",
    "                item.get('kpi_text', '').strip() and\n",
    "                item.get('quantitative_value', '').strip()):\n",
    "\n",
    "                kpi = {\n",
    "                    'kpi_text': item.get('kpi_text', '').strip(),\n",
    "                    'quantitative_value': str(item.get('quantitative_value', '')).strip(),\n",
    "                    'unit': item.get('unit', '').strip(),\n",
    "                    'kpi_theme': item.get('kpi_theme', 'Environmental').strip(),\n",
    "                    'kpi_category': item.get('kpi_category', '').strip(),\n",
    "                    'time_period': item.get('time_period', '').strip(),\n",
    "                    'source_page': page_number,\n",
    "                    'source_type': 'image'\n",
    "                }\n",
    "                validated_kpis.append(kpi)\n",
    "\n",
    "        return validated_kpis\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Optimized response parsing failed: {e}\")\n",
    "        return []\n",
    "\n",
    "# 4. Parallel image processing\n",
    "def process_images_in_parallel(image_data: List[Dict], max_workers: int = 3) -> List[Dict]:\n",
    "    \"\"\"Parallel image processing\"\"\"\n",
    "    if not image_data:\n",
    "        return []\n",
    "\n",
    "    print(f\"🔄 Processing {len(image_data)} images in parallel...\")\n",
    "\n",
    "    all_kpis = []\n",
    "    completed_count = 0\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_info = {}\n",
    "        for img_info in image_data:\n",
    "            future = executor.submit(\n",
    "                extract_kpi_optimized,\n",
    "                img_info['image'],\n",
    "                img_info['page_number']\n",
    "            )\n",
    "            future_to_info[future] = img_info\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_info):\n",
    "            img_info = future_to_info[future]\n",
    "            try:\n",
    "                kpis = future.result(timeout=90)\n",
    "                all_kpis.extend(kpis)\n",
    "                completed_count += 1\n",
    "\n",
    "                if completed_count % 5 == 0:\n",
    "                    progress = completed_count / len(image_data) * 100\n",
    "                    print(f\"   📈 Progress: {completed_count}/{len(image_data)} ({progress:.1f}%)\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Image processing failed for page {img_info['page_number']}: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"📊 Parallel processing completed: {len(all_kpis)} KPIs extracted\")\n",
    "    print(f\"📋 {fast_cache.get_stats()}\")\n",
    "\n",
    "    return all_kpis"
   ],
   "metadata": {
    "id": "7j4_GQ6Llw8e"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "N7yh-bSwZonF",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9e7216e1-0002-47b5-ce36-f82be809cc08"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ============ Main processing function ============\n",
    "def process_sustainability_report_with_enhanced_images(pdf_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Main processing function with image analysis\"\"\"\n",
    "    logging.info(\"Starting enhanced PDF processing with image analysis...\")\n",
    "\n",
    "    # Step 1: Text and table extraction\n",
    "    logging.info(\"Step 1/5: Reading PDF text and tables...\")\n",
    "    full_text = pdf_to_text_and_tables(pdf_path)\n",
    "\n",
    "    camelot_tables = camelot_extra_tables_enhanced(pdf_path)\n",
    "    if camelot_tables:\n",
    "        full_text += \"\\n\\n\" + \"\\n\\n\".join(camelot_tables)\n",
    "\n",
    "    logging.info(\"Step 2/5: Chunking text...\")\n",
    "    chunks = split_into_chunks(full_text, MAX_TOKENS_CHUNK)\n",
    "\n",
    "    logging.info(\"Step 3/5: Extracting KPIs from text...\")\n",
    "    text_kpis = []\n",
    "    for idx, chunk in enumerate(chunks, 1):\n",
    "        logging.info(f\"Processing text chunk {idx}/{len(chunks)}\")\n",
    "        if chunk.strip():\n",
    "            chunk_kpis = extract_kpi_from_chunk_universal(chunk)\n",
    "            text_kpis.extend(chunk_kpis)\n",
    "            if idx < len(chunks):\n",
    "                time.sleep(SLEEP_SEC)\n",
    "\n",
    "    # Step 4: Image KPI extraction\n",
    "    logging.info(\"Step 4/5: Extracting KPIs from images...\")\n",
    "    image_kpis = process_pdf_images_for_kpis_fixed(pdf_path)\n",
    "\n",
    "    # Step 5: Combine and process\n",
    "    logging.info(\"Step 5/5: Combining and processing all KPIs...\")\n",
    "\n",
    "    for kpi in text_kpis:\n",
    "        if 'source_type' not in kpi:\n",
    "            kpi['source_type'] = 'text'\n",
    "\n",
    "    all_kpis = text_kpis + image_kpis\n",
    "    all_kpis = post_process_kpis_universal(all_kpis)\n",
    "\n",
    "    df_auto = pd.DataFrame(all_kpis)\n",
    "\n",
    "    if not df_auto.empty:\n",
    "        if 'source_type' not in df_auto.columns:\n",
    "            df_auto['source_type'] = 'text'\n",
    "\n",
    "        initial_count = len(df_auto)\n",
    "        df_auto = df_auto.drop_duplicates(subset=['kpi_text'], keep='first')\n",
    "        final_count = len(df_auto)\n",
    "\n",
    "        logging.info(f\"Removed {initial_count - final_count} duplicate KPIs\")\n",
    "\n",
    "        try:\n",
    "            df_auto = df_auto.sort_values(['source_type', 'kpi_theme', 'kpi_category'], na_position='last')\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "        text_kpi_count = len([kpi for kpi in all_kpis if kpi.get('source_type', 'text') != 'image'])\n",
    "        image_kpi_count = len([kpi for kpi in all_kpis if kpi.get('source_type') == 'image'])\n",
    "\n",
    "        logging.info(f\"KPI Summary: {text_kpi_count} from text/tables, {image_kpi_count} from images\")\n",
    "\n",
    "    return df_auto\n"
   ],
   "metadata": {
    "id": "Mr1OIB2BZosj"
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============ Optimize moderator processing function ============\n",
    "def process_sustainability_report_OPTIMIZED(pdf_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Optimize moderator processing functions - improve performance while ensuring integrity\"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"⚡ Starting OPTIMIZED processing with completeness guarantee...\")\n",
    "\n",
    "    try:\n",
    "        # Parallel text and image preprocessing\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "            print(\"🔄 Starting parallel text and image preprocessing...\")\n",
    "\n",
    "            # Text processing (using your existing logic)\n",
    "            def extract_text_kpis():\n",
    "                full_text = pdf_to_text_and_tables(pdf_path)\n",
    "                camelot_tables = camelot_extra_tables_enhanced(pdf_path)\n",
    "                if camelot_tables:\n",
    "                    full_text += \"\\n\\n\" + \"\\n\\n\".join(camelot_tables)\n",
    "\n",
    "                chunks = split_into_chunks(full_text, MAX_TOKENS_CHUNK)\n",
    "                text_kpis = []\n",
    "                for idx, chunk in enumerate(chunks, 1):\n",
    "                    if chunk.strip():\n",
    "                        chunk_kpis = extract_kpi_from_chunk_universal(chunk)\n",
    "                        text_kpis.extend(chunk_kpis)\n",
    "                        if idx < len(chunks):\n",
    "                            time.sleep(SLEEP_SEC)\n",
    "                return text_kpis\n",
    "\n",
    "            #Image preprocessing (using optimized filtering)\n",
    "            def extract_and_filter_images():\n",
    "                all_images = extract_images_from_pdf_fixed(pdf_path)\n",
    "                filtered_images = []\n",
    "\n",
    "                for img_info in all_images:\n",
    "                    should_process, reason = conservative_image_filter(img_info['image'])\n",
    "                    if should_process:\n",
    "                        filtered_images.append(img_info)\n",
    "\n",
    "                print(f\"📊 Conservative filtering: Kept {len(filtered_images)}/{len(all_images)} images\")\n",
    "                return filtered_images\n",
    "\n",
    "            text_future = executor.submit(extract_text_kpis)\n",
    "            image_future = executor.submit(extract_and_filter_images)\n",
    "\n",
    "            text_kpis = text_future.result()\n",
    "            image_data = image_future.result()\n",
    "\n",
    "        preprocessing_time = time.time() - start_time\n",
    "        print(f\"⏱️  Preprocessing completed in {preprocessing_time:.1f}s\")\n",
    "\n",
    "        # Parallel Image KPI Extraction\n",
    "        image_start = time.time()\n",
    "        image_kpis = process_images_in_parallel(image_data, max_workers=3)\n",
    "        image_time = time.time() - image_start\n",
    "        print(f\"⏱️  Image processing completed in {image_time:.1f}s\")\n",
    "\n",
    "        # Post-processing\n",
    "        all_kpis = text_kpis + image_kpis\n",
    "        all_kpis = post_process_kpis_universal(all_kpis)\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df_auto = pd.DataFrame(all_kpis)\n",
    "\n",
    "        if not df_auto.empty:\n",
    "            initial_count = len(df_auto)\n",
    "            df_auto = df_auto.drop_duplicates(subset=['kpi_text'], keep='first')\n",
    "            final_count = len(df_auto)\n",
    "\n",
    "            if 'source_type' not in df_auto.columns:\n",
    "                df_auto['source_type'] = 'text'\n",
    "\n",
    "            print(f\"🔄 Removed {initial_count - final_count} exact duplicates\")\n",
    "\n",
    "        # Performance Statistics\n",
    "        total_time = time.time() - start_time\n",
    "        text_count = len([k for k in all_kpis if k.get('source_type') != 'image'])\n",
    "        image_count = len([k for k in all_kpis if k.get('source_type') == 'image'])\n",
    "\n",
    "        print(f\"\\n⚡ OPTIMIZED processing completed!\")\n",
    "        print(f\"⏱️  Total time: {total_time:.1f}s ({total_time/60:.1f}min)\")\n",
    "        print(f\"📊 Results:\")\n",
    "        print(f\"   - Text/Tables: {text_count} KPIs\")\n",
    "        print(f\"   - Images/Charts: {image_count} KPIs\")\n",
    "        print(f\"   - Total unique: {len(df_auto)} KPIs\")\n",
    "        print(f\"⚡ Performance: {len(df_auto)/total_time:.1f} KPIs/second\")\n",
    "\n",
    "        return df_auto\n",
    "\n",
    "    except Exception as e:\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"❌ Optimized processing failed after {total_time:.1f}s: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()"
   ],
   "metadata": {
    "id": "YWPKhJAPmHg7"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============ Result saving and comparison functions ============\n",
    "def infer_stakeholder(row) -> str:\n",
    "    \"\"\"Infer affected stakeholders based on KPI theme and category\"\"\"\n",
    "    theme = row.get('kpi_theme', '').lower()\n",
    "    category = row.get('kpi_category', '').lower()\n",
    "    kpi_text = row.get('kpi_text', '').lower()\n",
    "\n",
    "    if theme == 'environmental':\n",
    "        return \"Environment, Community, Future Generations\"\n",
    "    elif theme == 'social':\n",
    "        if 'employee' in category or 'workforce' in category or 'gender' in category:\n",
    "            return \"Employees\"\n",
    "        elif 'customer' in category or 'safety' in category:\n",
    "            return \"Customers, Community\"\n",
    "        elif 'community' in category:\n",
    "            return \"Local Communities\"\n",
    "        elif 'supply' in category or 'supplier' in kpi_text:\n",
    "            return \"Suppliers, Business Partners\"\n",
    "        else:\n",
    "            return \"Employees, Community\"\n",
    "    elif theme == 'governance':\n",
    "        if 'board' in category:\n",
    "            return \"Shareholders, Investors\"\n",
    "        elif 'cyber' in category or 'data' in category:\n",
    "            return \"Customers, Employees, Business Partners\"\n",
    "        else:\n",
    "            return \"Shareholders, Investors, Stakeholders\"\n",
    "    else:\n",
    "        return \"All Stakeholders\"\n",
    "\n",
    "def save_results(df_auto: pd.DataFrame, output_path: str, pdf_path: str = \"\") -> None:\n",
    "    \"\"\"Save results to Excel file with proper formatting\"\"\"\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True)\n",
    "\n",
    "        if not df_auto.empty:\n",
    "            # Add metadata columns\n",
    "            pdf_filename = os.path.basename(pdf_path) if pdf_path else \"Unknown\"\n",
    "            df_auto['PDF file name'] = pdf_filename\n",
    "            df_auto['Title of the report'] = \"\"\n",
    "\n",
    "            if 'source_page' in df_auto.columns:\n",
    "                df_auto['Absolute Page Number'] = df_auto['source_page']\n",
    "                df_auto = df_auto.drop('source_page', axis=1)\n",
    "            else:\n",
    "                df_auto['Absolute Page Number'] = \"Unknown\"\n",
    "\n",
    "            df_auto['Impacted Stakeholder'] = df_auto.apply(infer_stakeholder, axis=1)\n",
    "\n",
    "            # Reorder columns\n",
    "            original_columns = [col for col in df_auto.columns if col not in\n",
    "                              ['PDF file name', 'Title of the report', 'Absolute Page Number', 'Impacted Stakeholder']]\n",
    "            new_column_order = ['PDF file name', 'Title of the report', 'Absolute Page Number', 'Impacted Stakeholder'] + original_columns\n",
    "            df_auto = df_auto[new_column_order]\n",
    "\n",
    "        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "            df_auto.to_excel(writer, sheet_name='Auto_KPIs', index=False)\n",
    "\n",
    "            if not df_auto.empty:\n",
    "                # Theme summary\n",
    "                theme_summary = df_auto.groupby('kpi_theme').size().reset_index(name='count')\n",
    "                theme_summary.to_excel(writer, sheet_name='Theme_Summary', index=False)\n",
    "\n",
    "                # Category summary\n",
    "                category_summary = df_auto.groupby(['kpi_theme', 'kpi_category']).size().reset_index(name='count')\n",
    "                category_summary.to_excel(writer, sheet_name='Category_Summary', index=False)\n",
    "\n",
    "        logging.info(f\"Results saved to {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving results: {e}\")\n",
    "\n",
    "def compare_with_manual_kpis(df_auto: pd.DataFrame, manual_xlsx_path: str) -> None:\n",
    "    \"\"\"Compare automatically extracted KPIs with manually annotated ones\"\"\"\n",
    "    if not os.path.exists(manual_xlsx_path):\n",
    "        logging.info(\"Manual KPI file not found, skipping comparison.\")\n",
    "        return\n",
    "\n",
    "    logging.info(\"Comparing with manual KPIs...\")\n",
    "\n",
    "    try:\n",
    "        df_manual = pd.read_excel(manual_xlsx_path)\n",
    "\n",
    "        if 'kpi_text' not in df_manual.columns:\n",
    "            logging.warning(\"Manual KPI file missing 'kpi_text' column\")\n",
    "            return\n",
    "\n",
    "        manual_kpis = set(df_manual['kpi_text'].astype(str).str.strip())\n",
    "        auto_kpis = set(df_auto['kpi_text'].astype(str).str.strip())\n",
    "\n",
    "        only_auto = auto_kpis - manual_kpis\n",
    "        only_manual = manual_kpis - auto_kpis\n",
    "        common = auto_kpis & manual_kpis\n",
    "\n",
    "        print(f\"\\n=== KPI Comparison Results ===\")\n",
    "        print(f\"Common KPIs: {len(common)}\")\n",
    "        print(f\"Only in automatic extraction: {len(only_auto)}\")\n",
    "        print(f\"Only in manual annotation: {len(only_manual)}\")\n",
    "\n",
    "        if only_auto:\n",
    "            print(f\"\\nKPIs found by model but not in manual annotation ({len(only_auto)}):\")\n",
    "            for kpi in sorted(only_auto):\n",
    "                if kpi.strip():\n",
    "                    print(f\"  - {kpi}\")\n",
    "\n",
    "        if only_manual:\n",
    "            print(f\"\\nKPIs in manual annotation but missed by model ({len(only_manual)}):\")\n",
    "            for kpi in sorted(only_manual):\n",
    "                if kpi.strip():\n",
    "                    print(f\"  - {kpi}\")\n",
    "\n",
    "        # Calculate metrics\n",
    "        if len(auto_kpis) > 0 and len(manual_kpis) > 0:\n",
    "            precision = len(common) / len(auto_kpis)\n",
    "            recall = len(common) / len(manual_kpis)\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "            print(f\"\\n=== Performance Metrics ===\")\n",
    "            print(f\"Precision: {precision:.3f}\")\n",
    "            print(f\"Recall: {recall:.3f}\")\n",
    "            print(f\"F1 Score: {f1_score:.3f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error comparing with manual KPIs: {e}\")"
   ],
   "metadata": {
    "id": "kucPzNTdZouV"
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============ Main execution function ============\n",
    "def main():\n",
    "    \"\"\"Enhanced main execution function with validation\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s: %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(PDF_PATH):\n",
    "            logging.error(f\"PDF file not found: {PDF_PATH}\")\n",
    "            return\n",
    "\n",
    "        # Process the PDF\n",
    "        df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
    "\n",
    "        # Save results\n",
    "        save_results(df_auto, EXPORT_AUTO_XLSX, PDF_PATH)\n",
    "        logging.info(f\"KPI extraction completed: {len(df_auto)} KPIs extracted\")\n",
    "\n",
    "        # Enhanced validation with comprehensive analysis\n",
    "        if MANUAL_XLSX and Path(MANUAL_XLSX).exists():\n",
    "            print(\"\\n🔍 Running comprehensive validation...\")\n",
    "            validation_results = enhanced_compare_with_manual_kpis(\n",
    "                df_auto, MANUAL_XLSX, \"comprehensive_validation\"\n",
    "            )\n",
    "\n",
    "            if validation_results:\n",
    "                print(\"✅ Validation completed with detailed analysis!\")\n",
    "                print(f\"📁 Detailed results saved to: comprehensive_validation/\")\n",
    "            else:\n",
    "                print(\"⚠️ Validation encountered issues\")\n",
    "        else:\n",
    "            logging.info(\"Manual KPI file not found, skipping validation.\")\n",
    "\n",
    "        # Display summary\n",
    "        if not df_auto.empty:\n",
    "            print(f\"\\n=== Extraction Summary ===\")\n",
    "            print(f\"Total KPIs extracted: {len(df_auto)}\")\n",
    "\n",
    "            # Source statistics\n",
    "            if 'source_type' in df_auto.columns:\n",
    "                source_counts = df_auto['source_type'].value_counts()\n",
    "                print(f\"From text/tables: {source_counts.get('text', 0)}\")\n",
    "                print(f\"From images/charts: {source_counts.get('image', 0)}\")\n",
    "\n",
    "            # Theme statistics\n",
    "            if 'kpi_theme' in df_auto.columns:\n",
    "                theme_counts = df_auto['kpi_theme'].value_counts()\n",
    "                print(f\"\\nKPI Distribution by Theme:\")\n",
    "                for theme, count in theme_counts.items():\n",
    "                    print(f\"  {theme}: {count}\")\n",
    "        else:\n",
    "            print(\"\\nNo KPIs were extracted from the document.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    # logging.basicConfig(\n",
    "    #     level=logging.INFO,\n",
    "    #     format=\"%(asctime)s - %(levelname)s: %(message)s\",\n",
    "    #     datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    # )\n",
    "\n",
    "    # try:\n",
    "    #     if not os.path.exists(PDF_PATH):\n",
    "    #         logging.error(f\"PDF file not found: {PDF_PATH}\")\n",
    "    #         return\n",
    "\n",
    "    #     # Process the PDF\n",
    "    #     df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
    "\n",
    "    #     # Save results\n",
    "    #     save_results(df_auto, EXPORT_AUTO_XLSX, PDF_PATH)\n",
    "\n",
    "    #     logging.info(f\"KPI extraction completed: {len(df_auto)} KPIs extracted\")\n",
    "\n",
    "    #     # Compare with manual annotations if available\n",
    "    #     if MANUAL_XLSX:\n",
    "    #         compare_with_manual_kpis(df_auto, MANUAL_XLSX)\n",
    "\n",
    "    #     # Display summary\n",
    "    #     if not df_auto.empty:\n",
    "    #         print(f\"\\n=== Extraction Summary ===\")\n",
    "    #         print(f\"Total KPIs extracted: {len(df_auto)}\")\n",
    "\n",
    "    #         # Source statistics\n",
    "    #         if 'source_type' in df_auto.columns:\n",
    "    #             source_counts = df_auto['source_type'].value_counts()\n",
    "    #             print(f\"From text/tables: {source_counts.get('text', 0)}\")\n",
    "    #             print(f\"From images/charts: {source_counts.get('image', 0)}\")\n",
    "\n",
    "    #         # Theme statistics\n",
    "    #         if 'kpi_theme' in df_auto.columns:\n",
    "    #             theme_counts = df_auto['kpi_theme'].value_counts()\n",
    "    #             print(f\"\\nKPI Distribution by Theme:\")\n",
    "    #             for theme, count in theme_counts.items():\n",
    "    #                 print(f\"  {theme}: {count}\")\n",
    "    #     else:\n",
    "    #         print(\"\\nNo KPIs were extracted from the document.\")\n",
    "\n",
    "    # except Exception as e:\n",
    "    #     logging.error(f\"Error in main execution: {e}\")\n",
    "    #     import traceback\n",
    "    #     traceback.print_exc()"
   ],
   "metadata": {
    "id": "uSueutkDZowK"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "vOmur6_l5p6R"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============ Auxiliary functions ============\n",
    "def install_dependencies():\n",
    "    \"\"\"Install required dependencies\"\"\"\n",
    "    try:\n",
    "        import subprocess\n",
    "        import sys\n",
    "\n",
    "        dependencies = [\n",
    "            \"openai\",\n",
    "            \"python-dotenv\",\n",
    "            \"pdfplumber\",\n",
    "            \"tiktoken\",\n",
    "            \"pandas\",\n",
    "            \"PyMuPDF\",\n",
    "            \"Pillow\",\n",
    "            \"openpyxl\"\n",
    "        ]\n",
    "\n",
    "        for dep in dependencies:\n",
    "            try:\n",
    "                __import__(dep.replace('-', '_'))\n",
    "                print(f\"✅ {dep} is already installed\")\n",
    "            except ImportError:\n",
    "                print(f\"Installing {dep}...\")\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", dep])\n",
    "                print(f\"✅ Installed {dep}\")\n",
    "\n",
    "        # Optional Camelot installation\n",
    "        try:\n",
    "            import camelot\n",
    "            print(\"✅ Camelot is already installed\")\n",
    "        except ImportError:\n",
    "            print(\"Installing Camelot (optional)...\")\n",
    "            try:\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"camelot-py[cv]\"])\n",
    "                print(\"✅ Installed Camelot\")\n",
    "            except:\n",
    "                print(\"⚠️ Camelot installation failed (optional dependency)\")\n",
    "\n",
    "        print(\"🎉 All dependencies checked/installed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error with dependencies: {e}\")\n",
    "\n",
    "def validate_environment():\n",
    "    \"\"\"Validate environment setup\"\"\"\n",
    "    issues = []\n",
    "\n",
    "    # Check API key\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        issues.append(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "    # Check PDF file\n",
    "    if not os.path.exists(PDF_PATH):\n",
    "        issues.append(f\"PDF file not found: {PDF_PATH}\")\n",
    "\n",
    "    # Check required imports\n",
    "    required_modules = ['openai', 'pdfplumber', 'pandas', 'tiktoken', 'PIL', 'fitz']\n",
    "    for module in required_modules:\n",
    "        try:\n",
    "            __import__(module)\n",
    "        except ImportError:\n",
    "            issues.append(f\"Required module '{module}' not installed\")\n",
    "\n",
    "    if issues:\n",
    "        print(\"❌ Environment validation failed:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  - {issue}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"✅ Environment validation passed\")\n",
    "        return True\n"
   ],
   "metadata": {
    "id": "qcCShhlea6ZK"
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============ Simplified execution interface ============\n",
    "def run_kpi_extraction():\n",
    "    \"\"\"Simplified interface to run KPI extraction\"\"\"\n",
    "    print(\"🚀 Starting KPI extraction process...\")\n",
    "\n",
    "    # Validate environment\n",
    "    if not validate_environment():\n",
    "        print(\"Please fix the environment issues before running.\")\n",
    "        return\n",
    "\n",
    "    # Run main function\n",
    "    main()"
   ],
   "metadata": {
    "id": "Qw6B5H8ga6bl"
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============ Optimized execution interface ============\n",
    "def run_optimized_kpi_extraction():\n",
    "    \"\"\"Run optimized KPI extraction\"\"\"\n",
    "    print(\"⚡ Starting OPTIMIZED KPI extraction...\")\n",
    "    print(\"🎯 Goal: Extract ALL KPIs with 60-70% better performance\")\n",
    "\n",
    "    # Verify the environment\n",
    "    if not validate_environment():\n",
    "        print(\"Please fix the environment issues before running.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Run optimization process\n",
    "        df_results = process_sustainability_report_OPTIMIZED(PDF_PATH)\n",
    "\n",
    "        # Save the results\n",
    "        output_file = \"OPTIMIZED_\" + EXPORT_AUTO_XLSX\n",
    "        save_results(df_results, output_file, PDF_PATH)\n",
    "        print(f\"💾 Results saved to: {output_file}\")\n",
    "\n",
    "        # Show Statistics\n",
    "        if not df_results.empty and 'source_type' in df_results.columns:\n",
    "            source_counts = df_results['source_type'].value_counts()\n",
    "            print(f\"\\n📈 Final Statistics:\")\n",
    "            for source, count in source_counts.items():\n",
    "                print(f\"   - {source}: {count} KPIs\")\n",
    "\n",
    "        return df_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Optimized extraction failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def compare_original_vs_optimized():\n",
    "    \"\"\"Compare the performance of the original version and the optimized version\"\"\"\n",
    "    print(\"🔬 Performance Comparison Test\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Test the original version\n",
    "    print(\"\\n📊 Testing Original Version...\")\n",
    "    original_start = time.time()\n",
    "    try:\n",
    "        original_df = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
    "        original_time = time.time() - original_start\n",
    "        print(f\"⏱️  Original version: {original_time:.1f}s, {len(original_df)} KPIs\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Original version failed: {e}\")\n",
    "        original_time = 999\n",
    "        original_df = pd.DataFrame()\n",
    "\n",
    "    # Test optimized version\n",
    "    print(\"\\n⚡ Testing Optimized Version...\")\n",
    "    optimized_start = time.time()\n",
    "    try:\n",
    "        optimized_df = process_sustainability_report_OPTIMIZED(PDF_PATH)\n",
    "        optimized_time = time.time() - optimized_start\n",
    "        print(f\"⏱️  Optimized version: {optimized_time:.1f}s, {len(optimized_df)} KPIs\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Optimized version failed: {e}\")\n",
    "        optimized_time = 999\n",
    "        optimized_df = pd.DataFrame()\n",
    "\n",
    "    # Performance comparison\n",
    "    if original_time < 999 and optimized_time < 999:\n",
    "        speedup = original_time / optimized_time\n",
    "        time_saved = original_time - optimized_time\n",
    "        kpi_diff = abs(len(optimized_df) - len(original_df))\n",
    "\n",
    "        print(f\"\\n🚀 Performance Results:\")\n",
    "        print(f\"   - Speed improvement: {speedup:.1f}x faster\")\n",
    "        print(f\"   - Time saved: {time_saved:.1f}s ({time_saved/60:.1f}min)\")\n",
    "        print(f\"   - KPI difference: {kpi_diff} KPIs\")\n",
    "        print(f\"   - Completeness: {len(optimized_df)/len(original_df)*100:.1f}% of original\" if len(original_df) > 0 else \"\")\n",
    "\n",
    "        return {\"original\": original_df, \"optimized\": optimized_df, \"speedup\": speedup}\n",
    "\n",
    "    return None"
   ],
   "metadata": {
    "id": "aJrVHLTgmPJw"
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============ Debug and test functions ============\n",
    "def test_text_extraction_only():\n",
    "    \"\"\"Test only text extraction without images\"\"\"\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    try:\n",
    "        # Extract text and tables\n",
    "        full_text = pdf_to_text_and_tables(PDF_PATH)\n",
    "        camelot_tables = camelot_extra_tables_enhanced(PDF_PATH)\n",
    "\n",
    "        if camelot_tables:\n",
    "            full_text += \"\\n\\n\" + \"\\n\\n\".join(camelot_tables)\n",
    "\n",
    "        # Chunk text\n",
    "        chunks = split_into_chunks(full_text, MAX_TOKENS_CHUNK)\n",
    "\n",
    "        # Extract KPIs from first few chunks\n",
    "        test_kpis = []\n",
    "        for idx, chunk in enumerate(chunks[:3]):  # Test first 3 chunks\n",
    "            chunk_kpis = extract_kpi_from_chunk_universal(chunk)\n",
    "            test_kpis.extend(chunk_kpis)\n",
    "            time.sleep(SLEEP_SEC)\n",
    "\n",
    "        print(f\"Test extraction completed: {len(test_kpis)} KPIs found in first 3 chunks\")\n",
    "\n",
    "        for i, kpi in enumerate(test_kpis[:5]):  # Show first 5\n",
    "            print(f\"{i+1}. {kpi.get('kpi_text', 'No text')}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Test failed: {e}\")\n",
    "\n",
    "def debug_single_image_analysis(image_path: str):\n",
    "    \"\"\"Test single image analysis functionality\"\"\"\n",
    "    try:\n",
    "        from PIL import Image\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        print(f\"Analyzing image: {image_path}\")\n",
    "        print(f\"Image size: {image.width}x{image.height}\")\n",
    "\n",
    "        kpis = extract_kpi_from_image_fixed(image, 1)\n",
    "\n",
    "        print(f\"\\n=== Analysis Results ===\")\n",
    "        print(f\"Found {len(kpis)} KPIs:\")\n",
    "\n",
    "        for i, kpi in enumerate(kpis, 1):\n",
    "            print(f\"\\n{i}. {kpi.get('kpi_text', 'No text')}\")\n",
    "            print(f\"   Value: {kpi.get('quantitative_value', 'No value')}\")\n",
    "            print(f\"   Confidence: {kpi.get('estimation_confidence', 'Not specified')}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in debug analysis: {e}\")\n",
    "\n",
    "def process_text_only():\n",
    "    \"\"\"Process only text and tables, skip images\"\"\"\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    try:\n",
    "        logging.info(\"Starting text-only processing...\")\n",
    "\n",
    "        # Step 1: Text and table extraction\n",
    "        full_text = pdf_to_text_and_tables(PDF_PATH)\n",
    "        camelot_tables = camelot_extra_tables_enhanced(PDF_PATH)\n",
    "\n",
    "        if camelot_tables:\n",
    "            full_text += \"\\n\\n\" + \"\\n\\n\".join(camelot_tables)\n",
    "\n",
    "        # Step 2: Chunking\n",
    "        chunks = split_into_chunks(full_text, MAX_TOKENS_CHUNK)\n",
    "\n",
    "        # Step 3: Extract KPIs\n",
    "        all_kpis = []\n",
    "        for idx, chunk in enumerate(chunks, 1):\n",
    "            logging.info(f\"Processing chunk {idx}/{len(chunks)}\")\n",
    "            if chunk.strip():\n",
    "                chunk_kpis = extract_kpi_from_chunk_universal(chunk)\n",
    "                all_kpis.extend(chunk_kpis)\n",
    "                if idx < len(chunks):\n",
    "                    time.sleep(SLEEP_SEC)\n",
    "\n",
    "        # Post-processing\n",
    "        all_kpis = post_process_kpis_universal(all_kpis)\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df_auto = pd.DataFrame(all_kpis)\n",
    "\n",
    "        if not df_auto.empty:\n",
    "            df_auto = df_auto.drop_duplicates(subset=['kpi_text'], keep='first')\n",
    "\n",
    "        # Save results\n",
    "        text_only_output = \"text_only_\" + EXPORT_AUTO_XLSX\n",
    "        save_results(df_auto, text_only_output, PDF_PATH)\n",
    "\n",
    "        print(f\"Text-only processing completed: {len(df_auto)} KPIs extracted\")\n",
    "\n",
    "        return df_auto\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Text-only processing failed: {e}\")\n",
    "        return pd.DataFrame()"
   ],
   "metadata": {
    "id": "Oku3umuia6dK"
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============ 兼容性函数 ============\n",
    "def extract_kpi_from_chunk(chunk: str) -> List[Dict]:\n",
    "    \"\"\"Backward compatibility function\"\"\"\n",
    "    return extract_kpi_from_chunk_universal(chunk)\n",
    "\n",
    "def process_sustainability_report(pdf_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Backward compatibility function for text-only processing\"\"\"\n",
    "    return process_text_only()\n",
    "\n",
    "def process_sustainability_report_with_images(pdf_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Backward compatibility function for full processing\"\"\"\n",
    "    return process_sustainability_report_with_enhanced_images(pdf_path)\n"
   ],
   "metadata": {
    "id": "uhyRnexga6fA"
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============ 使用示例 ============\n",
    "def example_usage():\n",
    "    \"\"\"Usage examples\"\"\"\n",
    "    print(\"=== KPI Extraction Tool Usage Examples ===\\n\")\n",
    "\n",
    "    print(\"1. Full extraction (text + images):\")\n",
    "    print(\"   df_results = process_sustainability_report_with_enhanced_images(PDF_PATH)\")\n",
    "    print(\"   save_results(df_results, EXPORT_AUTO_XLSX, PDF_PATH)\\n\")\n",
    "\n",
    "    print(\"2. Text-only extraction:\")\n",
    "    print(\"   df_results = process_text_only()\")\n",
    "    print(\"   # Results automatically saved\\n\")\n",
    "\n",
    "    print(\"3. Simple run:\")\n",
    "    print(\"   run_kpi_extraction()  # Complete pipeline with validation\\n\")\n",
    "\n",
    "    print(\"4. Debug single component:\")\n",
    "    print(\"   test_text_extraction_only()  # Test first 3 chunks\")\n",
    "    print(\"   debug_single_image_analysis('path/to/image.jpg')\\n\")\n",
    "\n",
    "    print(\"5. Install dependencies:\")\n",
    "    print(\"   install_dependencies()  # Install all required packages\\n\")\n"
   ],
   "metadata": {
    "id": "fldjMEkaa6g1"
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# Debug code - Copy and paste directly to the end of your code\n",
    "# ============================================================================\n",
    "\n",
    "# Method 1: Check all image extraction and recognition status\n",
    "def debug_method_1_check_image_detection():\n",
    "   \"\"\"Check if all images in PDF are correctly extracted and if chart classifier is working properly\"\"\"\n",
    "   print(\"=== Method 1: Check Image Extraction and Chart Recognition ===\")\n",
    "\n",
    "   # Create debug folder\n",
    "   import os\n",
    "   debug_folder = \"debug_images_method1\"\n",
    "   os.makedirs(debug_folder, exist_ok=True)\n",
    "\n",
    "   try:\n",
    "       # Extract all images\n",
    "       images = extract_images_from_pdf_fixed(PDF_PATH)\n",
    "       print(f\"Extracted {len(images)} images from PDF\")\n",
    "\n",
    "       chart_count = 0\n",
    "       non_chart_count = 0\n",
    "\n",
    "       for i, img_info in enumerate(images):\n",
    "           page_num = img_info['page_number']\n",
    "           img_type = img_info['type']\n",
    "           image = img_info['image']\n",
    "\n",
    "           # Check if recognized as chart\n",
    "           is_chart = is_chart_image(image)\n",
    "\n",
    "           # Save image with recognition result in filename\n",
    "           chart_status = \"CHART\" if is_chart else \"NOT_CHART\"\n",
    "           filename = f\"{debug_folder}/page_{page_num}_{img_type}_{chart_status}_{i}.jpg\"\n",
    "           image.save(filename)\n",
    "\n",
    "           print(f\"Image {i+1}: Page {page_num}, Type {img_type}, Size {image.width}x{image.height}, Chart Recognition: {is_chart}\")\n",
    "\n",
    "           if is_chart:\n",
    "               chart_count += 1\n",
    "           else:\n",
    "               non_chart_count += 1\n",
    "\n",
    "       print(f\"\\nSummary:\")\n",
    "       print(f\"- Images recognized as charts: {chart_count}\")\n",
    "       print(f\"- Images not recognized as charts: {non_chart_count}\")\n",
    "       print(f\"- All images saved to {debug_folder} folder\")\n",
    "       print(f\"- Please manually check NOT_CHART images to see if they contain your missing pie charts\")\n",
    "\n",
    "       return images\n",
    "\n",
    "   except Exception as e:\n",
    "       print(f\"Method 1 execution error: {e}\")\n",
    "       import traceback\n",
    "       traceback.print_exc()\n",
    "       return []\n",
    "\n",
    "# Method 2: Temporarily disable chart classifier\n",
    "def debug_method_2_bypass_chart_filter():\n",
    "   \"\"\"Completely disable chart classifier, force processing all images\"\"\"\n",
    "   print(\"=== Method 2: Disable Chart Classifier ===\")\n",
    "\n",
    "   # Save original chart classifier function\n",
    "   global is_chart_image\n",
    "   original_chart_classifier = is_chart_image\n",
    "\n",
    "   # Create new classifier (always returns True)\n",
    "   def bypass_chart_classifier(image):\n",
    "       print(f\"  🔓 Force processing image (size: {image.width}x{image.height})\")\n",
    "       return True\n",
    "\n",
    "   # Temporarily replace classifier\n",
    "   is_chart_image = bypass_chart_classifier\n",
    "\n",
    "   try:\n",
    "       print(\"Starting to re-extract image KPIs (chart filtering disabled)...\")\n",
    "\n",
    "       # Re-run image processing\n",
    "       image_kpis = process_pdf_images_for_kpis_fixed(PDF_PATH)\n",
    "\n",
    "       print(f\"Extracted {len(image_kpis)} image KPIs after disabling filter\")\n",
    "\n",
    "       # Display results\n",
    "       for i, kpi in enumerate(image_kpis):\n",
    "           print(f\"{i+1}. Page {kpi.get('source_page', 'Unknown')}: {kpi.get('kpi_text', 'No text')[:100]}\")\n",
    "\n",
    "       return image_kpis\n",
    "\n",
    "   except Exception as e:\n",
    "       print(f\"Method 2 execution error: {e}\")\n",
    "       import traceback\n",
    "       traceback.print_exc()\n",
    "       return []\n",
    "\n",
    "   finally:\n",
    "       # Restore original classifier\n",
    "       is_chart_image = original_chart_classifier\n",
    "       print(\"Restored original chart classifier\")\n",
    "\n",
    "# Method 3: Manual test specific images\n",
    "def debug_method_3_manual_test():\n",
    "   \"\"\"Manually select images for testing\"\"\"\n",
    "   print(\"=== Method 3: Manual Test Specific Images ===\")\n",
    "\n",
    "   try:\n",
    "       images = extract_images_from_pdf_fixed(PDF_PATH)\n",
    "       print(f\"Found {len(images)} images\")\n",
    "\n",
    "       # Display all image information\n",
    "       for i, img_info in enumerate(images):\n",
    "           page_num = img_info['page_number']\n",
    "           img_type = img_info['type']\n",
    "           image = img_info['image']\n",
    "           is_chart = is_chart_image(image)\n",
    "\n",
    "           print(f\"{i+1}. Page {page_num}, Type {img_type}, Size {image.width}x{image.height}, Chart: {is_chart}\")\n",
    "\n",
    "       # Let user select image to test\n",
    "       while True:\n",
    "           try:\n",
    "               choice = input(f\"\\nPlease select image number to test (1-{len(images)}, enter 0 to exit): \")\n",
    "               if choice == '0':\n",
    "                   break\n",
    "\n",
    "               img_index = int(choice) - 1\n",
    "               if 0 <= img_index < len(images):\n",
    "                   img_info = images[img_index]\n",
    "                   page_num = img_info['page_number']\n",
    "                   image = img_info['image']\n",
    "\n",
    "                   print(f\"\\nTesting image {choice} (page {page_num})\")\n",
    "\n",
    "                   # Save this image for inspection\n",
    "                   test_filename = f\"test_image_{choice}_page_{page_num}.jpg\"\n",
    "                   image.save(test_filename)\n",
    "                   print(f\"Image saved as: {test_filename}\")\n",
    "\n",
    "                   # Test extraction\n",
    "                   kpis = extract_kpi_from_image_fixed(image, page_num, \"manual_test\")\n",
    "\n",
    "                   print(f\"Extraction result: {len(kpis)} KPIs\")\n",
    "                   for j, kpi in enumerate(kpis):\n",
    "                       print(f\"  KPI {j+1}: {kpi.get('kpi_text', 'No text')}\")\n",
    "\n",
    "               else:\n",
    "                   print(\"Invalid selection\")\n",
    "\n",
    "           except ValueError:\n",
    "               print(\"Please enter a valid number\")\n",
    "           except KeyboardInterrupt:\n",
    "               break\n",
    "           except Exception as e:\n",
    "               print(f\"Test error: {e}\")\n",
    "\n",
    "   except Exception as e:\n",
    "       print(f\"Method 3 execution error: {e}\")\n",
    "       import traceback\n",
    "       traceback.print_exc()\n",
    "\n",
    "# Method 4: Simplified extraction test\n",
    "def debug_method_4_simple_test():\n",
    "   \"\"\"Use simplified method to test image extraction\"\"\"\n",
    "   print(\"=== Method 4: Simplified Extraction Test ===\")\n",
    "\n",
    "   simple_prompt = \"\"\"请分析这个图表，提取所有的数字数据。\n",
    "\n",
    "Return JSON format, each data point contains:\n",
    "{\n",
    " \"description\": \"Data description\",\n",
    " \"value\": \"Numerical value\",\n",
    " \"unit\": \"Unit\"\n",
    "}\n",
    "\n",
    "If it's a pie chart, please extract the percentage of each slice.\n",
    "If it's a bar chart, please extract the value of each bar.\n",
    "If it's a table, please extract each number.\"\"\"\n",
    "\n",
    "   try:\n",
    "       images = extract_images_from_pdf_fixed(PDF_PATH)\n",
    "\n",
    "       for i, img_info in enumerate(images[:5]):  # Only test first 5 images\n",
    "           page_num = img_info['page_number']\n",
    "           image = img_info['image']\n",
    "\n",
    "           print(f\"\\n🔍 Simplified test image {i+1} (page {page_num})\")\n",
    "\n",
    "           try:\n",
    "               base64_image = image_to_base64_fixed(image)\n",
    "               if not base64_image:\n",
    "                   continue\n",
    "\n",
    "               response = client.chat.completions.create(\n",
    "                   model=\"gpt-4o\",\n",
    "                   messages=[{\n",
    "                       \"role\": \"user\",\n",
    "                       \"content\": [\n",
    "                           {\"type\": \"text\", \"text\": simple_prompt},\n",
    "                           {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "                       ]\n",
    "                   }],\n",
    "                   temperature=0.0,\n",
    "                   max_tokens=2000,\n",
    "                   timeout=30\n",
    "               )\n",
    "\n",
    "               content = response.choices[0].message.content.strip()\n",
    "               print(f\"API response: {content[:300]}...\")\n",
    "\n",
    "               # Check if contains your target data\n",
    "               if \"property type\" in content.lower() or \"service type\" in content.lower():\n",
    "                   print(\"🎉 Possibly found missing pie chart data!\")\n",
    "                   print(f\"Complete response: {content}\")\n",
    "\n",
    "           except Exception as e:\n",
    "               print(f\"❌ Simplified test failed: {e}\")\n",
    "\n",
    "           time.sleep(1)\n",
    "\n",
    "   except Exception as e:\n",
    "       print(f\"Method 4 execution error: {e}\")\n",
    "       import traceback\n",
    "       traceback.print_exc()\n",
    "\n",
    "# Debug main control function\n",
    "def run_debugging_session():\n",
    "   \"\"\"Debug session main control function\"\"\"\n",
    "   print(\"🔧 KPI Extraction Debug Session\")\n",
    "   print(\"=\" * 50)\n",
    "\n",
    "   while True:\n",
    "       try:\n",
    "           choice = input(\"\"\"\\nSelect debug method:\n",
    "1 - Check all image extraction and recognition status\n",
    "2 - Disable chart classifier, force processing all images\n",
    "3 - Manually select images for testing\n",
    "4 - Simplified API test (test first 5 images)\n",
    "0 - Exit debug\n",
    "\n",
    "Please enter selection (0-4): \"\"\")\n",
    "\n",
    "           if choice == \"1\":\n",
    "               debug_method_1_check_image_detection()\n",
    "           elif choice == \"2\":\n",
    "               debug_method_2_bypass_chart_filter()\n",
    "           elif choice == \"3\":\n",
    "               debug_method_3_manual_test()\n",
    "           elif choice == \"4\":\n",
    "               debug_method_4_simple_test()\n",
    "           elif choice == \"0\":\n",
    "               print(\"Exit debug session\")\n",
    "               break\n",
    "           else:\n",
    "               print(\"Invalid selection, please try again\")\n",
    "\n",
    "       except KeyboardInterrupt:\n",
    "           print(\"\\nUser interrupted, exit debug\")\n",
    "           break\n",
    "       except Exception as e:\n",
    "           print(f\"Debug session error: {e}\")\n",
    "           import traceback\n",
    "           traceback.print_exc()\n",
    "\n",
    "# ============================================================================\n",
    "# Standalone quick test function (if you don't want to use interactive interface)\n",
    "# ============================================================================\n",
    "\n",
    "def quick_debug():\n",
    "   \"\"\"Quick debug - directly run method 1\"\"\"\n",
    "   print(\"🚀 Quick debug mode\")\n",
    "   debug_method_1_check_image_detection()\n",
    "\n",
    "# ============================================================================\n",
    "# Usage\n",
    "# ============================================================================\n",
    "\n",
    "# At the end of your code, you can now run any of the following:\n",
    "\n",
    "# Option 1: Interactive debug (recommended)\n",
    "# run_debugging_session()\n",
    "\n",
    "# Option 2: Quick debug, directly check images\n",
    "# quick_debug()\n",
    "\n",
    "# Option 3: Directly run specific method\n",
    "# debug_method_1_check_image_detection()"
   ],
   "metadata": {
    "id": "zXuWkRTCgfWE"
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Test function: Verify if the new prompt is effective\n",
    "def test_improved_prompt():\n",
    "   \"\"\"Test if the improved prompt can correctly extract percentages\"\"\"\n",
    "   print(\"=== Testing Improved Prompt ===\")\n",
    "\n",
    "   try:\n",
    "       # Extract full page image from page 2\n",
    "       images = extract_images_from_pdf_fixed(PDF_PATH)\n",
    "       page2_image = None\n",
    "\n",
    "       for img_info in images:\n",
    "           if img_info['page_number'] == 2 and img_info['type'] == 'full_page':\n",
    "               page2_image = img_info['image']\n",
    "               break\n",
    "\n",
    "       if page2_image is None:\n",
    "           print(\"❌ Could not find page 2 image\")\n",
    "           return\n",
    "\n",
    "       print(f\"✅ Found page 2 image, size: {page2_image.width}x{page2_image.height}\")\n",
    "\n",
    "       # Test using improved prompt\n",
    "       base64_image = image_to_base64_fixed(page2_image)\n",
    "\n",
    "       print(\"🔄 Calling API with improved prompt...\")\n",
    "       response = client.chat.completions.create(\n",
    "           model=\"gpt-4o\",\n",
    "           messages=[\n",
    "               {\"role\": \"system\", \"content\": ENHANCED_IMAGE_KPI_SYSTEM_PROMPT},\n",
    "               {\"role\": \"user\", \"content\": [\n",
    "                   {\"type\": \"text\", \"text\": \"\"\"分析这个页面，重点关注两个饼图：\n",
    "\n",
    "1. 上方饼图：\"Energy Use by Property Type 2021\"\n",
    "2. 下方饼图：\"Energy Use by Service Type 2021\"\n",
    "\n",
    "请提取每个饼图中每个扇形的具体百分比数值。确保包含实际的数字，不只是描述。\"\"\"},\n",
    "                   {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\", \"detail\": \"high\"}}\n",
    "               ]}\n",
    "           ],\n",
    "           temperature=0.0,\n",
    "           max_tokens=4000,\n",
    "           timeout=90\n",
    "       )\n",
    "\n",
    "       content = response.choices[0].message.content.strip()\n",
    "       print(\"\\n📋 API Response:\")\n",
    "       print(content[:500] + \"...\" if len(content) > 500 else content)\n",
    "\n",
    "       # Try to parse JSON\n",
    "       try:\n",
    "           if content.startswith('```json'):\n",
    "               content = content[7:]\n",
    "           if content.endswith('```'):\n",
    "               content = content[:-3]\n",
    "           content = content.strip()\n",
    "\n",
    "           if content.startswith('['):\n",
    "               result = json.loads(content)\n",
    "               print(f\"\\n✅ Successfully parsed JSON, found {len(result)} KPIs\")\n",
    "\n",
    "               # Check if specific percentages were extracted\n",
    "               found_percentages = []\n",
    "               for kpi in result:\n",
    "                   if isinstance(kpi, dict):\n",
    "                       kpi_text = kpi.get('kpi_text', '')\n",
    "                       quantitative_value = kpi.get('quantitative_value', '')\n",
    "\n",
    "                       print(f\"KPI: {kpi_text}\")\n",
    "                       print(f\"  Value: {quantitative_value}\")\n",
    "\n",
    "                       # Check if contains expected percentages\n",
    "                       if any(target in kpi_text.lower() for target in ['64%', '33%', '68%', '30%', 'healthcare center', 'medical office', 'electricity', 'fuel']):\n",
    "                           found_percentages.append(kpi)\n",
    "                           print(f\"  🎯 Found target data!\")\n",
    "                       print()\n",
    "\n",
    "               if found_percentages:\n",
    "                   print(f\"🎉 Successfully extracted {len(found_percentages)} KPIs with specific percentages!\")\n",
    "                   return True\n",
    "               else:\n",
    "                   print(\"❌ Still could not extract specific percentage values\")\n",
    "                   return False\n",
    "           else:\n",
    "               print(\"❌ API response is not in JSON format\")\n",
    "               return False\n",
    "\n",
    "       except json.JSONDecodeError as e:\n",
    "           print(f\"❌ JSON parsing failed: {e}\")\n",
    "           return False\n",
    "\n",
    "   except Exception as e:\n",
    "       print(f\"❌ Test failed: {e}\")\n",
    "       import traceback\n",
    "       traceback.print_exc()\n",
    "       return False\n",
    "\n",
    "# Quick fix function: Replace prompt and re-run extraction\n",
    "def quick_fix_and_rerun():\n",
    "   \"\"\"Apply fix and re-run complete extraction process\"\"\"\n",
    "   print(\"🔧 Applying fix and re-running...\")\n",
    "\n",
    "   # First test new prompt\n",
    "   if test_improved_prompt():\n",
    "       print(\"\\n✅ New prompt test successful!\")\n",
    "\n",
    "       # Re-run complete extraction process\n",
    "       print(\"\\n🔄 Re-running complete KPI extraction...\")\n",
    "       try:\n",
    "           df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
    "\n",
    "           # Save results\n",
    "           save_results(df_auto, \"fixed_\" + EXPORT_AUTO_XLSX, PDF_PATH)\n",
    "\n",
    "           print(f\"\\n🎉 Fix completed! Total extracted: {len(df_auto)} KPIs\")\n",
    "           print(\"Results saved to fixed_\" + EXPORT_AUTO_XLSX)\n",
    "\n",
    "           # Show pie chart related KPIs\n",
    "           pie_chart_kpis = df_auto[df_auto['kpi_text'].str.contains('pie|Pie', case=False, na=False)]\n",
    "           print(f\"\\n📊 Pie chart related KPIs ({len(pie_chart_kpis)}):\")\n",
    "           for idx, row in pie_chart_kpis.iterrows():\n",
    "               print(f\"- {row['kpi_text']}\")\n",
    "\n",
    "           return df_auto\n",
    "\n",
    "       except Exception as e:\n",
    "           print(f\"❌ Re-run failed: {e}\")\n",
    "           return None\n",
    "   else:\n",
    "       print(\"\\n❌ New prompt test failed, need further debugging\")\n",
    "       return None"
   ],
   "metadata": {
    "id": "Yq5MEBJLop0V"
   },
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Universal testing function\n",
    "def test_universal_prompt():\n",
    "    \"\"\"Test the effectiveness of universal prompt\"\"\"\n",
    "    print(\"=== Testing Universal Image Analysis Prompt ===\")\n",
    "\n",
    "    try:\n",
    "        # Extract full page image from page 2 for testing\n",
    "        images = extract_images_from_pdf_fixed(PDF_PATH)\n",
    "        page2_image = None\n",
    "\n",
    "        for img_info in images:\n",
    "            if img_info['page_number'] == 2 and img_info['type'] == 'full_page':\n",
    "                page2_image = img_info['image']\n",
    "                break\n",
    "\n",
    "        if page2_image is None:\n",
    "            print(\"❌ Could not find page 2 image\")\n",
    "            return False\n",
    "\n",
    "        print(f\"✅ Found page 2 image, size: {page2_image.width}x{page2_image.height}\")\n",
    "\n",
    "        # Test using universal prompt\n",
    "        base64_image = image_to_base64_fixed(page2_image)\n",
    "\n",
    "        print(\"🔄 Calling API with universal prompt...\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": ENHANCED_IMAGE_KPI_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"\"\"Please analyze all charts and tables on this page, extracting all quantifiable data points.\n",
    "\n",
    "Focus on:\n",
    "- Specific percentages for each pie chart slice\n",
    "- All numerical data from tables\n",
    "- Ensure each extracted KPI contains specific numbers, not just descriptions\n",
    "\n",
    "Please extract complete contextual information.\"\"\"},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\", \"detail\": \"high\"}}\n",
    "                ]}\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            max_tokens=4000,\n",
    "            timeout=90\n",
    "        )\n",
    "\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        print(f\"\\n📋 API response length: {len(content)} characters\")\n",
    "\n",
    "        # Parse and validate results\n",
    "        try:\n",
    "            if content.startswith('```json'):\n",
    "                content = content[7:]\n",
    "            if content.endswith('```'):\n",
    "                content = content[:-3]\n",
    "            content = content.strip()\n",
    "\n",
    "            if content.startswith('['):\n",
    "                result = json.loads(content)\n",
    "                print(f\"✅ Successfully parsed JSON, found {len(result)} KPIs\")\n",
    "\n",
    "                # Analyze extraction quality\n",
    "                complete_kpis = 0\n",
    "                pie_chart_kpis = 0\n",
    "                table_kpis = 0\n",
    "\n",
    "                print(\"\\n📊 Extracted KPI list:\")\n",
    "                for i, kpi in enumerate(result, 1):\n",
    "                    if isinstance(kpi, dict):\n",
    "                        kpi_text = kpi.get('kpi_text', '')\n",
    "                        quantitative_value = kpi.get('quantitative_value', '')\n",
    "                        chart_type = kpi.get('chart_type', '')\n",
    "\n",
    "                        print(f\"{i:2d}. {kpi_text}\")\n",
    "                        print(f\"    Value: {quantitative_value} {kpi.get('unit', '')}\")\n",
    "                        print(f\"    Type: {chart_type}\")\n",
    "\n",
    "                        # Statistical analysis\n",
    "                        if quantitative_value and str(quantitative_value).strip():\n",
    "                            complete_kpis += 1\n",
    "\n",
    "                        if 'pie' in chart_type.lower():\n",
    "                            pie_chart_kpis += 1\n",
    "                        elif 'table' in chart_type.lower():\n",
    "                            table_kpis += 1\n",
    "\n",
    "                        print()\n",
    "\n",
    "                print(f\"📈 Quality analysis:\")\n",
    "                print(f\"  - KPIs with values: {complete_kpis}/{len(result)} ({complete_kpis/len(result)*100:.1f}%)\")\n",
    "                print(f\"  - Pie chart KPIs: {pie_chart_kpis}\")\n",
    "                print(f\"  - Table KPIs: {table_kpis}\")\n",
    "\n",
    "                # Check if target data was extracted\n",
    "                success_indicators = [\n",
    "                    any('64' in str(kpi.get('quantitative_value', '')) for kpi in result),\n",
    "                    any('33' in str(kpi.get('quantitative_value', '')) for kpi in result),\n",
    "                    any('68' in str(kpi.get('quantitative_value', '')) for kpi in result),\n",
    "                    any('30' in str(kpi.get('quantitative_value', '')) for kpi in result)\n",
    "                ]\n",
    "\n",
    "                if any(success_indicators):\n",
    "                    print(\"🎉 Successfully extracted target pie chart data!\")\n",
    "                    return True\n",
    "                else:\n",
    "                    print(\"⚠️ May not have extracted expected pie chart percentages\")\n",
    "                    return False\n",
    "            else:\n",
    "                print(\"❌ API response is not in JSON format\")\n",
    "                print(f\"Response content: {content[:300]}...\")\n",
    "                return False\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"❌ JSON parsing failed: {e}\")\n",
    "            print(f\"Response content: {content[:300]}...\")\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Apply universal fix\n",
    "def apply_universal_fix():\n",
    "    \"\"\"Apply universal prompt fix and re-run\"\"\"\n",
    "    print(\"🔧 Applying universal prompt fix...\")\n",
    "\n",
    "    # First test the new prompt\n",
    "    print(\"Step 1: Testing new universal prompt...\")\n",
    "    if test_universal_prompt():\n",
    "        print(\"\\n✅ Universal prompt test successful!\")\n",
    "\n",
    "        # Ask whether to continue with full extraction\n",
    "        try:\n",
    "            proceed = input(\"\\nContinue with full KPI extraction? (y/n): \").lower()\n",
    "            if proceed == 'y':\n",
    "                print(\"\\n🔄 Re-running full KPI extraction...\")\n",
    "                df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
    "\n",
    "                # Save results\n",
    "                output_file = \"universal_fixed_\" + EXPORT_AUTO_XLSX\n",
    "                save_results(df_auto, output_file, PDF_PATH)\n",
    "\n",
    "                print(f\"\\n🎉 Fix completed! Total extracted: {len(df_auto)} KPIs\")\n",
    "                print(f\"Results saved to {output_file}\")\n",
    "\n",
    "                # Show image-sourced KPI statistics\n",
    "                if 'source_type' in df_auto.columns:\n",
    "                    image_kpis = df_auto[df_auto['source_type'] == 'image']\n",
    "                    print(f\"\\n📊 KPIs extracted from images: {len(image_kpis)}\")\n",
    "\n",
    "                return df_auto\n",
    "            else:\n",
    "                print(\"Cancelled full extraction\")\n",
    "                return None\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nUser cancelled operation\")\n",
    "            return None\n",
    "\n",
    "    else:\n",
    "        print(\"\\n❌ Universal prompt test failed\")\n",
    "        print(\"Recommend checking API response or further adjusting prompt\")\n",
    "        return None"
   ],
   "metadata": {
    "id": "7CYTzMwVqSbv"
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class KPIValidationPipeline:\n",
    "    \"\"\"Comprehensive KPI validation and evaluation system\"\"\"\n",
    "\n",
    "    def __init__(self, manual_excel_path: str, auto_excel_path: str,\n",
    "                 output_dir: str = \"validation_results\"):\n",
    "        \"\"\"\n",
    "        Initialize validation pipeline\n",
    "\n",
    "        Args:\n",
    "            manual_excel_path: Path to manual KPI annotations\n",
    "            auto_excel_path: Path to automatically extracted KPIs\n",
    "            output_dir: Directory to save validation results\n",
    "        \"\"\"\n",
    "        self.manual_path = manual_excel_path\n",
    "        self.auto_path = auto_excel_path\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # Load data\n",
    "        self.manual_df = self._load_excel_safe(manual_excel_path, \"manual\")\n",
    "        self.auto_df = self._load_excel_safe(auto_excel_path, \"auto\")\n",
    "\n",
    "        # Validation results\n",
    "        self.validation_results = {}\n",
    "        self.detailed_analysis = {}\n",
    "\n",
    "        # Similarity thresholds\n",
    "        self.similarity_thresholds = {\n",
    "            'exact': 1.0,\n",
    "            'high': 0.9,\n",
    "            'medium': 0.7,\n",
    "            'low': 0.5\n",
    "        }\n",
    "\n",
    "        logging.info(f\"Validation pipeline initialized:\")\n",
    "        logging.info(f\"  Manual KPIs: {len(self.manual_df)}\")\n",
    "        logging.info(f\"  Auto KPIs: {len(self.auto_df)}\")\n",
    "\n",
    "    def _load_excel_safe(self, filepath: str, source_type: str) -> pd.DataFrame:\n",
    "        \"\"\"Safely load Excel file with error handling\"\"\"\n",
    "        try:\n",
    "            if not Path(filepath).exists():\n",
    "                logging.warning(f\"{source_type.title()} file not found: {filepath}\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            df = pd.read_excel(filepath)\n",
    "            logging.info(f\"Loaded {source_type} file: {len(df)} rows\")\n",
    "\n",
    "            # Standardize column names\n",
    "            df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "            # Ensure required columns exist\n",
    "            required_cols = ['kpi_text']\n",
    "            for col in required_cols:\n",
    "                if col not in df.columns:\n",
    "                    # Try to find similar column names\n",
    "                    similar_cols = [c for c in df.columns if 'kpi' in c.lower() or 'text' in c.lower()]\n",
    "                    if similar_cols:\n",
    "                        df['kpi_text'] = df[similar_cols[0]]\n",
    "                        logging.info(f\"Using column '{similar_cols[0]}' as kpi_text\")\n",
    "                    else:\n",
    "                        logging.warning(f\"Required column '{col}' not found in {source_type} file\")\n",
    "                        df['kpi_text'] = \"\"\n",
    "\n",
    "            # Clean text data\n",
    "            df['kpi_text'] = df['kpi_text'].astype(str).str.strip()\n",
    "            df = df[df['kpi_text'] != ''].reset_index(drop=True)\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading {source_type} file: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def normalize_text(self, text: str) -> str:\n",
    "        \"\"\"Normalize text for comparison\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return \"\"\n",
    "\n",
    "        # Convert to string and lowercase\n",
    "        text = str(text).lower().strip()\n",
    "\n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        # Remove common punctuation but keep percentages and numbers\n",
    "        text = re.sub(r'[^\\w\\s\\%\\.\\,\\-]', ' ', text)\n",
    "\n",
    "        # Normalize number formats\n",
    "        text = re.sub(r'\\b(\\d+),(\\d+)\\b', r'\\1\\2', text)  # Remove commas in numbers\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Normalize spaces\n",
    "\n",
    "        return text.strip()\n",
    "\n",
    "    def calculate_text_similarity(self, text1: str, text2: str) -> Dict[str, float]:\n",
    "        \"\"\"Calculate multiple similarity metrics between two texts\"\"\"\n",
    "        norm1 = self.normalize_text(text1)\n",
    "        norm2 = self.normalize_text(text2)\n",
    "\n",
    "        if not norm1 or not norm2:\n",
    "            return {'sequence': 0.0, 'cosine': 0.0, 'jaccard': 0.0, 'combined': 0.0}\n",
    "\n",
    "        # 1. Sequence similarity (exact match)\n",
    "        sequence_sim = SequenceMatcher(None, norm1, norm2).ratio()\n",
    "\n",
    "        # 2. Cosine similarity (semantic)\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=1)\n",
    "            tfidf_matrix = vectorizer.fit_transform([norm1, norm2])\n",
    "            cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "        except:\n",
    "            cosine_sim = 0.0\n",
    "\n",
    "        # 3. Jaccard similarity (token overlap)\n",
    "        tokens1 = set(norm1.split())\n",
    "        tokens2 = set(norm2.split())\n",
    "        if tokens1 or tokens2:\n",
    "            jaccard_sim = len(tokens1.intersection(tokens2)) / len(tokens1.union(tokens2))\n",
    "        else:\n",
    "            jaccard_sim = 0.0\n",
    "\n",
    "        # 4. Combined similarity\n",
    "        combined_sim = (sequence_sim * 0.4 + cosine_sim * 0.4 + jaccard_sim * 0.2)\n",
    "\n",
    "        return {\n",
    "            'sequence': sequence_sim,\n",
    "            'cosine': cosine_sim,\n",
    "            'jaccard': jaccard_sim,\n",
    "            'combined': combined_sim\n",
    "        }\n",
    "\n",
    "    def find_matches(self, threshold: float = 0.7, similarity_type: str = 'combined') -> pd.DataFrame:\n",
    "        \"\"\"Find matches between manual and auto KPIs\"\"\"\n",
    "        matches = []\n",
    "        auto_matched = set()\n",
    "\n",
    "        for manual_idx, manual_row in self.manual_df.iterrows():\n",
    "            manual_text = manual_row['kpi_text']\n",
    "            best_match = None\n",
    "            best_similarity = 0.0\n",
    "\n",
    "            for auto_idx, auto_row in self.auto_df.iterrows():\n",
    "                if auto_idx in auto_matched:\n",
    "                    continue\n",
    "\n",
    "                auto_text = auto_row['kpi_text']\n",
    "                similarities = self.calculate_text_similarity(manual_text, auto_text)\n",
    "                similarity = similarities[similarity_type]\n",
    "\n",
    "                if similarity > best_similarity and similarity >= threshold:\n",
    "                    best_similarity = similarity\n",
    "                    best_match = {\n",
    "                        'manual_idx': manual_idx,\n",
    "                        'auto_idx': auto_idx,\n",
    "                        'manual_text': manual_text,\n",
    "                        'auto_text': auto_text,\n",
    "                        'similarity': similarity,\n",
    "                        'all_similarities': similarities\n",
    "                    }\n",
    "\n",
    "            if best_match:\n",
    "                matches.append(best_match)\n",
    "                auto_matched.add(best_match['auto_idx'])\n",
    "\n",
    "        return pd.DataFrame(matches)\n",
    "\n",
    "    def calculate_metrics_at_threshold(self, threshold: float = 0.7,\n",
    "                                     similarity_type: str = 'combined') -> Dict[str, float]:\n",
    "        \"\"\"Calculate precision, recall, F1 at specific threshold\"\"\"\n",
    "        matches_df = self.find_matches(threshold, similarity_type)\n",
    "\n",
    "        true_positives = len(matches_df)\n",
    "        false_positives = len(self.auto_df) - true_positives\n",
    "        false_negatives = len(self.manual_df) - true_positives\n",
    "\n",
    "        # Calculate metrics\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        return {\n",
    "            'threshold': threshold,\n",
    "            'similarity_type': similarity_type,\n",
    "            'true_positives': true_positives,\n",
    "            'false_positives': false_positives,\n",
    "            'false_negatives': false_negatives,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score,\n",
    "            'total_manual': len(self.manual_df),\n",
    "            'total_auto': len(self.auto_df),\n",
    "            'match_rate': true_positives / len(self.manual_df) if len(self.manual_df) > 0 else 0.0\n",
    "        }\n",
    "\n",
    "    def run_comprehensive_evaluation(self) -> Dict[str, any]:\n",
    "        \"\"\"Run comprehensive evaluation across multiple thresholds and similarity types\"\"\"\n",
    "        logging.info(\"Running comprehensive evaluation...\")\n",
    "\n",
    "        results = {\n",
    "            'threshold_analysis': [],\n",
    "            'similarity_type_analysis': [],\n",
    "            'category_analysis': {},\n",
    "            'detailed_matches': {},\n",
    "            'false_positives': [],\n",
    "            'false_negatives': []\n",
    "        }\n",
    "\n",
    "        # 1. Threshold analysis\n",
    "        thresholds = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "        similarity_types = ['combined', 'sequence', 'cosine', 'jaccard']\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            for sim_type in similarity_types:\n",
    "                metrics = self.calculate_metrics_at_threshold(threshold, sim_type)\n",
    "                results['threshold_analysis'].append(metrics)\n",
    "\n",
    "        # 2. Find optimal threshold\n",
    "        best_f1 = 0.0\n",
    "        best_config = None\n",
    "        for metrics in results['threshold_analysis']:\n",
    "            if metrics['f1_score'] > best_f1:\n",
    "                best_f1 = metrics['f1_score']\n",
    "                best_config = (metrics['threshold'], metrics['similarity_type'])\n",
    "\n",
    "        # 3. Detailed analysis at optimal threshold\n",
    "        if best_config:\n",
    "            optimal_threshold, optimal_sim_type = best_config\n",
    "            logging.info(f\"Optimal configuration: threshold={optimal_threshold}, similarity={optimal_sim_type}\")\n",
    "\n",
    "            matches_df = self.find_matches(optimal_threshold, optimal_sim_type)\n",
    "            results['detailed_matches'] = matches_df.to_dict('records')\n",
    "\n",
    "            # Find false positives and false negatives\n",
    "            matched_auto_indices = set(matches_df['auto_idx'].tolist()) if not matches_df.empty else set()\n",
    "            matched_manual_indices = set(matches_df['manual_idx'].tolist()) if not matches_df.empty else set()\n",
    "\n",
    "            # False positives (auto KPIs not matched to manual)\n",
    "            fp_indices = set(range(len(self.auto_df))) - matched_auto_indices\n",
    "            results['false_positives'] = [\n",
    "                {\n",
    "                    'index': idx,\n",
    "                    'kpi_text': self.auto_df.iloc[idx]['kpi_text'],\n",
    "                    'category': self.auto_df.iloc[idx].get('kpi_category', 'Unknown'),\n",
    "                    'theme': self.auto_df.iloc[idx].get('kpi_theme', 'Unknown'),\n",
    "                    'source': self.auto_df.iloc[idx].get('source_type', 'Unknown')\n",
    "                }\n",
    "                for idx in fp_indices\n",
    "            ]\n",
    "\n",
    "            # False negatives (manual KPIs not matched by auto)\n",
    "            fn_indices = set(range(len(self.manual_df))) - matched_manual_indices\n",
    "            results['false_negatives'] = [\n",
    "                {\n",
    "                    'index': idx,\n",
    "                    'kpi_text': self.manual_df.iloc[idx]['kpi_text'],\n",
    "                    'category': self.manual_df.iloc[idx].get('kpi_category', 'Unknown'),\n",
    "                    'theme': self.manual_df.iloc[idx].get('kpi_theme', 'Unknown')\n",
    "                }\n",
    "                for idx in fn_indices\n",
    "            ]\n",
    "\n",
    "        # 4. Category-level analysis\n",
    "        if 'kpi_category' in self.manual_df.columns and 'kpi_category' in self.auto_df.columns:\n",
    "            results['category_analysis'] = self._analyze_by_category()\n",
    "\n",
    "        # 5. Theme-level analysis\n",
    "        if 'kpi_theme' in self.manual_df.columns and 'kpi_theme' in self.auto_df.columns:\n",
    "            results['theme_analysis'] = self._analyze_by_theme()\n",
    "\n",
    "        self.validation_results = results\n",
    "        return results\n",
    "\n",
    "    def _analyze_by_category(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Analyze performance by KPI category\"\"\"\n",
    "        category_results = {}\n",
    "\n",
    "        manual_categories = self.manual_df['kpi_category'].value_counts()\n",
    "        auto_categories = self.auto_df['kpi_category'].value_counts()\n",
    "\n",
    "        all_categories = set(manual_categories.index) | set(auto_categories.index)\n",
    "\n",
    "        for category in all_categories:\n",
    "            manual_count = manual_categories.get(category, 0)\n",
    "            auto_count = auto_categories.get(category, 0)\n",
    "\n",
    "            # Find matches within this category\n",
    "            manual_cat_df = self.manual_df[self.manual_df['kpi_category'] == category]\n",
    "            auto_cat_df = self.auto_df[self.auto_df['kpi_category'] == category]\n",
    "\n",
    "            category_matches = 0\n",
    "            if not manual_cat_df.empty and not auto_cat_df.empty:\n",
    "                for _, manual_row in manual_cat_df.iterrows():\n",
    "                    best_sim = 0.0\n",
    "                    for _, auto_row in auto_cat_df.iterrows():\n",
    "                        sim = self.calculate_text_similarity(\n",
    "                            manual_row['kpi_text'],\n",
    "                            auto_row['kpi_text']\n",
    "                        )['combined']\n",
    "                        best_sim = max(best_sim, sim)\n",
    "                    if best_sim >= 0.7:\n",
    "                        category_matches += 1\n",
    "\n",
    "            category_precision = category_matches / auto_count if auto_count > 0 else 0.0\n",
    "            category_recall = category_matches / manual_count if manual_count > 0 else 0.0\n",
    "            category_f1 = 2 * (category_precision * category_recall) / (category_precision + category_recall) if (category_precision + category_recall) > 0 else 0.0\n",
    "\n",
    "            category_results[category] = {\n",
    "                'manual_count': manual_count,\n",
    "                'auto_count': auto_count,\n",
    "                'matches': category_matches,\n",
    "                'precision': category_precision,\n",
    "                'recall': category_recall,\n",
    "                'f1_score': category_f1\n",
    "            }\n",
    "\n",
    "        return category_results\n",
    "\n",
    "    def _analyze_by_theme(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Analyze performance by KPI theme\"\"\"\n",
    "        theme_results = {}\n",
    "\n",
    "        manual_themes = self.manual_df['kpi_theme'].value_counts()\n",
    "        auto_themes = self.auto_df['kpi_theme'].value_counts()\n",
    "\n",
    "        all_themes = set(manual_themes.index) | set(auto_themes.index)\n",
    "\n",
    "        for theme in all_themes:\n",
    "            manual_count = manual_themes.get(theme, 0)\n",
    "            auto_count = auto_themes.get(theme, 0)\n",
    "\n",
    "            theme_results[theme] = {\n",
    "                'manual_count': manual_count,\n",
    "                'auto_count': auto_count,\n",
    "                'coverage': auto_count / manual_count if manual_count > 0 else 0.0\n",
    "            }\n",
    "\n",
    "        return theme_results\n",
    "\n",
    "    def generate_visualizations(self):\n",
    "        \"\"\"Generate comprehensive visualizations\"\"\"\n",
    "        if not self.validation_results:\n",
    "            logging.warning(\"No validation results found. Run evaluation first.\")\n",
    "            return\n",
    "\n",
    "        # Set style\n",
    "        try:\n",
    "            plt.style.use('seaborn-v0_8')\n",
    "        except:\n",
    "            plt.style.use('seaborn')  # 备用样式\n",
    "        fig = plt.figure(figsize=(20, 16))\n",
    "\n",
    "        # 1. Threshold analysis\n",
    "        threshold_df = pd.DataFrame(self.validation_results['threshold_analysis'])\n",
    "\n",
    "        plt.subplot(3, 3, 1)\n",
    "        for sim_type in threshold_df['similarity_type'].unique():\n",
    "            data = threshold_df[threshold_df['similarity_type'] == sim_type]\n",
    "            plt.plot(data['threshold'], data['f1_score'], marker='o', label=sim_type)\n",
    "        plt.xlabel('Similarity Threshold')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title('F1 Score vs Threshold by Similarity Type')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # 2. Precision-Recall curve\n",
    "        plt.subplot(3, 3, 2)\n",
    "        for sim_type in threshold_df['similarity_type'].unique():\n",
    "            data = threshold_df[threshold_df['similarity_type'] == sim_type]\n",
    "            plt.plot(data['recall'], data['precision'], marker='o', label=sim_type)\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curves')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # 3. Category analysis\n",
    "        if 'category_analysis' in self.validation_results:\n",
    "            plt.subplot(3, 3, 3)\n",
    "            cat_analysis = self.validation_results['category_analysis']\n",
    "            categories = list(cat_analysis.keys())[:10]  # Top 10 categories\n",
    "            f1_scores = [cat_analysis[cat]['f1_score'] for cat in categories]\n",
    "\n",
    "            plt.barh(categories, f1_scores)\n",
    "            plt.xlabel('F1 Score')\n",
    "            plt.title('F1 Score by Category (Top 10)')\n",
    "            plt.tight_layout()\n",
    "\n",
    "        # 4. Theme distribution comparison\n",
    "        plt.subplot(3, 3, 4)\n",
    "        if 'kpi_theme' in self.manual_df.columns:\n",
    "            manual_themes = self.manual_df['kpi_theme'].value_counts()\n",
    "            auto_themes = self.auto_df['kpi_theme'].value_counts()\n",
    "\n",
    "            x = np.arange(len(manual_themes))\n",
    "            width = 0.35\n",
    "\n",
    "            plt.bar(x - width/2, manual_themes.values, width, label='Manual', alpha=0.8)\n",
    "            plt.bar(x + width/2, auto_themes.reindex(manual_themes.index, fill_value=0).values,\n",
    "                   width, label='Auto', alpha=0.8)\n",
    "\n",
    "            plt.xlabel('Theme')\n",
    "            plt.ylabel('Count')\n",
    "            plt.title('KPI Count by Theme')\n",
    "            plt.xticks(x, manual_themes.index, rotation=45)\n",
    "            plt.legend()\n",
    "\n",
    "        # 5. Similarity distribution\n",
    "        plt.subplot(3, 3, 5)\n",
    "        if self.validation_results['detailed_matches']:\n",
    "            similarities = [match['similarity'] for match in self.validation_results['detailed_matches']]\n",
    "            plt.hist(similarities, bins=20, edgecolor='black', alpha=0.7)\n",
    "            plt.xlabel('Similarity Score')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title('Distribution of Similarity Scores (Matches)')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # 6. Error analysis\n",
    "        plt.subplot(3, 3, 6)\n",
    "        fp_count = len(self.validation_results['false_positives'])\n",
    "        fn_count = len(self.validation_results['false_negatives'])\n",
    "        tp_count = len(self.validation_results['detailed_matches'])\n",
    "\n",
    "        labels = ['True Positives', 'False Positives', 'False Negatives']\n",
    "        counts = [tp_count, fp_count, fn_count]\n",
    "        colors = ['green', 'red', 'orange']\n",
    "\n",
    "        plt.pie(counts, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "        plt.title('Classification Results')\n",
    "\n",
    "        # 7. Coverage by source type\n",
    "        plt.subplot(3, 3, 7)\n",
    "        if 'source_type' in self.auto_df.columns:\n",
    "            source_counts = self.auto_df['source_type'].value_counts()\n",
    "            plt.pie(source_counts.values, labels=source_counts.index, autopct='%1.1f%%')\n",
    "            plt.title('Auto KPIs by Source Type')\n",
    "\n",
    "        # 8. Performance metrics summary\n",
    "        plt.subplot(3, 3, 8)\n",
    "        best_metrics = max(self.validation_results['threshold_analysis'],\n",
    "                          key=lambda x: x['f1_score'])\n",
    "\n",
    "        metrics = ['Precision', 'Recall', 'F1 Score']\n",
    "        values = [best_metrics['precision'], best_metrics['recall'], best_metrics['f1_score']]\n",
    "\n",
    "        bars = plt.bar(metrics, values, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "        plt.ylabel('Score')\n",
    "        plt.title(f'Best Performance Metrics\\n(Threshold: {best_metrics[\"threshold\"]})')\n",
    "        plt.ylim(0, 1)\n",
    "\n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, values):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{value:.3f}', ha='center', va='bottom')\n",
    "\n",
    "        # 9. Match quality distribution\n",
    "        plt.subplot(3, 3, 9)\n",
    "        if self.validation_results['detailed_matches']:\n",
    "            match_similarities = [match['similarity'] for match in self.validation_results['detailed_matches']]\n",
    "            quality_bins = [0.5, 0.7, 0.8, 0.9, 1.0]\n",
    "            quality_labels = ['Medium', 'Good', 'Very Good', 'Excellent']\n",
    "\n",
    "            quality_counts = []\n",
    "            for i in range(len(quality_bins)-1):\n",
    "                count = sum(1 for sim in match_similarities\n",
    "                          if quality_bins[i] <= sim < quality_bins[i+1])\n",
    "                quality_counts.append(count)\n",
    "\n",
    "            plt.bar(quality_labels, quality_counts, color='lightblue', edgecolor='black')\n",
    "            plt.ylabel('Number of Matches')\n",
    "            plt.title('Match Quality Distribution')\n",
    "            plt.xticks(rotation=45)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save visualization\n",
    "        viz_path = self.output_dir / \"validation_visualizations.png\"\n",
    "        plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        logging.info(f\"Visualizations saved to {viz_path}\")\n",
    "\n",
    "    def generate_detailed_report(self) -> str:\n",
    "        \"\"\"Generate comprehensive validation report\"\"\"\n",
    "        if not self.validation_results:\n",
    "            logging.warning(\"No validation results found. Run evaluation first.\")\n",
    "            return \"\"\n",
    "\n",
    "        # Find best configuration\n",
    "        best_metrics = max(self.validation_results['threshold_analysis'],\n",
    "                          key=lambda x: x['f1_score'])\n",
    "\n",
    "        report = f\"\"\"\n",
    "# KPI Extraction Validation Report\n",
    "Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Dataset Overview\n",
    "- **Manual KPIs**: {len(self.manual_df)} annotations\n",
    "- **Auto KPIs**: {len(self.auto_df)} extractions\n",
    "- **Manual file**: {self.manual_path}\n",
    "- **Auto file**: {self.auto_path}\n",
    "\n",
    "## Best Performance Configuration\n",
    "- **Similarity Type**: {best_metrics['similarity_type']}\n",
    "- **Threshold**: {best_metrics['threshold']}\n",
    "- **Precision**: {best_metrics['precision']:.3f}\n",
    "- **Recall**: {best_metrics['recall']:.3f}\n",
    "- **F1 Score**: {best_metrics['f1_score']:.3f}\n",
    "\n",
    "## Detailed Metrics\n",
    "- **True Positives**: {best_metrics['true_positives']}\n",
    "- **False Positives**: {best_metrics['false_positives']}\n",
    "- **False Negatives**: {best_metrics['false_negatives']}\n",
    "- **Match Rate**: {best_metrics['match_rate']:.3f}\n",
    "\n",
    "## Error Analysis\n",
    "\n",
    "### False Positives ({len(self.validation_results['false_positives'])})\n",
    "KPIs extracted automatically but not in manual annotations:\n",
    "\"\"\"\n",
    "\n",
    "        # Add false positives\n",
    "        for i, fp in enumerate(self.validation_results['false_positives'][:10], 1):\n",
    "            report += f\"\\n{i}. **{fp['category']}** | {fp['kpi_text']}\\n\"\n",
    "\n",
    "        if len(self.validation_results['false_positives']) > 10:\n",
    "            report += f\"\\n... and {len(self.validation_results['false_positives']) - 10} more\\n\"\n",
    "\n",
    "        report += f\"\"\"\n",
    "### False Negatives ({len(self.validation_results['false_negatives'])})\n",
    "KPIs in manual annotations but missed by extraction:\n",
    "\"\"\"\n",
    "\n",
    "        # Add false negatives\n",
    "        for i, fn in enumerate(self.validation_results['false_negatives'][:10], 1):\n",
    "            report += f\"\\n{i}. **{fn['category']}** | {fn['kpi_text']}\\n\"\n",
    "\n",
    "        if len(self.validation_results['false_negatives']) > 10:\n",
    "            report += f\"\\n... and {len(self.validation_results['false_negatives']) - 10} more\\n\"\n",
    "\n",
    "        # Category analysis\n",
    "        if 'category_analysis' in self.validation_results:\n",
    "            report += \"\\n## Category-wise Performance\\n\\n\"\n",
    "            report += \"| Category | Manual | Auto | Matches | Precision | Recall | F1 |\\n\"\n",
    "            report += \"|----------|--------|------|---------|-----------|--------|----|\\\\n\"\n",
    "\n",
    "            for category, metrics in self.validation_results['category_analysis'].items():\n",
    "                report += f\"| {category[:20]} | {metrics['manual_count']} | {metrics['auto_count']} | {metrics['matches']} | {metrics['precision']:.3f} | {metrics['recall']:.3f} | {metrics['f1_score']:.3f} |\\n\"\n",
    "\n",
    "        # Theme analysis\n",
    "        if 'theme_analysis' in self.validation_results:\n",
    "            report += \"\\n## Theme-wise Coverage\\n\\n\"\n",
    "            report += \"| Theme | Manual Count | Auto Count | Coverage |\\n\"\n",
    "            report += \"|-------|--------------|------------|----------|\\n\"\n",
    "\n",
    "            for theme, metrics in self.validation_results['theme_analysis'].items():\n",
    "                report += f\"| {theme} | {metrics['manual_count']} | {metrics['auto_count']} | {metrics['coverage']:.3f} |\\n\"\n",
    "\n",
    "        # Recommendations\n",
    "        report += f\"\"\"\n",
    "## Recommendations\n",
    "\n",
    "### Strengths\n",
    "- Overall F1 Score: {best_metrics['f1_score']:.3f}\n",
    "- Precision: {best_metrics['precision']:.3f} (low false positive rate)\n",
    "- Recall: {best_metrics['recall']:.3f} (good coverage)\n",
    "\n",
    "### Areas for Improvement\n",
    "\"\"\"\n",
    "\n",
    "        if best_metrics['precision'] < 0.8:\n",
    "            report += \"- **Precision**: Consider stricter filtering to reduce false positives\\n\"\n",
    "\n",
    "        if best_metrics['recall'] < 0.8:\n",
    "            report += \"- **Recall**: Improve extraction to catch more manual KPIs\\n\"\n",
    "\n",
    "        if best_metrics['f1_score'] < 0.7:\n",
    "            report += \"- **Overall Performance**: Significant room for improvement in both precision and recall\\n\"\n",
    "\n",
    "        # Source-specific recommendations\n",
    "        if 'source_type' in self.auto_df.columns:\n",
    "            text_kpis = len(self.auto_df[self.auto_df['source_type'] == 'text'])\n",
    "            image_kpis = len(self.auto_df[self.auto_df['source_type'] == 'image'])\n",
    "\n",
    "            report += f\"\"\"\n",
    "### Source Type Analysis\n",
    "- **Text/Table KPIs**: {text_kpis}\n",
    "- **Image/Chart KPIs**: {image_kpis}\n",
    "- **Image Coverage**: {image_kpis / (text_kpis + image_kpis) * 100:.1f}%\n",
    "\"\"\"\n",
    "\n",
    "        return report\n",
    "\n",
    "    def save_results(self):\n",
    "        \"\"\"Save all validation results to files\"\"\"\n",
    "        # Save detailed results as JSON\n",
    "        results_path = self.output_dir / \"validation_results.json\"\n",
    "        with open(results_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.validation_results, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "        # Save matches as Excel\n",
    "        if self.validation_results['detailed_matches']:\n",
    "            matches_df = pd.DataFrame(self.validation_results['detailed_matches'])\n",
    "            matches_path = self.output_dir / \"detailed_matches.xlsx\"\n",
    "            matches_df.to_excel(matches_path, index=False)\n",
    "\n",
    "        # Save false positives and negatives\n",
    "        fp_df = pd.DataFrame(self.validation_results['false_positives'])\n",
    "        fn_df = pd.DataFrame(self.validation_results['false_negatives'])\n",
    "\n",
    "        with pd.ExcelWriter(self.output_dir / \"error_analysis.xlsx\") as writer:\n",
    "            fp_df.to_excel(writer, sheet_name='False_Positives', index=False)\n",
    "            fn_df.to_excel(writer, sheet_name='False_Negatives', index=False)\n",
    "\n",
    "        # Save report\n",
    "        report = self.generate_detailed_report()\n",
    "        report_path = self.output_dir / \"validation_report.md\"\n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "\n",
    "        # Save metrics summary\n",
    "        threshold_df = pd.DataFrame(self.validation_results['threshold_analysis'])\n",
    "        threshold_df.to_excel(self.output_dir / \"threshold_analysis.xlsx\", index=False)\n",
    "\n",
    "        logging.info(f\"All validation results saved to {self.output_dir}\")\n",
    "\n",
    "        return {\n",
    "            'results_json': results_path,\n",
    "            'matches_excel': self.output_dir / \"detailed_matches.xlsx\",\n",
    "            'error_analysis': self.output_dir / \"error_analysis.xlsx\",\n",
    "            'report_markdown': report_path,\n",
    "            'threshold_analysis': self.output_dir / \"threshold_analysis.xlsx\",\n",
    "            'visualizations': self.output_dir / \"validation_visualizations.png\"\n",
    "        }\n",
    "\n",
    "    def run_full_validation(self) -> Dict[str, any]:\n",
    "        \"\"\"Run complete validation pipeline\"\"\"\n",
    "        logging.info(\"Starting full validation pipeline...\")\n",
    "\n",
    "        # Step 1: Run comprehensive evaluation\n",
    "        self.run_comprehensive_evaluation()\n",
    "\n",
    "        # Step 2: Generate visualizations\n",
    "        self.generate_visualizations()\n",
    "\n",
    "        # Step 3: Save all results\n",
    "        saved_files = self.save_results()\n",
    "\n",
    "        # Step 4: Print summary\n",
    "        best_metrics = max(self.validation_results['threshold_analysis'],\n",
    "                          key=lambda x: x['f1_score'])\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"KPI EXTRACTION VALIDATION SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"📊 Dataset: {len(self.manual_df)} manual vs {len(self.auto_df)} auto KPIs\")\n",
    "        print(f\"🎯 Best F1 Score: {best_metrics['f1_score']:.3f}\")\n",
    "        print(f\"📈 Precision: {best_metrics['precision']:.3f}\")\n",
    "        print(f\"📉 Recall: {best_metrics['recall']:.3f}\")\n",
    "        print(f\"✅ True Positives: {best_metrics['true_positives']}\")\n",
    "        print(f\"❌ False Positives: {best_metrics['false_positives']}\")\n",
    "        print(f\"⚠️  False Negatives: {best_metrics['false_negatives']}\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"📁 Results saved to: {self.output_dir}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        return {\n",
    "            'validation_results': self.validation_results,\n",
    "            'saved_files': saved_files,\n",
    "            'best_metrics': best_metrics\n",
    "        }\n"
   ],
   "metadata": {
    "id": "UfLJWwsd4nDX"
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class BatchKPIProcessor:\n",
    "    \"\"\"批量KPI处理器 - 支持多个PDF和Manual文件\"\"\"\n",
    "\n",
    "    def __init__(self, base_output_dir: str = \"batch_kpi_results\"):\n",
    "        \"\"\"\n",
    "        初始化批量处理器\n",
    "\n",
    "        Args:\n",
    "            base_output_dir: 批量处理结果的基础目录\n",
    "        \"\"\"\n",
    "        self.base_output_dir = Path(base_output_dir)\n",
    "        self.base_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # 存储所有文件配对\n",
    "        self.file_pairs = []\n",
    "        self.batch_results = []\n",
    "\n",
    "        # 创建时间戳用于本次批量处理\n",
    "        self.batch_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.current_batch_dir = self.base_output_dir / f\"batch_{self.batch_timestamp}\"\n",
    "        self.current_batch_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        logging.info(f\"批量处理器初始化完成，结果保存到: {self.current_batch_dir}\")\n",
    "\n",
    "    def add_file_pair(self, pdf_path: str, manual_path: str, document_name: str = None):\n",
    "        \"\"\"\n",
    "        添加一对PDF和Manual文件\n",
    "\n",
    "        Args:\n",
    "            pdf_path: PDF文件路径\n",
    "            manual_path: Manual标注文件路径\n",
    "            document_name: 文档名称（可选，默认使用PDF文件名）\n",
    "        \"\"\"\n",
    "        pdf_path = Path(pdf_path)\n",
    "        manual_path = Path(manual_path)\n",
    "\n",
    "        # 验证文件存在\n",
    "        if not pdf_path.exists():\n",
    "            logging.error(f\"PDF文件不存在: {pdf_path}\")\n",
    "            return False\n",
    "\n",
    "        if not manual_path.exists():\n",
    "            logging.error(f\"Manual文件不存在: {manual_path}\")\n",
    "            return False\n",
    "\n",
    "        # 自动生成文档名称\n",
    "        if document_name is None:\n",
    "            document_name = pdf_path.stem\n",
    "\n",
    "        file_pair = {\n",
    "            'pdf_path': str(pdf_path),\n",
    "            'manual_path': str(manual_path),\n",
    "            'document_name': document_name,\n",
    "            'doc_id': len(self.file_pairs) + 1\n",
    "        }\n",
    "\n",
    "        self.file_pairs.append(file_pair)\n",
    "        logging.info(f\"添加文件对 {len(self.file_pairs)}: {document_name}\")\n",
    "        return True\n",
    "\n",
    "    def add_multiple_pairs_from_directory(self, pdf_dir: str, manual_dir: str,\n",
    "                                         pdf_pattern: str = \"*.pdf\",\n",
    "                                         manual_pattern: str = \"*.xlsx\"):\n",
    "        \"\"\"\n",
    "        从目录批量添加文件对（按文件名匹配）\n",
    "\n",
    "        Args:\n",
    "            pdf_dir: PDF文件目录\n",
    "            manual_dir: Manual文件目录\n",
    "            pdf_pattern: PDF文件匹配模式\n",
    "            manual_pattern: Manual文件匹配模式\n",
    "        \"\"\"\n",
    "        pdf_dir = Path(pdf_dir)\n",
    "        manual_dir = Path(manual_dir)\n",
    "\n",
    "        if not pdf_dir.exists() or not manual_dir.exists():\n",
    "            logging.error(f\"目录不存在: {pdf_dir} 或 {manual_dir}\")\n",
    "            return 0\n",
    "\n",
    "        # 获取所有PDF文件\n",
    "        pdf_files = list(pdf_dir.glob(pdf_pattern))\n",
    "        added_count = 0\n",
    "\n",
    "        for pdf_file in pdf_files:\n",
    "            # 尝试找到对应的Manual文件\n",
    "            base_name = pdf_file.stem\n",
    "\n",
    "            # 尝试多种匹配模式\n",
    "            possible_manual_names = [\n",
    "                f\"{base_name}.xlsx\",\n",
    "                f\"{base_name}_manual.xlsx\",\n",
    "                f\"manual_{base_name}.xlsx\",\n",
    "                f\"{base_name}.xls\"\n",
    "            ]\n",
    "\n",
    "            manual_file = None\n",
    "            for manual_name in possible_manual_names:\n",
    "                potential_manual = manual_dir / manual_name\n",
    "                if potential_manual.exists():\n",
    "                    manual_file = potential_manual\n",
    "                    break\n",
    "\n",
    "            if manual_file:\n",
    "                if self.add_file_pair(str(pdf_file), str(manual_file), base_name):\n",
    "                    added_count += 1\n",
    "            else:\n",
    "                logging.warning(f\"未找到 {base_name} 对应的Manual文件\")\n",
    "\n",
    "        logging.info(f\"从目录批量添加了 {added_count} 个文件对\")\n",
    "        return added_count\n",
    "\n",
    "    def list_file_pairs(self):\n",
    "        \"\"\"显示所有已添加的文件对\"\"\"\n",
    "        if not self.file_pairs:\n",
    "            print(\"❌ 没有添加任何文件对\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\n📋 已添加的文件对 (共 {len(self.file_pairs)} 对):\")\n",
    "        print(\"-\" * 80)\n",
    "        for pair in self.file_pairs:\n",
    "            print(f\"{pair['doc_id']:2d}. 文档: {pair['document_name']}\")\n",
    "            print(f\"    PDF:    {pair['pdf_path']}\")\n",
    "            print(f\"    Manual: {pair['manual_path']}\")\n",
    "            print()\n",
    "\n",
    "    def process_single_document(self, file_pair: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        处理单个文档（PDF + Manual）\n",
    "\n",
    "        Args:\n",
    "            file_pair: 文件对信息\n",
    "\n",
    "        Returns:\n",
    "            处理结果字典\n",
    "        \"\"\"\n",
    "        doc_name = file_pair['document_name']\n",
    "        pdf_path = file_pair['pdf_path']\n",
    "        manual_path = file_pair['manual_path']\n",
    "        doc_id = file_pair['doc_id']\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"📄 处理文档 {doc_id}/{len(self.file_pairs)}: {doc_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # 为每个文档创建独立目录\n",
    "        doc_output_dir = self.current_batch_dir / f\"doc_{doc_id}_{doc_name}\"\n",
    "        doc_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        start_time = time.time()\n",
    "        result = {\n",
    "            'doc_id': doc_id,\n",
    "            'document_name': doc_name,\n",
    "            'pdf_path': pdf_path,\n",
    "            'manual_path': manual_path,\n",
    "            'output_dir': str(doc_output_dir),\n",
    "            'start_time': datetime.now().isoformat(),\n",
    "            'status': 'processing'\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Step 1: 临时修改全局PDF路径\n",
    "            global PDF_PATH\n",
    "            original_pdf_path = PDF_PATH\n",
    "            PDF_PATH = pdf_path\n",
    "\n",
    "            print(f\"📊 Step 1: 提取KPI from {Path(pdf_path).name}...\")\n",
    "\n",
    "            # Step 2: 运行KPI提取\n",
    "            df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
    "\n",
    "            # Step 3: 保存自动提取结果\n",
    "            auto_excel_path = doc_output_dir / f\"{doc_name}_auto_kpis.xlsx\"\n",
    "            save_results(df_auto, str(auto_excel_path), PDF_PATH)\n",
    "\n",
    "            print(f\"✅ 提取完成: {len(df_auto)} KPIs\")\n",
    "\n",
    "            # Step 4: 运行验证\n",
    "            print(f\"🔍 Step 2: 运行验证 against {Path(manual_path).name}...\")\n",
    "            validation_output_dir = doc_output_dir / \"validation\"\n",
    "            validation_results = enhanced_compare_with_manual_kpis(\n",
    "                df_auto, manual_path, str(validation_output_dir)\n",
    "            )\n",
    "\n",
    "            # Step 5: 收集结果\n",
    "            processing_time = time.time() - start_time\n",
    "\n",
    "            result.update({\n",
    "                'status': 'completed',\n",
    "                'processing_time_seconds': processing_time,\n",
    "                'extracted_kpis_count': len(df_auto),\n",
    "                'auto_excel_path': str(auto_excel_path),\n",
    "                'validation_output_dir': str(validation_output_dir),\n",
    "                'end_time': datetime.now().isoformat()\n",
    "            })\n",
    "\n",
    "            # 添加验证指标\n",
    "            if validation_results and 'best_metrics' in validation_results:\n",
    "                metrics = validation_results['best_metrics']\n",
    "                result.update({\n",
    "                    'validation_f1_score': metrics.get('f1_score', 0),\n",
    "                    'validation_precision': metrics.get('precision', 0),\n",
    "                    'validation_recall': metrics.get('recall', 0),\n",
    "                    'true_positives': metrics.get('true_positives', 0),\n",
    "                    'false_positives': metrics.get('false_positives', 0),\n",
    "                    'false_negatives': metrics.get('false_negatives', 0)\n",
    "                })\n",
    "\n",
    "                print(f\"🎯 验证完成:\")\n",
    "                print(f\"   F1 Score: {metrics.get('f1_score', 0):.3f}\")\n",
    "                print(f\"   Precision: {metrics.get('precision', 0):.3f}\")\n",
    "                print(f\"   Recall: {metrics.get('recall', 0):.3f}\")\n",
    "\n",
    "            print(f\"⏱️  处理耗时: {processing_time:.1f}秒\")\n",
    "            print(f\"📁 结果保存到: {doc_output_dir}\")\n",
    "\n",
    "            # 恢复原始PDF路径\n",
    "            PDF_PATH = original_pdf_path\n",
    "\n",
    "        except Exception as e:\n",
    "            processing_time = time.time() - start_time\n",
    "            error_msg = str(e)\n",
    "\n",
    "            result.update({\n",
    "                'status': 'failed',\n",
    "                'processing_time_seconds': processing_time,\n",
    "                'error_message': error_msg,\n",
    "                'end_time': datetime.now().isoformat()\n",
    "            })\n",
    "\n",
    "            print(f\"❌ 处理失败: {error_msg}\")\n",
    "\n",
    "            # 恢复原始PDF路径\n",
    "            PDF_PATH = original_pdf_path\n",
    "\n",
    "            # 保存错误日志\n",
    "            error_log_path = doc_output_dir / \"error_log.txt\"\n",
    "            with open(error_log_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"文档: {doc_name}\\n\")\n",
    "                f.write(f\"错误时间: {datetime.now()}\\n\")\n",
    "                f.write(f\"错误信息: {error_msg}\\n\")\n",
    "                f.write(f\"PDF路径: {pdf_path}\\n\")\n",
    "                f.write(f\"Manual路径: {manual_path}\\n\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    def run_batch_processing(self, max_workers: int = 1):\n",
    "        \"\"\"\n",
    "        运行批量处理\n",
    "\n",
    "        Args:\n",
    "            max_workers: 最大并发处理数（建议保持为1，避免API限制）\n",
    "        \"\"\"\n",
    "        if not self.file_pairs:\n",
    "            print(\"❌ 没有要处理的文件对\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\n🚀 开始批量处理 {len(self.file_pairs)} 个文档...\")\n",
    "        print(f\"📁 结果将保存到: {self.current_batch_dir}\")\n",
    "\n",
    "        batch_start_time = time.time()\n",
    "\n",
    "        # 处理每个文档\n",
    "        for file_pair in self.file_pairs:\n",
    "            result = self.process_single_document(file_pair)\n",
    "            self.batch_results.append(result)\n",
    "\n",
    "            # 实时保存进度（防止中断丢失结果）\n",
    "            self.save_batch_progress()\n",
    "\n",
    "        # 生成最终报告\n",
    "        batch_total_time = time.time() - batch_start_time\n",
    "        self.generate_batch_summary(batch_total_time)\n",
    "\n",
    "        print(f\"\\n🎉 批量处理完成!\")\n",
    "        print(f\"⏱️  总耗时: {batch_total_time:.1f}秒 ({batch_total_time/60:.1f}分钟)\")\n",
    "        print(f\"📊 处理统计: {self.get_batch_statistics()}\")\n",
    "        print(f\"📁 完整结果查看: {self.current_batch_dir}\")\n",
    "\n",
    "    def save_batch_progress(self):\n",
    "        \"\"\"保存批量处理进度\"\"\"\n",
    "        progress_file = self.current_batch_dir / \"batch_progress.json\"\n",
    "        with open(progress_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'batch_timestamp': self.batch_timestamp,\n",
    "                'file_pairs': self.file_pairs,\n",
    "                'results': self.batch_results,\n",
    "                'last_updated': datetime.now().isoformat()\n",
    "            }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    def get_batch_statistics(self) -> str:\n",
    "        \"\"\"获取批量处理统计信息\"\"\"\n",
    "        if not self.batch_results:\n",
    "            return \"无结果\"\n",
    "\n",
    "        total = len(self.batch_results)\n",
    "        completed = len([r for r in self.batch_results if r['status'] == 'completed'])\n",
    "        failed = len([r for r in self.batch_results if r['status'] == 'failed'])\n",
    "\n",
    "        # 计算平均验证指标\n",
    "        completed_results = [r for r in self.batch_results if r['status'] == 'completed']\n",
    "        if completed_results:\n",
    "            avg_f1 = sum(r.get('validation_f1_score', 0) for r in completed_results) / len(completed_results)\n",
    "            avg_precision = sum(r.get('validation_precision', 0) for r in completed_results) / len(completed_results)\n",
    "            avg_recall = sum(r.get('validation_recall', 0) for r in completed_results) / len(completed_results)\n",
    "            total_kpis = sum(r.get('extracted_kpis_count', 0) for r in completed_results)\n",
    "        else:\n",
    "            avg_f1 = avg_precision = avg_recall = total_kpis = 0\n",
    "\n",
    "        return f\"\"\"\n",
    "        成功: {completed}/{total} ({completed/total*100:.1f}%)\n",
    "        失败: {failed}/{total} ({failed/total*100:.1f}%)\n",
    "        总KPI数: {total_kpis}\n",
    "        平均F1: {avg_f1:.3f}\n",
    "        平均精确率: {avg_precision:.3f}\n",
    "        平均召回率: {avg_recall:.3f}\n",
    "        \"\"\"\n",
    "\n",
    "    def generate_batch_summary(self, total_time: float):\n",
    "        \"\"\"生成批量处理汇总报告\"\"\"\n",
    "        # 1. 保存详细结果到Excel\n",
    "        results_df = pd.DataFrame(self.batch_results)\n",
    "        excel_path = self.current_batch_dir / \"batch_summary.xlsx\"\n",
    "\n",
    "        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "            # 主要结果\n",
    "            results_df.to_excel(writer, sheet_name='处理结果', index=False)\n",
    "\n",
    "            # 验证指标汇总\n",
    "            if not results_df.empty:\n",
    "                completed_df = results_df[results_df['status'] == 'completed']\n",
    "                if not completed_df.empty:\n",
    "                    validation_columns = ['document_name', 'extracted_kpis_count',\n",
    "                                        'validation_f1_score', 'validation_precision',\n",
    "                                        'validation_recall', 'true_positives',\n",
    "                                        'false_positives', 'false_negatives']\n",
    "\n",
    "                    validation_df = completed_df[validation_columns].copy()\n",
    "                    validation_df.to_excel(writer, sheet_name='验证指标', index=False)\n",
    "\n",
    "        # 2. 生成Markdown报告\n",
    "        report_path = self.current_batch_dir / \"batch_report.md\"\n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"\"\"# 批量KPI提取与验证报告\n",
    "\n",
    "## 处理概览\n",
    "- **处理时间**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **批次ID**: {self.batch_timestamp}\n",
    "- **总文档数**: {len(self.file_pairs)}\n",
    "- **总耗时**: {total_time:.1f}秒 ({total_time/60:.1f}分钟)\n",
    "\n",
    "## 处理统计\n",
    "{self.get_batch_statistics()}\n",
    "\n",
    "## 详细结果\n",
    "\n",
    "| 文档ID | 文档名称 | 状态 | KPI数量 | F1分数 | 精确率 | 召回率 | 处理时间(秒) |\n",
    "|--------|----------|------|---------|--------|--------|--------|-------------|\n",
    "\"\"\")\n",
    "\n",
    "            for result in self.batch_results:\n",
    "                f.write(f\"| {result['doc_id']} | {result['document_name']} | {result['status']} | \"\n",
    "                       f\"{result.get('extracted_kpis_count', 'N/A')} | \"\n",
    "                       f\"{result.get('validation_f1_score', 0):.3f} | \"\n",
    "                       f\"{result.get('validation_precision', 0):.3f} | \"\n",
    "                       f\"{result.get('validation_recall', 0):.3f} | \"\n",
    "                       f\"{result.get('processing_time_seconds', 0):.1f} |\\n\")\n",
    "\n",
    "            if any(r['status'] == 'failed' for r in self.batch_results):\n",
    "                f.write(f\"\\n## 失败的文档\\n\")\n",
    "                for result in self.batch_results:\n",
    "                    if result['status'] == 'failed':\n",
    "                        f.write(f\"- **{result['document_name']}**: {result.get('error_message', '未知错误')}\\n\")\n",
    "\n",
    "        print(f\"📋 批量处理报告生成: {report_path}\")\n",
    "        print(f\"📊 详细结果Excel: {excel_path}\")\n"
   ],
   "metadata": {
    "id": "fP6Nh5cT_rRu"
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class FileUploadManager:\n",
    "    \"\"\"文件上传和管理器\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.uploaded_files = []\n",
    "        self.pdf_files = []\n",
    "        self.manual_files = []\n",
    "        self.file_pairs = []\n",
    "\n",
    "        # 创建工作目录\n",
    "        self.work_dir = Path(\"/content/kpi_files\")\n",
    "        self.work_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        print(f\"📁 工作目录: {self.work_dir}\")\n",
    "\n",
    "    def upload_files_directly(self):\n",
    "        \"\"\"直接上传文件到Colab\"\"\"\n",
    "        print(\"📤 请选择要上传的文件...\")\n",
    "        print(\"可以同时选择多个PDF和Excel文件\")\n",
    "\n",
    "        uploaded = files.upload()\n",
    "\n",
    "        for filename, content in uploaded.items():\n",
    "            file_path = self.work_dir / filename\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(content)\n",
    "\n",
    "            self.uploaded_files.append(str(file_path))\n",
    "            print(f\"✅ 已上传: {filename}\")\n",
    "\n",
    "        self._categorize_files()\n",
    "        return len(uploaded)\n",
    "\n",
    "    def mount_google_drive(self):\n",
    "        \"\"\"挂载Google Drive\"\"\"\n",
    "        try:\n",
    "            drive.mount('/content/drive')\n",
    "            print(\"✅ Google Drive已挂载\")\n",
    "            print(\"📁 你的文件在: /content/drive/MyDrive/\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Drive挂载失败: {e}\")\n",
    "            return False\n",
    "\n",
    "    def scan_drive_directory(self, drive_path: str):\n",
    "        \"\"\"扫描Drive目录中的文件\"\"\"\n",
    "        drive_path = Path(drive_path)\n",
    "\n",
    "        if not drive_path.exists():\n",
    "            print(f\"❌ 目录不存在: {drive_path}\")\n",
    "            return 0\n",
    "\n",
    "        # 扫描PDF文件\n",
    "        pdf_files = list(drive_path.glob(\"*.pdf\"))\n",
    "        excel_files = list(drive_path.glob(\"*.xlsx\")) + list(drive_path.glob(\"*.xls\"))\n",
    "\n",
    "        print(f\"📊 发现文件:\")\n",
    "        print(f\"   PDF文件: {len(pdf_files)}个\")\n",
    "        print(f\"   Excel文件: {len(excel_files)}个\")\n",
    "\n",
    "        # 复制到工作目录\n",
    "        for pdf_file in pdf_files:\n",
    "            dest = self.work_dir / pdf_file.name\n",
    "            shutil.copy2(pdf_file, dest)\n",
    "            self.uploaded_files.append(str(dest))\n",
    "            print(f\"📄 复制PDF: {pdf_file.name}\")\n",
    "\n",
    "        for excel_file in excel_files:\n",
    "            dest = self.work_dir / excel_file.name\n",
    "            shutil.copy2(excel_file, dest)\n",
    "            self.uploaded_files.append(str(dest))\n",
    "            print(f\"📊 复制Excel: {excel_file.name}\")\n",
    "\n",
    "        self._categorize_files()\n",
    "        return len(pdf_files) + len(excel_files)\n",
    "\n",
    "    def _categorize_files(self):\n",
    "        \"\"\"分类文件\"\"\"\n",
    "        self.pdf_files = []\n",
    "        self.manual_files = []\n",
    "\n",
    "        for file_path in self.uploaded_files:\n",
    "            path = Path(file_path)\n",
    "            if path.suffix.lower() == '.pdf':\n",
    "                self.pdf_files.append(file_path)\n",
    "            elif path.suffix.lower() in ['.xlsx', '.xls']:\n",
    "                self.manual_files.append(file_path)\n",
    "\n",
    "        print(f\"\\n📋 文件分类完成:\")\n",
    "        print(f\"   PDF文件: {len(self.pdf_files)}个\")\n",
    "        print(f\"   Manual文件: {len(self.manual_files)}个\")\n",
    "\n",
    "    def auto_match_files(self) -> List[Tuple[str, str, str]]:\n",
    "        \"\"\"自动匹配PDF和Manual文件\"\"\"\n",
    "        matches = []\n",
    "\n",
    "        for pdf_path in self.pdf_files:\n",
    "            pdf_name = Path(pdf_path).stem\n",
    "\n",
    "            # 尝试多种匹配模式\n",
    "            potential_matches = []\n",
    "\n",
    "            for manual_path in self.manual_files:\n",
    "                manual_name = Path(manual_path).stem\n",
    "\n",
    "                # 匹配模式1: 完全相同\n",
    "                if pdf_name.lower() == manual_name.lower():\n",
    "                    potential_matches.append((manual_path, 1.0, \"完全匹配\"))\n",
    "\n",
    "                # 匹配模式2: PDF名称_manual\n",
    "                elif manual_name.lower() == f\"{pdf_name.lower()}_manual\":\n",
    "                    potential_matches.append((manual_path, 0.9, \"后缀匹配\"))\n",
    "\n",
    "                # 匹配模式3: manual_PDF名称\n",
    "                elif manual_name.lower() == f\"manual_{pdf_name.lower()}\":\n",
    "                    potential_matches.append((manual_path, 0.9, \"前缀匹配\"))\n",
    "\n",
    "                # 匹配模式4: 包含关系\n",
    "                elif pdf_name.lower() in manual_name.lower() or manual_name.lower() in pdf_name.lower():\n",
    "                    potential_matches.append((manual_path, 0.7, \"部分匹配\"))\n",
    "\n",
    "            # 选择最佳匹配\n",
    "            if potential_matches:\n",
    "                best_match = max(potential_matches, key=lambda x: x[1])\n",
    "                matches.append((pdf_path, best_match[0], pdf_name))\n",
    "                print(f\"✅ 匹配: {pdf_name} → {Path(best_match[0]).name} ({best_match[2]})\")\n",
    "            else:\n",
    "                print(f\"❌ 未找到匹配: {pdf_name}\")\n",
    "\n",
    "        self.file_pairs = matches\n",
    "        return matches\n",
    "\n",
    "    def manual_pair_files(self):\n",
    "        \"\"\"手动配对文件\"\"\"\n",
    "        print(\"\\n🔧 手动文件配对\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        self.file_pairs = []\n",
    "\n",
    "        print(\"可用的PDF文件:\")\n",
    "        for i, pdf_path in enumerate(self.pdf_files, 1):\n",
    "            print(f\"  {i}. {Path(pdf_path).name}\")\n",
    "\n",
    "        print(\"\\n可用的Manual文件:\")\n",
    "        for i, manual_path in enumerate(self.manual_files, 1):\n",
    "            print(f\"  {i}. {Path(manual_path).name}\")\n",
    "\n",
    "        for pdf_path in self.pdf_files:\n",
    "            pdf_name = Path(pdf_path).name\n",
    "            print(f\"\\n为PDF文件 '{pdf_name}' 选择Manual文件:\")\n",
    "\n",
    "            for i, manual_path in enumerate(self.manual_files, 1):\n",
    "                print(f\"  {i}. {Path(manual_path).name}\")\n",
    "\n",
    "            try:\n",
    "                choice = int(input(\"请输入Manual文件编号 (0跳过): \"))\n",
    "                if choice > 0 and choice <= len(self.manual_files):\n",
    "                    manual_path = self.manual_files[choice - 1]\n",
    "                    doc_name = Path(pdf_path).stem\n",
    "                    self.file_pairs.append((pdf_path, manual_path, doc_name))\n",
    "                    print(f\"✅ 配对成功: {pdf_name} → {Path(manual_path).name}\")\n",
    "                else:\n",
    "                    print(f\"⏭️ 跳过: {pdf_name}\")\n",
    "            except ValueError:\n",
    "                print(f\"⏭️ 输入无效，跳过: {pdf_name}\")\n",
    "\n",
    "    def validate_manual_files(self) -> Dict[str, bool]:\n",
    "        \"\"\"验证Manual文件格式\"\"\"\n",
    "        validation_results = {}\n",
    "\n",
    "        print(\"\\n🔍 验证Manual文件格式...\")\n",
    "\n",
    "        for manual_path in self.manual_files:\n",
    "            file_name = Path(manual_path).name\n",
    "            try:\n",
    "                df = pd.read_excel(manual_path)\n",
    "\n",
    "                # 检查必需列\n",
    "                required_columns = ['kpi_text']\n",
    "                missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "                if missing_columns:\n",
    "                    print(f\"❌ {file_name}: 缺少列 {missing_columns}\")\n",
    "                    validation_results[manual_path] = False\n",
    "                else:\n",
    "                    # 检查数据\n",
    "                    non_empty_rows = df['kpi_text'].notna().sum()\n",
    "                    print(f\"✅ {file_name}: {len(df)}行数据, {non_empty_rows}个有效KPI\")\n",
    "                    validation_results[manual_path] = True\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ {file_name}: 读取失败 - {e}\")\n",
    "                validation_results[manual_path] = False\n",
    "\n",
    "        return validation_results\n",
    "\n",
    "    def show_file_summary(self):\n",
    "        \"\"\"显示文件汇总\"\"\"\n",
    "        print(f\"\\n📊 文件上传汇总\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"总文件数: {len(self.uploaded_files)}\")\n",
    "        print(f\"PDF文件: {len(self.pdf_files)}\")\n",
    "        print(f\"Manual文件: {len(self.manual_files)}\")\n",
    "        print(f\"配对文件: {len(self.file_pairs)}\")\n",
    "\n",
    "        if self.file_pairs:\n",
    "            print(f\"\\n📋 配对结果:\")\n",
    "            for i, (pdf_path, manual_path, doc_name) in enumerate(self.file_pairs, 1):\n",
    "                print(f\"  {i}. {doc_name}\")\n",
    "                print(f\"     PDF: {Path(pdf_path).name}\")\n",
    "                print(f\"     Manual: {Path(manual_path).name}\")\n",
    "\n",
    "    def get_file_pairs_for_batch_processing(self) -> List[Tuple[str, str, str]]:\n",
    "        \"\"\"获取用于批量处理的文件对\"\"\"\n",
    "        return self.file_pairs\n",
    "\n",
    "    def create_batch_processor_from_uploads(self):\n",
    "        \"\"\"从上传的文件创建批量处理器\"\"\"\n",
    "        if not self.file_pairs:\n",
    "            print(\"❌ 没有可用的文件对\")\n",
    "            return None\n",
    "\n",
    "        # 导入批量处理器\n",
    "        from your_main_script import create_batch_processor  # 需要替换为实际的导入\n",
    "\n",
    "        processor = create_batch_processor()\n",
    "\n",
    "        for pdf_path, manual_path, doc_name in self.file_pairs:\n",
    "            processor.add_file_pair(pdf_path, manual_path, doc_name)\n",
    "\n",
    "        return processor"
   ],
   "metadata": {
    "id": "InVqb0qIB7a0"
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============ 便捷使用函数 ============\n",
    "\n",
    "def create_batch_processor():\n",
    "    \"\"\"创建批量处理器的便捷函数\"\"\"\n",
    "    return BatchKPIProcessor()\n",
    "\n",
    "def quick_batch_from_directories(pdf_dir: str, manual_dir: str):\n",
    "    \"\"\"快速从目录创建批量处理\"\"\"\n",
    "    processor = BatchKPIProcessor()\n",
    "\n",
    "    # 添加文件对\n",
    "    added_count = processor.add_multiple_pairs_from_directory(pdf_dir, manual_dir)\n",
    "\n",
    "    if added_count == 0:\n",
    "        print(\"❌ 没有找到匹配的PDF和Manual文件对\")\n",
    "        return None\n",
    "\n",
    "    # 显示文件列表\n",
    "    processor.list_file_pairs()\n",
    "\n",
    "    # 询问是否继续\n",
    "    response = input(f\"\\n是否开始处理这 {added_count} 个文档? (y/n): \")\n",
    "    if response.lower() == 'y':\n",
    "        processor.run_batch_processing()\n",
    "        return processor\n",
    "    else:\n",
    "        print(\"取消批量处理\")\n",
    "        return processor\n",
    "\n",
    "def manual_batch_setup():\n",
    "    \"\"\"手动设置批量处理的交互式函数\"\"\"\n",
    "    processor = BatchKPIProcessor()\n",
    "\n",
    "    print(\"📋 手动批量处理设置\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    while True:\n",
    "        print(f\"\\n当前已添加 {len(processor.file_pairs)} 个文件对\")\n",
    "        print(\"1. 添加单个文件对\")\n",
    "        print(\"2. 从目录批量添加\")\n",
    "        print(\"3. 查看已添加的文件\")\n",
    "        print(\"4. 开始批量处理\")\n",
    "        print(\"5. 退出\")\n",
    "\n",
    "        choice = input(\"请选择操作 (1-5): \")\n",
    "\n",
    "        if choice == '1':\n",
    "            pdf_path = input(\"PDF文件路径: \")\n",
    "            manual_path = input(\"Manual文件路径: \")\n",
    "            doc_name = input(\"文档名称 (回车使用默认): \").strip()\n",
    "\n",
    "            if not doc_name:\n",
    "                doc_name = None\n",
    "\n",
    "            processor.add_file_pair(pdf_path, manual_path, doc_name)\n",
    "\n",
    "        elif choice == '2':\n",
    "            pdf_dir = input(\"PDF文件目录: \")\n",
    "            manual_dir = input(\"Manual文件目录: \")\n",
    "            added = processor.add_multiple_pairs_from_directory(pdf_dir, manual_dir)\n",
    "            print(f\"添加了 {added} 个文件对\")\n",
    "\n",
    "        elif choice == '3':\n",
    "            processor.list_file_pairs()\n",
    "\n",
    "        elif choice == '4':\n",
    "            if processor.file_pairs:\n",
    "                processor.run_batch_processing()\n",
    "                break\n",
    "            else:\n",
    "                print(\"❌ 没有添加任何文件对\")\n",
    "\n",
    "        elif choice == '5':\n",
    "            print(\"退出批量处理设置\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"无效选择，请重试\")\n",
    "\n",
    "    return processor"
   ],
   "metadata": {
    "id": "Mc6cffCKCHDk"
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Enhanced comparison function for the main code\n",
    "def enhanced_compare_with_manual_kpis(df_auto: pd.DataFrame, manual_xlsx_path: str,\n",
    "                                     output_dir: str = \"validation_results\") -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Enhanced comparison with manual KPIs using the validation pipeline\n",
    "\n",
    "    Args:\n",
    "        df_auto: DataFrame with automatically extracted KPIs\n",
    "        manual_xlsx_path: Path to manual KPI annotations\n",
    "        output_dir: Directory to save validation results\n",
    "\n",
    "    Returns:\n",
    "        Comprehensive validation results\n",
    "    \"\"\"\n",
    "    if not Path(manual_xlsx_path).exists():\n",
    "        logging.warning(f\"Manual KPI file not found: {manual_xlsx_path}\")\n",
    "        return {}\n",
    "\n",
    "    # Save auto KPIs to temporary file for validation pipeline\n",
    "    temp_auto_path = Path(output_dir) / \"temp_auto_kpis.xlsx\"\n",
    "    temp_auto_path.parent.mkdir(exist_ok=True)\n",
    "    df_auto.to_excel(temp_auto_path, index=False)\n",
    "\n",
    "    try:\n",
    "        # Initialize and run validation pipeline\n",
    "        validator = KPIValidationPipeline(\n",
    "            manual_excel_path=manual_xlsx_path,\n",
    "            auto_excel_path=str(temp_auto_path),\n",
    "            output_dir=output_dir\n",
    "        )\n",
    "\n",
    "        # Run full validation\n",
    "        results = validator.run_full_validation()\n",
    "\n",
    "        # Clean up temporary file\n",
    "        if temp_auto_path.exists():\n",
    "            temp_auto_path.unlink()\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Enhanced validation failed: {e}\")\n",
    "        # Clean up temporary file\n",
    "        if temp_auto_path.exists():\n",
    "            temp_auto_path.unlink()\n",
    "        return {}\n",
    "\n",
    "\n",
    "# Integration function for the main pipeline\n",
    "def run_kpi_extraction_with_validation():\n",
    "    \"\"\"Run KPI extraction with comprehensive validation\"\"\"\n",
    "    print(\"🚀 Starting KPI extraction with automated validation...\")\n",
    "\n",
    "    # Validate environment\n",
    "    if not validate_environment():\n",
    "        print(\"Please fix the environment issues before running.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Step 1: Run KPI extraction\n",
    "        print(\"\\n📊 Step 1: Extracting KPIs...\")\n",
    "        df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
    "\n",
    "        # Save auto results\n",
    "        save_results(df_auto, EXPORT_AUTO_XLSX, PDF_PATH)\n",
    "        print(f\"✅ Extracted {len(df_auto)} KPIs and saved to {EXPORT_AUTO_XLSX}\")\n",
    "\n",
    "        # Step 2: Run validation if manual file exists\n",
    "        if MANUAL_XLSX and Path(MANUAL_XLSX).exists():\n",
    "            print(f\"\\n🔍 Step 2: Running validation against {MANUAL_XLSX}...\")\n",
    "            validation_results = enhanced_compare_with_manual_kpis(\n",
    "                df_auto, MANUAL_XLSX, \"validation_results\"\n",
    "            )\n",
    "\n",
    "            if validation_results:\n",
    "                best_metrics = validation_results['best_metrics']\n",
    "                print(f\"\\n🎯 Validation completed!\")\n",
    "                print(f\"   F1 Score: {best_metrics['f1_score']:.3f}\")\n",
    "                print(f\"   Precision: {best_metrics['precision']:.3f}\")\n",
    "                print(f\"   Recall: {best_metrics['recall']:.3f}\")\n",
    "\n",
    "                return {\n",
    "                    'extracted_kpis': df_auto,\n",
    "                    'validation_results': validation_results\n",
    "                }\n",
    "            else:\n",
    "                print(\"⚠️ Validation failed, but extraction completed successfully\")\n",
    "                return {'extracted_kpis': df_auto}\n",
    "        else:\n",
    "            print(f\"\\n⚠️ Manual KPI file not found ({MANUAL_XLSX}), skipping validation\")\n",
    "            return {'extracted_kpis': df_auto}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Pipeline failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# Batch validation function for multiple documents\n",
    "def run_batch_validation(pdf_list: List[str], manual_list: List[str],\n",
    "                        output_base_dir: str = \"batch_validation\"):\n",
    "    \"\"\"\n",
    "    Run validation across multiple PDF documents\n",
    "\n",
    "    Args:\n",
    "        pdf_list: List of PDF file paths\n",
    "        manual_list: List of corresponding manual annotation files\n",
    "        output_base_dir: Base directory for validation results\n",
    "    \"\"\"\n",
    "    batch_results = []\n",
    "\n",
    "    for i, (pdf_path, manual_path) in enumerate(zip(pdf_list, manual_list)):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing document {i+1}/{len(pdf_list)}: {Path(pdf_path).name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        try:\n",
    "            # Set up paths for this document\n",
    "            doc_name = Path(pdf_path).stem\n",
    "            doc_output_dir = Path(output_base_dir) / doc_name\n",
    "            doc_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Extract KPIs\n",
    "            global PDF_PATH\n",
    "            original_pdf_path = PDF_PATH\n",
    "            PDF_PATH = pdf_path\n",
    "\n",
    "            df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
    "            auto_excel_path = doc_output_dir / f\"{doc_name}_auto_kpis.xlsx\"\n",
    "            save_results(df_auto, str(auto_excel_path), PDF_PATH)\n",
    "\n",
    "            # Run validation\n",
    "            validation_results = enhanced_compare_with_manual_kpis(\n",
    "                df_auto, manual_path, str(doc_output_dir / \"validation\")\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            doc_result = {\n",
    "                'document': doc_name,\n",
    "                'pdf_path': pdf_path,\n",
    "                'manual_path': manual_path,\n",
    "                'extracted_kpis': len(df_auto),\n",
    "                'validation_results': validation_results.get('best_metrics', {}),\n",
    "                'output_dir': str(doc_output_dir)\n",
    "            }\n",
    "            batch_results.append(doc_result)\n",
    "\n",
    "            # Restore original PDF path\n",
    "            PDF_PATH = original_pdf_path\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to process {doc_name}: {e}\")\n",
    "            batch_results.append({\n",
    "                'document': doc_name,\n",
    "                'pdf_path': pdf_path,\n",
    "                'manual_path': manual_path,\n",
    "                'error': str(e)\n",
    "            })\n",
    "\n",
    "    # Generate batch summary\n",
    "    batch_summary_path = Path(output_base_dir) / \"batch_summary.xlsx\"\n",
    "    batch_df = pd.DataFrame(batch_results)\n",
    "    batch_df.to_excel(batch_summary_path, index=False)\n",
    "\n",
    "    print(f\"\\n🎉 Batch validation completed!\")\n",
    "    print(f\"📊 Processed {len(pdf_list)} documents\")\n",
    "    print(f\"📁 Results saved to {output_base_dir}\")\n",
    "    print(f\"📋 Summary available at {batch_summary_path}\")\n",
    "\n",
    "    return batch_results\n",
    "\n",
    "\n",
    "# Quick validation function for testing\n",
    "def quick_validation_test(manual_xlsx: str = None, auto_xlsx: str = None):\n",
    "    \"\"\"Quick validation test with existing files\"\"\"\n",
    "    manual_file = manual_xlsx or MANUAL_XLSX\n",
    "    auto_file = auto_xlsx or EXPORT_AUTO_XLSX\n",
    "\n",
    "    if not Path(manual_file).exists():\n",
    "        print(f\"❌ Manual file not found: {manual_file}\")\n",
    "        return None\n",
    "\n",
    "    if not Path(auto_file).exists():\n",
    "        print(f\"❌ Auto file not found: {auto_file}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"🔍 Quick validation test:\")\n",
    "    print(f\"  Manual: {manual_file}\")\n",
    "    print(f\"  Auto: {auto_file}\")\n",
    "\n",
    "    try:\n",
    "        validator = KPIValidationPipeline(\n",
    "            manual_excel_path=manual_file,\n",
    "            auto_excel_path=auto_file,\n",
    "            output_dir=\"quick_validation\"\n",
    "        )\n",
    "\n",
    "        results = validator.run_full_validation()\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Quick validation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Performance benchmarking function\n",
    "def benchmark_extraction_methods():\n",
    "    \"\"\"Benchmark different extraction methods with validation\"\"\"\n",
    "    methods = {\n",
    "        'text_only': process_text_only,\n",
    "        'with_images': process_sustainability_report_with_enhanced_images,\n",
    "        'optimized': process_sustainability_report_OPTIMIZED\n",
    "    }\n",
    "\n",
    "    benchmark_results = {}\n",
    "\n",
    "    for method_name, method_func in methods.items():\n",
    "        print(f\"\\n🧪 Benchmarking {method_name}...\")\n",
    "\n",
    "        try:\n",
    "            import time\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Run extraction\n",
    "            df_result = method_func(PDF_PATH)\n",
    "            extraction_time = time.time() - start_time\n",
    "\n",
    "            # Save results\n",
    "            method_output = f\"{method_name}_{EXPORT_AUTO_XLSX}\"\n",
    "            save_results(df_result, method_output, PDF_PATH)\n",
    "\n",
    "            # Run validation if manual file exists\n",
    "            validation_metrics = {}\n",
    "            if MANUAL_XLSX and Path(MANUAL_XLSX).exists():\n",
    "                validation_results = enhanced_compare_with_manual_kpis(\n",
    "                    df_result, MANUAL_XLSX, f\"benchmark_{method_name}\"\n",
    "                )\n",
    "                if validation_results:\n",
    "                    validation_metrics = validation_results['best_metrics']\n",
    "\n",
    "            benchmark_results[method_name] = {\n",
    "                'extraction_time': extraction_time,\n",
    "                'kpi_count': len(df_result),\n",
    "                'kpis_per_second': len(df_result) / extraction_time,\n",
    "                'validation_metrics': validation_metrics\n",
    "            }\n",
    "\n",
    "            print(f\"✅ {method_name}: {len(df_result)} KPIs in {extraction_time:.1f}s\")\n",
    "            if validation_metrics:\n",
    "                print(f\"   F1: {validation_metrics.get('f1_score', 0):.3f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {method_name} failed: {e}\")\n",
    "            benchmark_results[method_name] = {'error': str(e)}\n",
    "\n",
    "    # Save benchmark results\n",
    "    benchmark_df = pd.DataFrame(benchmark_results).T\n",
    "    benchmark_df.to_excel(\"extraction_benchmark.xlsx\")\n",
    "\n",
    "    print(f\"\\n🏆 Benchmark completed!\")\n",
    "    print(f\"📊 Results saved to extraction_benchmark.xlsx\")\n",
    "\n",
    "    return benchmark_results\n",
    "\n",
    "\n",
    "# Usage examples and documentation\n",
    "def validation_usage_examples():\n",
    "    \"\"\"Show usage examples for the validation pipeline\"\"\"\n",
    "    print(\"\"\"\n",
    "# KPI Validation Pipeline Usage Examples\n",
    "\n",
    "## 1. Basic validation with existing files\n",
    "```python\n",
    "validator = KPIValidationPipeline(\n",
    "    manual_excel_path=\"manual_kpis.xlsx\",\n",
    "    auto_excel_path=\"auto_kpis.xlsx\"\n",
    ")\n",
    "results = validator.run_full_validation()\n",
    "```\n",
    "\n",
    "## 2. Integrated extraction + validation\n",
    "```python\n",
    "results = run_kpi_extraction_with_validation()\n",
    "```\n",
    "\n",
    "## 3. Quick validation test\n",
    "```python\n",
    "results = quick_validation_test(\"manual.xlsx\", \"auto.xlsx\")\n",
    "```\n",
    "\n",
    "## 4. Batch validation for multiple documents\n",
    "```python\n",
    "pdf_files = [\"doc1.pdf\", \"doc2.pdf\", \"doc3.pdf\"]\n",
    "manual_files = [\"manual1.xlsx\", \"manual2.xlsx\", \"manual3.xlsx\"]\n",
    "batch_results = run_batch_validation(pdf_files, manual_files)\n",
    "```\n",
    "\n",
    "## 5. Benchmark different extraction methods\n",
    "```python\n",
    "benchmark_results = benchmark_extraction_methods()\n",
    "```\n",
    "\n",
    "## 6. Custom threshold analysis\n",
    "```python\n",
    "validator = KPIValidationPipeline(\"manual.xlsx\", \"auto.xlsx\")\n",
    "validator.run_comprehensive_evaluation()\n",
    "\n",
    "# Check performance at different thresholds\n",
    "for threshold in [0.5, 0.7, 0.9]:\n",
    "    metrics = validator.calculate_metrics_at_threshold(threshold)\n",
    "    print(f\"Threshold {threshold}: F1={metrics['f1_score']:.3f}\")\n",
    "```\n",
    "\n",
    "## Output Files Generated:\n",
    "- validation_results.json - Complete results in JSON format\n",
    "- detailed_matches.xlsx - All matched KPIs with similarity scores\n",
    "- error_analysis.xlsx - False positives and false negatives\n",
    "- validation_report.md - Human-readable report\n",
    "- threshold_analysis.xlsx - Performance across different thresholds\n",
    "- validation_visualizations.png - Comprehensive charts and graphs\n",
    "\n",
    "## Key Metrics Explained:\n",
    "- **Precision**: % of auto KPIs that match manual annotations\n",
    "- **Recall**: % of manual KPIs found by automatic extraction\n",
    "- **F1 Score**: Harmonic mean of precision and recall\n",
    "- **True Positives**: Correctly identified KPIs\n",
    "- **False Positives**: Auto KPIs not in manual annotations\n",
    "- **False Negatives**: Manual KPIs missed by extraction\n",
    "\"\"\")"
   ],
   "metadata": {
    "id": "vv0x-kG2_odA"
   },
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============ Execution entry ============\n",
    "#if __name__ == \"__main__\":\n",
    "    # Uncomment to install dependencies first\n",
    "    # install_dependencies()\n",
    "    # Uncomment to see usage examples\n",
    "    # example_usage()\n",
    "    # Run the main extraction\n",
    "    #run_kpi_extraction()\n",
    "    # 方式1: Run optimized version (recommended)\n",
    "    #run_optimized_kpi_extraction()\n",
    "    #run_debugging_session()\n",
    "    #apply_universal_fix()\n",
    "    # 测试1: 检查环境\n",
    "    #validate_environment()\n",
    "    # 方式1: 运行带验证的主函数（当前）\n",
    "    #main()\n",
    "    # 或者选择以下任一方式：\n",
    "    # 方式2: 完整的提取+验证流程\n",
    "    # results = run_kpi_extraction_with_validation()\n",
    "    # 方式3: 优化版本\n",
    "    # results = run_optimized_kpi_extraction()\n",
    "    # 方式4: 快速验证测试\n",
    "    # results = quick_validation_test()"
   ],
   "metadata": {
    "id": "_ByxPUdia6io"
   },
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 选择处理模式\n",
    "    print(\"🚀 KPI提取系统\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"1. 单个PDF处理 (原有模式)\")\n",
    "    print(\"2. 批量PDF处理\")\n",
    "    print(\"3. 快速批量处理 (从目录)\")\n",
    "    print(\"4. 交互式批量设置\")\n",
    "    print(\"5. 查看批量处理示例\")\n",
    "\n",
    "    try:\n",
    "        choice = input(\"请选择处理模式 (1-5): \")\n",
    "\n",
    "        if choice == '1':\n",
    "            # 原有的单个PDF处理\n",
    "            main()\n",
    "\n",
    "        elif choice == '2':\n",
    "            # 手动批量处理\n",
    "            processor = create_batch_processor()\n",
    "\n",
    "            # 让用户手动添加文件对\n",
    "            while True:\n",
    "                pdf_path = input(\"输入PDF文件路径 (回车结束): \").strip()\n",
    "                if not pdf_path:\n",
    "                    break\n",
    "                manual_path = input(\"输入对应的Manual文件路径: \").strip()\n",
    "                doc_name = input(\"文档名称 (回车使用默认): \").strip() or None\n",
    "\n",
    "                processor.add_file_pair(pdf_path, manual_path, doc_name)\n",
    "\n",
    "            if processor.file_pairs:\n",
    "                processor.list_file_pairs()\n",
    "                processor.run_batch_processing()\n",
    "            else:\n",
    "                print(\"❌ 没有添加任何文件对\")\n",
    "\n",
    "        elif choice == '3':\n",
    "            # 快速目录批量处理\n",
    "            pdf_dir = input(\"PDF文件目录路径: \").strip()\n",
    "            manual_dir = input(\"Manual文件目录路径: \").strip()\n",
    "            quick_batch_from_directories(pdf_dir, manual_dir)\n",
    "\n",
    "        elif choice == '4':\n",
    "            # 交互式批量设置\n",
    "            manual_batch_setup()\n",
    "\n",
    "        elif choice == '5':\n",
    "            # 显示使用示例\n",
    "            batch_processing_examples()\n",
    "\n",
    "        else:\n",
    "            print(\"无效选择，运行默认单个PDF处理\")\n",
    "            main()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n用户取消操作\")\n",
    "    except Exception as e:\n",
    "        print(f\"执行出错: {e}\")\n",
    "        print(\"运行默认单个PDF处理\")\n",
    "        main()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mbBRhgS1ARfC",
    "outputId": "fcf349aa-aa74-4b01-d0c4-0afd7b14926e"
   },
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 KPI提取系统\n",
      "========================================\n",
      "1. 单个PDF处理 (原有模式)\n",
      "2. 批量PDF处理\n",
      "3. 快速批量处理 (从目录)\n",
      "4. 交互式批量设置\n",
      "5. 查看批量处理示例\n",
      "请选择处理模式 (1-5): 2\n",
      "输入PDF文件路径 (回车结束): /content\n",
      "输入对应的Manual文件路径: /content\n",
      "文档名称 (回车使用默认): \n",
      "输入PDF文件路径 (回车结束): /content/document_C.pdf\n",
      "输入对应的Manual文件路径: /content/document_C.xlsx\n",
      "文档名称 (回车使用默认): \n",
      "输入PDF文件路径 (回车结束): /content/document_F.pdf\n",
      "输入对应的Manual文件路径: /content/document_F.xlsx\n",
      "文档名称 (回车使用默认): \n",
      "输入PDF文件路径 (回车结束): /content/document_N.pdf\n",
      "输入对应的Manual文件路径: /content/document_N.xlsx\n",
      "文档名称 (回车使用默认): \n",
      "输入PDF文件路径 (回车结束): \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ERROR:root:Error opening PDF file: [Errno 21] Is a directory: '/content'\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "📋 已添加的文件对 (共 4 对):\n",
      "--------------------------------------------------------------------------------\n",
      " 1. 文档: content\n",
      "    PDF:    /content\n",
      "    Manual: /content\n",
      "\n",
      " 2. 文档: document_C\n",
      "    PDF:    /content/document_C.pdf\n",
      "    Manual: /content/document_C.xlsx\n",
      "\n",
      " 3. 文档: document_F\n",
      "    PDF:    /content/document_F.pdf\n",
      "    Manual: /content/document_F.xlsx\n",
      "\n",
      " 4. 文档: document_N\n",
      "    PDF:    /content/document_N.pdf\n",
      "    Manual: /content/document_N.xlsx\n",
      "\n",
      "\n",
      "🚀 开始批量处理 4 个文档...\n",
      "📁 结果将保存到: batch_kpi_results/batch_20250728_220458\n",
      "\n",
      "============================================================\n",
      "📄 处理文档 1/4: content\n",
      "============================================================\n",
      "📊 Step 1: 提取KPI from content...\n",
      "❌ 处理失败: [Errno 21] Is a directory: '/content'\n",
      "\n",
      "============================================================\n",
      "📄 处理文档 2/4: document_C\n",
      "============================================================\n",
      "📊 Step 1: 提取KPI from document_C.pdf...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:root:Image analysis response not JSON list: The image provided is a logo and does not contain any charts or graphs with quantifiable performance...\n",
      "WARNING:root:Image analysis response not JSON list: The image provided is a logo and does not contain any charts or graphs with quantifiable performance...\n",
      "WARNING:root:Image analysis response not JSON list: The image provided is a text document discussing Cineplex's Corporate Social Responsibility approach...\n",
      "WARNING:root:Image analysis response not JSON list: The image provided is a text document discussing Cineplex's Corporate Social Responsibility approach...\n",
      "WARNING:root:Image analysis response not JSON list: The image provided is a text document rather than a chart or graph. It contains descriptions of vari...\n",
      "WARNING:root:Image analysis response not JSON list: The image provided is a text document, not a chart or graph. It contains descriptions of various ini...\n",
      "WARNING:root:Image analysis response not JSON list: The image contains text with quantifiable performance data related to inclusivity, diversity, and ac...\n",
      "WARNING:root:Image analysis response not JSON list: The image provided is a text document and does not contain any charts or graphs. Therefore, there ar...\n",
      "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It is a...\n",
      "WARNING:root:Image analysis response not JSON list: The image provided is a text document and does not contain any charts or graphs. Therefore, there ar...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ 提取完成: 10 KPIs\n",
      "🔍 Step 2: 运行验证 against document_C.xlsx...\n",
      "\n",
      "============================================================\n",
      "KPI EXTRACTION VALIDATION SUMMARY\n",
      "============================================================\n",
      "📊 Dataset: 6 manual vs 10 auto KPIs\n",
      "🎯 Best F1 Score: 0.625\n",
      "📈 Precision: 0.500\n",
      "📉 Recall: 0.833\n",
      "✅ True Positives: 5\n",
      "❌ False Positives: 5\n",
      "⚠️  False Negatives: 1\n",
      "============================================================\n",
      "📁 Results saved to: batch_kpi_results/batch_20250728_220458/doc_2_document_C/validation\n",
      "============================================================\n",
      "🎯 验证完成:\n",
      "   F1 Score: 0.625\n",
      "   Precision: 0.500\n",
      "   Recall: 0.833\n",
      "⏱️  处理耗时: 108.1秒\n",
      "📁 结果保存到: batch_kpi_results/batch_20250728_220458/doc_2_document_C\n",
      "\n",
      "============================================================\n",
      "📄 处理文档 3/4: document_F\n",
      "============================================================\n",
      "📊 Step 1: 提取KPI from document_F.pdf...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
      "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
      "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
      "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
      "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
      "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
      "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It prim...\n",
      "WARNING:root:Image analysis response not JSON list: The image provided is a text-heavy document related to Corporate Social Responsibility (CSR) initiat...\n",
      "WARNING:root:Image analysis response not JSON list: The image provided is a photograph of a group of people holding a banner. It does not contain any ch...\n",
      "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
      "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
      "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
      "WARNING:root:Image analysis response not JSON list: The image contains a diagram related to environmental activities, specifically focusing on the overa...\n",
      "WARNING:root:Image analysis response not JSON list: The image contains a diagram related to environmental activities, specifically focusing on the overa...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ 提取完成: 19 KPIs\n",
      "🔍 Step 2: 运行验证 against document_F.xlsx...\n",
      "\n",
      "============================================================\n",
      "KPI EXTRACTION VALIDATION SUMMARY\n",
      "============================================================\n",
      "📊 Dataset: 11 manual vs 19 auto KPIs\n",
      "🎯 Best F1 Score: 0.600\n",
      "📈 Precision: 0.474\n",
      "📉 Recall: 0.818\n",
      "✅ True Positives: 9\n",
      "❌ False Positives: 10\n",
      "⚠️  False Negatives: 2\n",
      "============================================================\n",
      "📁 Results saved to: batch_kpi_results/batch_20250728_220458/doc_3_document_F/validation\n",
      "============================================================\n",
      "🎯 验证完成:\n",
      "   F1 Score: 0.600\n",
      "   Precision: 0.474\n",
      "   Recall: 0.818\n",
      "⏱️  处理耗时: 158.6秒\n",
      "📁 结果保存到: batch_kpi_results/batch_20250728_220458/doc_3_document_F\n",
      "\n",
      "============================================================\n",
      "📄 处理文档 4/4: document_N\n",
      "============================================================\n",
      "📊 Step 1: 提取KPI from document_N.pdf...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
      "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
      "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
      "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ 提取完成: 60 KPIs\n",
      "🔍 Step 2: 运行验证 against document_N.xlsx...\n",
      "\n",
      "============================================================\n",
      "KPI EXTRACTION VALIDATION SUMMARY\n",
      "============================================================\n",
      "📊 Dataset: 46 manual vs 60 auto KPIs\n",
      "🎯 Best F1 Score: 0.868\n",
      "📈 Precision: 0.767\n",
      "📉 Recall: 1.000\n",
      "✅ True Positives: 46\n",
      "❌ False Positives: 14\n",
      "⚠️  False Negatives: 0\n",
      "============================================================\n",
      "📁 Results saved to: batch_kpi_results/batch_20250728_220458/doc_4_document_N/validation\n",
      "============================================================\n",
      "🎯 验证完成:\n",
      "   F1 Score: 0.868\n",
      "   Precision: 0.767\n",
      "   Recall: 1.000\n",
      "⏱️  处理耗时: 461.8秒\n",
      "📁 结果保存到: batch_kpi_results/batch_20250728_220458/doc_4_document_N\n",
      "📋 批量处理报告生成: batch_kpi_results/batch_20250728_220458/batch_report.md\n",
      "📊 详细结果Excel: batch_kpi_results/batch_20250728_220458/batch_summary.xlsx\n",
      "\n",
      "🎉 批量处理完成!\n",
      "⏱️  总耗时: 728.5秒 (12.1分钟)\n",
      "📊 处理统计: \n",
      "        成功: 3/4 (75.0%)\n",
      "        失败: 1/4 (25.0%)\n",
      "        总KPI数: 89\n",
      "        平均F1: 0.698\n",
      "        平均精确率: 0.580\n",
      "        平均召回率: 0.884\n",
      "        \n",
      "📁 完整结果查看: batch_kpi_results/batch_20250728_220458\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Oe8TGqRNEk_X"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ibmTPOJu5b5r"
   },
   "execution_count": 37,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ccce40b728940e084ed44759e6f6a3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1a172779d818401d9e5f0a214815f483",
       "IPY_MODEL_8d2c6c2957554af89331c72491000a60",
       "IPY_MODEL_a635c12c3dba492dae47dbb1505d4672"
      ],
      "layout": "IPY_MODEL_008e4c8201424c8cb2f398dc22e529da"
     }
    },
    "1a172779d818401d9e5f0a214815f483": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b2440ae5c97a4fcb92c0d4f8c89a5aeb",
      "placeholder": "​",
      "style": "IPY_MODEL_22d5a2876f2f44c29243d7b96f432861",
      "value": "config.json: "
     }
    },
    "8d2c6c2957554af89331c72491000a60": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b2a3c0a87f94b8898c83024220d1e11",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c31a22c8653147eda676a7dc391e6cab",
      "value": 1
     }
    },
    "a635c12c3dba492dae47dbb1505d4672": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2aa8b8f0df94d5daa9970e077c79993",
      "placeholder": "​",
      "style": "IPY_MODEL_98bfd7b5b8804088a9acdee84db1d0a9",
      "value": " 4.19k/? [00:00&lt;00:00, 156kB/s]"
     }
    },
    "008e4c8201424c8cb2f398dc22e529da": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2440ae5c97a4fcb92c0d4f8c89a5aeb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22d5a2876f2f44c29243d7b96f432861": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5b2a3c0a87f94b8898c83024220d1e11": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "c31a22c8653147eda676a7dc391e6cab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c2aa8b8f0df94d5daa9970e077c79993": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98bfd7b5b8804088a9acdee84db1d0a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "83fc87aef0e44f1c9efa556852a50535": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4fb7b631141d4dc386a395446a222d26",
       "IPY_MODEL_f45e3c6571a5479685e1a5ff0de7baee",
       "IPY_MODEL_00206bf0a5d149bdbb534e917db1bef2"
      ],
      "layout": "IPY_MODEL_9501f3fedfdb4d159f3c3b892432d729"
     }
    },
    "4fb7b631141d4dc386a395446a222d26": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e943538241e6457ba970352a6f96059f",
      "placeholder": "​",
      "style": "IPY_MODEL_db5e00d2fef84840a9cacefe3fc7c1d9",
      "value": "pytorch_model.bin: 100%"
     }
    },
    "f45e3c6571a5479685e1a5ff0de7baee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6c84fd09471495fa3631ecd76e85d97",
      "max": 605247071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2441d4481cc14e14828e852d3a1a153e",
      "value": 605247071
     }
    },
    "00206bf0a5d149bdbb534e917db1bef2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a490041783ef45cc98a54630bc9cd393",
      "placeholder": "​",
      "style": "IPY_MODEL_9b02fcdba7c44d5f9d8dcb862e30a82d",
      "value": " 605M/605M [00:14&lt;00:00, 82.2MB/s]"
     }
    },
    "9501f3fedfdb4d159f3c3b892432d729": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e943538241e6457ba970352a6f96059f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db5e00d2fef84840a9cacefe3fc7c1d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e6c84fd09471495fa3631ecd76e85d97": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2441d4481cc14e14828e852d3a1a153e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a490041783ef45cc98a54630bc9cd393": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b02fcdba7c44d5f9d8dcb862e30a82d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad004cb9ee8f4a4e96b022f585a0907a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b56b6c4b69ac4cf28e2febb0531d2b92",
       "IPY_MODEL_0bc9f1efaab5417c86bcb20ea56ee8b8",
       "IPY_MODEL_e9535287c0e54cf692bfa24b702b6475"
      ],
      "layout": "IPY_MODEL_f40b7ec0ba034f0aa8b9094503918b3d"
     }
    },
    "b56b6c4b69ac4cf28e2febb0531d2b92": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6dc7bf1d30354c00a0ba6aa1d14908d6",
      "placeholder": "​",
      "style": "IPY_MODEL_1f9ab98197d643ca80ed16ad28a52da2",
      "value": "model.safetensors: 100%"
     }
    },
    "0bc9f1efaab5417c86bcb20ea56ee8b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c6096b1b211d40adaca08c2fc635a7a1",
      "max": 605157884,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fc1a53207c8e4286a3c0399449ffdc4b",
      "value": 605157884
     }
    },
    "e9535287c0e54cf692bfa24b702b6475": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7995bd155434285abe5622772ae529f",
      "placeholder": "​",
      "style": "IPY_MODEL_739e450e933e4b9597a02486adbd6cb9",
      "value": " 605M/605M [00:08&lt;00:00, 70.2MB/s]"
     }
    },
    "f40b7ec0ba034f0aa8b9094503918b3d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6dc7bf1d30354c00a0ba6aa1d14908d6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f9ab98197d643ca80ed16ad28a52da2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c6096b1b211d40adaca08c2fc635a7a1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc1a53207c8e4286a3c0399449ffdc4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e7995bd155434285abe5622772ae529f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "739e450e933e4b9597a02486adbd6cb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6984cb1a9323453c80c386e066abb72f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aa7b609f6ce746459777798f8cad4e7c",
       "IPY_MODEL_b3b19a69907c41a5987d4003e5778e6b",
       "IPY_MODEL_587539bde2ef4a15bcd1f3964af9ea10"
      ],
      "layout": "IPY_MODEL_c493417e34204b04a4a3b4401d2c397a"
     }
    },
    "aa7b609f6ce746459777798f8cad4e7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0587c5ccd53146309278915ff67d5e3c",
      "placeholder": "​",
      "style": "IPY_MODEL_beb304462d8149ed94cae4554a0d2b0d",
      "value": "preprocessor_config.json: 100%"
     }
    },
    "b3b19a69907c41a5987d4003e5778e6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_59b3c0904f1949c8ad7d8ebe85ab9699",
      "max": 316,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b1aa3a529f0f4d02b8d804ab3fe6833d",
      "value": 316
     }
    },
    "587539bde2ef4a15bcd1f3964af9ea10": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06902734c8d74980a8ab896a8a4e75ca",
      "placeholder": "​",
      "style": "IPY_MODEL_fc7096c1a41e4485987f3bff71879990",
      "value": " 316/316 [00:00&lt;00:00, 7.14kB/s]"
     }
    },
    "c493417e34204b04a4a3b4401d2c397a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0587c5ccd53146309278915ff67d5e3c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "beb304462d8149ed94cae4554a0d2b0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "59b3c0904f1949c8ad7d8ebe85ab9699": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1aa3a529f0f4d02b8d804ab3fe6833d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "06902734c8d74980a8ab896a8a4e75ca": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc7096c1a41e4485987f3bff71879990": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0c2a41c3cfd348cc909ada49d30fbf53": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_37c52b64dd224bb7a8211a1bc9127045",
       "IPY_MODEL_d67e417be4604dee918b05f20229d49f",
       "IPY_MODEL_d470b6ef7a874108bce384ea882aa9bc"
      ],
      "layout": "IPY_MODEL_70905b5292514b73aad50ecf57071621"
     }
    },
    "37c52b64dd224bb7a8211a1bc9127045": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c848e102d0c4a559fa5959307eca19f",
      "placeholder": "​",
      "style": "IPY_MODEL_8866fca2dba142959336ca621822ed57",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "d67e417be4604dee918b05f20229d49f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d40bdd0b6dfa408fad567ae182586ca2",
      "max": 592,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_abd9ea4a8caa47629630c649601af91f",
      "value": 592
     }
    },
    "d470b6ef7a874108bce384ea882aa9bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e27e8476ce94485a341cb2912787687",
      "placeholder": "​",
      "style": "IPY_MODEL_0cfb5ab6124c465896ae7efa919ab888",
      "value": " 592/592 [00:00&lt;00:00, 9.90kB/s]"
     }
    },
    "70905b5292514b73aad50ecf57071621": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c848e102d0c4a559fa5959307eca19f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8866fca2dba142959336ca621822ed57": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d40bdd0b6dfa408fad567ae182586ca2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abd9ea4a8caa47629630c649601af91f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2e27e8476ce94485a341cb2912787687": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0cfb5ab6124c465896ae7efa919ab888": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3ab4f790ec134fac8bb8c7e520e24fda": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a62e35740c9c4be8a33fdc15577fea13",
       "IPY_MODEL_e642eab9e2834d8f9b2a979bf9ac2a51",
       "IPY_MODEL_8048e7d6e1a94c9eb65b08b05602ed1c"
      ],
      "layout": "IPY_MODEL_cc92a407708046e48b0a73427c889ccd"
     }
    },
    "a62e35740c9c4be8a33fdc15577fea13": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bef94210926b49539c34807397ec6115",
      "placeholder": "​",
      "style": "IPY_MODEL_dc39303ab58a4313b0bc72b13bbddbac",
      "value": "vocab.json: "
     }
    },
    "e642eab9e2834d8f9b2a979bf9ac2a51": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ac180c8a52f4679ac04df996aec2c31",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_59d358dafacd45f395c4a83b54274d1f",
      "value": 1
     }
    },
    "8048e7d6e1a94c9eb65b08b05602ed1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90b3410ad0864b41894e816220b0a006",
      "placeholder": "​",
      "style": "IPY_MODEL_0c97ad7a8e9242e1af4efc874c5c8c24",
      "value": " 862k/? [00:00&lt;00:00, 3.69MB/s]"
     }
    },
    "cc92a407708046e48b0a73427c889ccd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bef94210926b49539c34807397ec6115": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc39303ab58a4313b0bc72b13bbddbac": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9ac180c8a52f4679ac04df996aec2c31": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "59d358dafacd45f395c4a83b54274d1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "90b3410ad0864b41894e816220b0a006": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c97ad7a8e9242e1af4efc874c5c8c24": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e718b9af3ce1406989c267de242e42ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c58f091283354f698beac0a8f4ceda9e",
       "IPY_MODEL_cedb9014073f4564823a9ae25cbd94ba",
       "IPY_MODEL_89382a88b3804ccdb5d707ead82231fe"
      ],
      "layout": "IPY_MODEL_e55fc8ecc872430799f7578b416e2197"
     }
    },
    "c58f091283354f698beac0a8f4ceda9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_884438185fe643a6a5f0c106121e8378",
      "placeholder": "​",
      "style": "IPY_MODEL_0b4e3b2ed9354a49b5cce768ed95b757",
      "value": "merges.txt: "
     }
    },
    "cedb9014073f4564823a9ae25cbd94ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4365b450bf4648808faa4f158767a270",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9e4201d4a3b643a0b19ae2b3bcc334d5",
      "value": 1
     }
    },
    "89382a88b3804ccdb5d707ead82231fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_35af810e53514a6db976c4c89720f7b8",
      "placeholder": "​",
      "style": "IPY_MODEL_a7a44700821e4a56ba54f01ee1eeaefa",
      "value": " 525k/? [00:00&lt;00:00, 1.36MB/s]"
     }
    },
    "e55fc8ecc872430799f7578b416e2197": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "884438185fe643a6a5f0c106121e8378": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b4e3b2ed9354a49b5cce768ed95b757": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4365b450bf4648808faa4f158767a270": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "9e4201d4a3b643a0b19ae2b3bcc334d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "35af810e53514a6db976c4c89720f7b8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7a44700821e4a56ba54f01ee1eeaefa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6fc9ae0453fd4fe09d393e92ec6b7289": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_417dea0e987540c0b7ade5e03cde85f4",
       "IPY_MODEL_64a250f4e69347d48d822b934abf134e",
       "IPY_MODEL_2b4617e352874e2fb7317f7ae1f660f3"
      ],
      "layout": "IPY_MODEL_8c4ac8a99b2f42c0b11e924c321fcca2"
     }
    },
    "417dea0e987540c0b7ade5e03cde85f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c19b4a646864f348b72a2507871c01a",
      "placeholder": "​",
      "style": "IPY_MODEL_1ca96e705e964c61af7622291d7624e8",
      "value": "tokenizer.json: "
     }
    },
    "64a250f4e69347d48d822b934abf134e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1146807e77bc4fdfbcd074bf04f18d0f",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f7f8c1af66e24d309bb3ed5c4d2f90a7",
      "value": 1
     }
    },
    "2b4617e352874e2fb7317f7ae1f660f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba8cc3ceac6f4500a99d5409c78d87ae",
      "placeholder": "​",
      "style": "IPY_MODEL_43aa4639221f4a81b688010bc4c2e20b",
      "value": " 2.22M/? [00:00&lt;00:00, 3.73MB/s]"
     }
    },
    "8c4ac8a99b2f42c0b11e924c321fcca2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c19b4a646864f348b72a2507871c01a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ca96e705e964c61af7622291d7624e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1146807e77bc4fdfbcd074bf04f18d0f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "f7f8c1af66e24d309bb3ed5c4d2f90a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ba8cc3ceac6f4500a99d5409c78d87ae": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43aa4639221f4a81b688010bc4c2e20b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "957c2f3e8e214a56894f191bbee07ee4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_86bad43bd7334e1aade785e368095a2d",
       "IPY_MODEL_a9f2ea6fa7dc42edb8db0f4dcb154c4a",
       "IPY_MODEL_6926dc6ab1ab4e62af882354b0cc357b"
      ],
      "layout": "IPY_MODEL_14a9c4052f674f5386584a266c2c4dff"
     }
    },
    "86bad43bd7334e1aade785e368095a2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_faeb39a8529c4de198a2acd641aba786",
      "placeholder": "​",
      "style": "IPY_MODEL_b3e5f2841bca45429e0b03eb58e415b1",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "a9f2ea6fa7dc42edb8db0f4dcb154c4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_60ef1db8886c45c58d46319e9e65f3f1",
      "max": 389,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a309b6bee65d481085d2ff28ea2ec7c9",
      "value": 389
     }
    },
    "6926dc6ab1ab4e62af882354b0cc357b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_50040d9132fa49baac87ce120d69471c",
      "placeholder": "​",
      "style": "IPY_MODEL_c247c3bec87f4ade9939b18d226bc661",
      "value": " 389/389 [00:00&lt;00:00, 6.05kB/s]"
     }
    },
    "14a9c4052f674f5386584a266c2c4dff": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "faeb39a8529c4de198a2acd641aba786": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b3e5f2841bca45429e0b03eb58e415b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "60ef1db8886c45c58d46319e9e65f3f1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a309b6bee65d481085d2ff28ea2ec7c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "50040d9132fa49baac87ce120d69471c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c247c3bec87f4ade9939b18d226bc661": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "state": {}
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
