{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esemsc-rw1024/irp-rw1024-public/blob/main/KPI_extraction_Jul25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "extract_sustainability_kpi.py\n",
        "==================================\n",
        "Automatically extract KPI sentences/table rows from Sustainability Report PDF\n",
        "and compare with manual KPI annotations\n",
        "--------------------------------------------------\n",
        "1. pdfplumber extracts text + tables\n",
        "2. Camelot supplements complex table parsing (optional)\n",
        "3. Chunking to control tokens\n",
        "4. OpenAI ChatCompletion API call (GPT-4o / GPT-4 / GPT-3.5)\n",
        "5. Aggregate, deduplicate, and export to auto_kpi.xlsx\n",
        "6. Compare with manual_kpi.xlsx for differences\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eDB9oRTEZoZw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "9bbcc21b-24f8-4ecf-9be8-796b7dd022a0"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nextract_sustainability_kpi.py\\n==================================\\nAutomatically extract KPI sentences/table rows from Sustainability Report PDF\\nand compare with manual KPI annotations\\n--------------------------------------------------\\n1. pdfplumber extracts text + tables\\n2. Camelot supplements complex table parsing (optional)\\n3. Chunking to control tokens\\n4. OpenAI ChatCompletion API call (GPT-4o / GPT-4 / GPT-3.5)\\n5. Aggregate, deduplicate, and export to auto_kpi.xlsx\\n6. Compare with manual_kpi.xlsx for differences\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "id": "LaGnafXLZxfg"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "3w2Bya8SZob3"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai python-dotenv pdfplumber tiktoken pandas\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install -y ghostscript\n",
        "!pip install \"camelot-py[cv]\"\n",
        "!pip install PyMuPDF Pillow\n",
        "!pip install -q transformers pillow torchvision"
      ],
      "metadata": {
        "id": "JSakj9TyZodt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac113630-1235-44fa-9b87-96f5db6a21c5"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.97.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.7)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ghostscript is already the newest version (9.55.0~dfsg1-0ubuntu5.12).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "Requirement already satisfied: camelot-py[cv] in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "\u001b[33mWARNING: camelot-py 1.0.0 does not provide the extra 'cv'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (8.2.1)\n",
            "Requirement already satisfied: chardet>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (5.2.0)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (2.0.2)\n",
            "Requirement already satisfied: openpyxl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (3.1.5)\n",
            "Requirement already satisfied: pdfminer-six>=20240706 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (20250506)\n",
            "Requirement already satisfied: pypdf<4.0,>=3.17 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (3.17.4)\n",
            "Requirement already satisfied: pandas>=2.2.2 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (2.2.2)\n",
            "Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (0.9.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.7.0.68 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (4.12.0.88)\n",
            "Requirement already satisfied: pypdfium2>=4 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (4.30.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl>=3.1.0->camelot-py[cv]) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.2->camelot-py[cv]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.2->camelot-py[cv]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.2->camelot-py[cv]) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer-six>=20240706->camelot-py[cv]) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer-six>=20240706->camelot-py[cv]) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[cv]) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.2->camelot-py[cv]) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[cv]) (2.22)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, time, textwrap, argparse, logging\n",
        "import pdfplumber, pandas as pd, tiktoken\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from typing import List, Dict, Optional, Set, Tuple\n",
        "from pathlib import Path\n",
        "from difflib import SequenceMatcher\n",
        "import base64\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import fitz  # PyMuPDF\n",
        "import numpy as np\n",
        "from transformers import CLIPProcessor, CLIPModel\n"
      ],
      "metadata": {
        "id": "rNQHu6_fZofh"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------- Configuration -----------------------------\n",
        "PDF_PATH          = \"/content/Global_Resource_Corp_eco1999e_2dcbgi89 (1).pdf\"\n",
        "MANUAL_XLSX       = \"manual_kpi.xlsx\"   # Leave empty if not available\n",
        "EXPORT_AUTO_XLSX  = \"auto_kpi.xlsx\"\n",
        "MODEL_NAME        = \"gpt-4o\"       # Adjust based on account availability\n",
        "MAX_TOKENS_CHUNK  = 1500               # Token limit per chunk\n",
        "SLEEP_SEC         = 0.6                # Rate limiting\n",
        "ENABLE_QUALITY_VALIDATION = True       # Enable additional quality checks\n",
        "# -----------------------------------------------------------------"
      ],
      "metadata": {
        "id": "L9B6ORg1Zohn"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ 修复的初始化部分 ============\n",
        "def initialize_environment():\n",
        "    \"\"\"Initialize the environment and API client\"\"\"\n",
        "    # Load environment variables\n",
        "    load_dotenv(\"ruojia_api_key.env\")\n",
        "\n",
        "    # Initialize OpenAI client\n",
        "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"OPENAI_API_KEY not found in environment variables!\")\n",
        "\n",
        "    client = OpenAI(api_key=api_key)\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "    return client, enc\n",
        "\n",
        "# Initialize global variables\n",
        "client, enc = initialize_environment()"
      ],
      "metadata": {
        "id": "mi52QoSbZoja"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ 修复的PDF文本提取 ============\n",
        "def pdf_to_text_and_tables(path: str) -> str:\n",
        "    \"\"\"Extract text paragraphs and tables using pdfplumber.\"\"\"\n",
        "    all_chunks = []\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"PDF file not found: {path}\")\n",
        "\n",
        "    try:\n",
        "        with pdfplumber.open(path) as pdf:\n",
        "            logging.info(f\"Processing PDF with {len(pdf.pages)} pages...\")\n",
        "\n",
        "            for page_num, page in enumerate(pdf.pages, 1):\n",
        "                try:\n",
        "                    # Extract text\n",
        "                    text = page.extract_text() or \"\"\n",
        "                    if text.strip():\n",
        "                        all_chunks.append(f\"PAGE_{page_num}_TEXT:\\n{text}\")\n",
        "\n",
        "                    # Extract tables\n",
        "                    tables = page.extract_tables()\n",
        "                    for table_num, tb in enumerate(tables):\n",
        "                        if tb and len(tb) > 0:\n",
        "                            try:\n",
        "                                # Handle table headers safely\n",
        "                                if tb[0]:\n",
        "                                    headers = tb[0]\n",
        "                                else:\n",
        "                                    headers = [f\"Col_{i}\" for i in range(len(tb[1]) if len(tb) > 1 else 1)]\n",
        "\n",
        "                                rows = tb[1:] if len(tb) > 1 else []\n",
        "\n",
        "                                if rows:\n",
        "                                    df = pd.DataFrame(rows, columns=headers)\n",
        "                                    # Clean DataFrame\n",
        "                                    df = df.dropna(how='all')  # Remove empty rows\n",
        "                                    if not df.empty:\n",
        "                                        table_txt = f\"TABLE_START_PAGE_{page_num}_{table_num}\\n\" + df.to_csv(index=False) + \"\\nTABLE_END\"\n",
        "                                        all_chunks.append(table_txt)\n",
        "                            except Exception as e:\n",
        "                                logging.warning(f\"Error processing table on page {page_num}: {e}\")\n",
        "                                continue\n",
        "\n",
        "                except Exception as e:\n",
        "                    logging.warning(f\"Error processing page {page_num}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        return \"\\n\\n\".join(all_chunks)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error opening PDF file: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "IkSbr5pqaGsO"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ 修复的Camelot表格提取 ============\n",
        "def generate_table_fingerprint(df: pd.DataFrame) -> str:\n",
        "    \"\"\"Generate table fingerprint for deduplication\"\"\"\n",
        "    try:\n",
        "        fingerprint_parts = []\n",
        "        fingerprint_parts.append(f\"shape_{df.shape[0]}x{df.shape[1]}\")\n",
        "\n",
        "        if not df.columns.empty:\n",
        "            col_names = [str(col).strip().lower().replace(' ', '') for col in df.columns]\n",
        "            col_fingerprint = '_'.join(sorted(col_names))\n",
        "            fingerprint_parts.append(f\"cols_{hash(col_fingerprint)}\")\n",
        "\n",
        "        if df.shape[0] > 0:\n",
        "            numeric_values = []\n",
        "            for col in df.columns:\n",
        "                for val in df[col].head(3):\n",
        "                    if pd.notna(val):\n",
        "                        numbers = re.findall(r'\\d+\\.?\\d*', str(val))\n",
        "                        numeric_values.extend(numbers)\n",
        "\n",
        "            if numeric_values:\n",
        "                numeric_fingerprint = hash('_'.join(sorted(numeric_values[:10])))\n",
        "                fingerprint_parts.append(f\"nums_{numeric_fingerprint}\")\n",
        "\n",
        "        return '_'.join(fingerprint_parts)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error generating table fingerprint: {e}\")\n",
        "        return str(hash(df.to_csv()))\n",
        "\n",
        "def clean_table_data_improved(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Improved table data cleaning\"\"\"\n",
        "    try:\n",
        "        cleaned_df = df.copy()\n",
        "        cleaned_df = cleaned_df.dropna(how='all')\n",
        "        cleaned_df = cleaned_df.dropna(axis=1, how='all')\n",
        "\n",
        "        for col in cleaned_df.columns:\n",
        "            if cleaned_df[col].dtype == 'object':\n",
        "                cleaned_df[col] = cleaned_df[col].astype(str).str.strip()\n",
        "                cleaned_df[col] = cleaned_df[col].replace(['nan', 'NaN', 'None'], '')\n",
        "\n",
        "        if not cleaned_df.empty:\n",
        "            new_columns = []\n",
        "            for i, col in enumerate(cleaned_df.columns):\n",
        "                col_str = str(col).strip()\n",
        "                if col_str in ['nan', 'NaN', 'None', ''] or pd.isna(col):\n",
        "                    new_columns.append(f'Column_{i}')\n",
        "                else:\n",
        "                    new_columns.append(col_str)\n",
        "            cleaned_df.columns = new_columns\n",
        "\n",
        "        cleaned_df = cleaned_df.reset_index(drop=True)\n",
        "        return cleaned_df\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error in table cleaning: {e}\")\n",
        "        return df\n",
        "\n",
        "def is_valid_table_improved(df: pd.DataFrame) -> bool:\n",
        "    \"\"\"Improved table validation\"\"\"\n",
        "    try:\n",
        "        if df.empty or df.shape[0] < 1 or df.shape[1] < 1:\n",
        "            return False\n",
        "\n",
        "        non_null_cells = 0\n",
        "        total_cells = df.shape[0] * df.shape[1]\n",
        "\n",
        "        for col in df.columns:\n",
        "            for val in df[col]:\n",
        "                if pd.notna(val) and str(val).strip() not in ['', 'nan', 'NaN', 'None']:\n",
        "                    non_null_cells += 1\n",
        "\n",
        "        if non_null_cells / total_cells < 0.2:\n",
        "            return False\n",
        "\n",
        "        has_meaningful_content = False\n",
        "        for col in df.columns:\n",
        "            text_content = ' '.join(df[col].dropna().astype(str))\n",
        "            if (any(char.isdigit() for char in text_content) or\n",
        "                '%' in text_content or\n",
        "                any(keyword in text_content.lower() for keyword in [\n",
        "                    'rate', 'percentage', 'total', 'number', 'emission', 'energy',\n",
        "                    'water', 'waste', 'employee', 'year', '2020', '2021', '2022', '2023'\n",
        "                ])):\n",
        "                has_meaningful_content = True\n",
        "                break\n",
        "\n",
        "        return has_meaningful_content\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error validating table: {e}\")\n",
        "        return True\n",
        "\n",
        "def format_table_output_improved(df: pd.DataFrame, table_id: str, parsing_report=None) -> str:\n",
        "    \"\"\"Improved table output formatting\"\"\"\n",
        "    try:\n",
        "        table_info = f\"TABLE_START_{table_id}\\n\"\n",
        "        table_info += f\"DIMENSIONS: {df.shape[0]} rows × {df.shape[1]} columns\\n\"\n",
        "\n",
        "        col_info = \"COLUMNS: \" + \" | \".join([f\"{i}:{col}\" for i, col in enumerate(df.columns)])\n",
        "        table_info += col_info + \"\\n\"\n",
        "\n",
        "        if df.shape[0] > 0:\n",
        "            preview_rows = min(2, df.shape[0])\n",
        "            table_info += f\"PREVIEW_FIRST_{preview_rows}_ROWS:\\n\"\n",
        "            for i in range(preview_rows):\n",
        "                row_preview = \" | \".join([str(df.iloc[i, j])[:20] for j in range(min(5, df.shape[1]))])\n",
        "                table_info += f\"  Row_{i}: {row_preview}\\n\"\n",
        "\n",
        "        if parsing_report:\n",
        "            try:\n",
        "                accuracy = getattr(parsing_report, 'accuracy', 'N/A')\n",
        "                if accuracy != 'N/A':\n",
        "                    table_info += f\"EXTRACTION_ACCURACY: {accuracy:.2f}\\n\"\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        table_info += \"TABLE_DATA_START\\n\"\n",
        "        table_csv = df.to_csv(index=False, na_rep='', quoting=1, escapechar='\\\\')\n",
        "        table_end = f\"TABLE_DATA_END\\nTABLE_END_{table_id}\\n\"\n",
        "\n",
        "        return table_info + table_csv + table_end\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error formatting table output: {e}\")\n",
        "        return f\"TABLE_START_{table_id}\\n{df.to_csv(index=False)}\\nTABLE_END_{table_id}\\n\"\n",
        "\n",
        "def camelot_extra_tables_enhanced(path: str) -> List[str]:\n",
        "    \"\"\"Enhanced table extraction using Camelot with better error handling\"\"\"\n",
        "    try:\n",
        "        import camelot\n",
        "    except ImportError:\n",
        "        logging.warning(\"Camelot not installed, skipping Camelot table parsing.\")\n",
        "        return []\n",
        "\n",
        "    extra_chunks = []\n",
        "    extracted_tables_fingerprints = set()\n",
        "\n",
        "    try:\n",
        "        logging.info(\"Starting Camelot table extraction...\")\n",
        "\n",
        "        # Stream mode extraction\n",
        "        try:\n",
        "            stream_tables = camelot.read_pdf(\n",
        "                path,\n",
        "                pages=\"all\",\n",
        "                flavor=\"stream\",\n",
        "                edge_tol=50,\n",
        "                row_tol=2,\n",
        "                column_tol=0\n",
        "            )\n",
        "\n",
        "            stream_count = 0\n",
        "            for i, table in enumerate(stream_tables):\n",
        "                if not table.df.empty and table.df.shape[0] > 0:\n",
        "                    table_fingerprint = generate_table_fingerprint(table.df)\n",
        "\n",
        "                    if table_fingerprint not in extracted_tables_fingerprints:\n",
        "                        cleaned_df = clean_table_data_improved(table.df)\n",
        "\n",
        "                        if is_valid_table_improved(cleaned_df):\n",
        "                            table_txt = format_table_output_improved(cleaned_df, f\"STREAM_{i}\", table.parsing_report)\n",
        "                            extra_chunks.append(table_txt)\n",
        "                            extracted_tables_fingerprints.add(table_fingerprint)\n",
        "                            stream_count += 1\n",
        "\n",
        "            logging.info(f\"Stream mode extracted {stream_count} valid tables\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Stream mode extraction failed: {e}\")\n",
        "\n",
        "        # Lattice mode extraction\n",
        "        try:\n",
        "            lattice_tables = camelot.read_pdf(\n",
        "                path,\n",
        "                pages=\"all\",\n",
        "                flavor=\"lattice\",\n",
        "                line_scale=15,\n",
        "                line_tol=2,\n",
        "                joint_tol=2\n",
        "            )\n",
        "\n",
        "            lattice_count = 0\n",
        "            for i, table in enumerate(lattice_tables):\n",
        "                if not table.df.empty and table.df.shape[0] > 0:\n",
        "                    table_fingerprint = generate_table_fingerprint(table.df)\n",
        "\n",
        "                    if table_fingerprint not in extracted_tables_fingerprints:\n",
        "                        cleaned_df = clean_table_data_improved(table.df)\n",
        "\n",
        "                        if is_valid_table_improved(cleaned_df):\n",
        "                            table_txt = format_table_output_improved(cleaned_df, f\"LATTICE_{i}\", table.parsing_report)\n",
        "                            extra_chunks.append(table_txt)\n",
        "                            extracted_tables_fingerprints.add(table_fingerprint)\n",
        "                            lattice_count += 1\n",
        "\n",
        "            logging.info(f\"Lattice mode extracted {lattice_count} additional unique tables\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Lattice mode extraction failed: {e}\")\n",
        "\n",
        "        total_extracted = len(extra_chunks)\n",
        "        logging.info(f\"Camelot extraction completed: {total_extracted} total unique tables extracted\")\n",
        "        return extra_chunks\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Camelot table extraction failed: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "amKWHBAHaGuk"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ 文本分块 ============\n",
        "def split_into_chunks(full_text: str, max_tokens: int) -> List[str]:\n",
        "    \"\"\"Split text into chunks based on token limit\"\"\"\n",
        "    paragraphs = [p for p in full_text.split(\"\\n\") if p.strip()]\n",
        "    chunks, current = [], []\n",
        "    current_tokens = 0\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        para_tokens = len(enc.encode(paragraph))\n",
        "\n",
        "        if current_tokens + para_tokens > max_tokens and current:\n",
        "            chunks.append(\"\\n\".join(current))\n",
        "            current = [paragraph]\n",
        "            current_tokens = para_tokens\n",
        "        else:\n",
        "            current.append(paragraph)\n",
        "            current_tokens += para_tokens\n",
        "\n",
        "    if current:\n",
        "        chunks.append(\"\\n\".join(current))\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "ZSLAWykuaGxs"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ 系统提示词 ============\n",
        "UNIVERSAL_SYSTEM_PROMPT = textwrap.dedent(\"\"\"\n",
        "    You are a professional ESG data analyst specializing in extracting Key Performance Indicators (KPIs) from sustainability reports.\n",
        "\n",
        "    ## CRITICAL: What is a KPI?\n",
        "    A KPI MUST contain SPECIFIC NUMBERS, PERCENTAGES, or MEASURABLE QUANTITIES that demonstrate actual performance or concrete targets.\n",
        "\n",
        "    ## IMPORTANT: Table Data Processing Rules\n",
        "    When processing table data:\n",
        "    1. Pay close attention to column headers to identify the correct time periods\n",
        "    2. Match data values with their corresponding year columns\n",
        "    3. If you see table format like \"Metric, 2021, 2022\" - the first number after metric belongs to 2021, second to 2022\n",
        "    4. Look for table headers that indicate year columns (e.g., \"2020\", \"2021\", \"2022\")\n",
        "    5. Extract each year's data as separate KPIs\n",
        "    6. Avoid extracting the same KPI multiple times - consolidate similar metrics\n",
        "\n",
        "    ## ENHANCED: Advanced Table Processing\n",
        "    7. **EXTRACT ALL DATA POINTS**: For each table cell containing a number, create a separate KPI\n",
        "    8. **REGIONAL/LOCATION DATA**: Pay special attention to location-specific data (countries, regions, cities)\n",
        "    9. **WORKFORCE DATA**: Extract all employee numbers, headcount data, and demographic information\n",
        "    10. **INCOMPLETE DATA**: Extract available data even if some cells are empty or missing\n",
        "    11. **TOTALS AND SUBTOTALS**: Always extract total values and aggregated numbers\n",
        "\n",
        "    ## ✅ VALID KPI EXAMPLES:\n",
        "    - \"Achieved 89.4% reuse and recycle rate for cloud hardware in 2023\"\n",
        "    - \"Diverted over 18,537 metric tons of waste from landfills in 2023\"\n",
        "    - \"Reduced single-use plastics in product packaging to 2.7%\"\n",
        "    - \"Contracted 19 GW of new renewable energy across 16 countries in 2024\"\n",
        "    - \"Provided clean water access to over 1.5 million people in 2023\"\n",
        "    - \"Protected 15,849 acres of land—exceeding target by more than 30%\"\n",
        "    - \"Allocated 761 million toward innovative climate technologies\"\n",
        "    - \"Achieved 80% renewable energy operations by 2024\"\n",
        "    - \"Water replenishment projects estimated to provide over 25 million cubic meters\"\n",
        "    - \"Exceeded annual target to divert 75% of construction waste by reaching 85%\"\n",
        "    - \"Board independence: 78% of directors\"\n",
        "    - \"Women in senior leadership increased to 35% in 2023\"\n",
        "    - \"Employee engagement score: 87% in annual survey\"\n",
        "    - \"Reduced greenhouse gas emissions by 50% compared to 2019 baseline\"\n",
        "    - \"Zero workplace fatalities achieved for third consecutive year\"\n",
        "    - \"Training completion rate: 98% for mandatory compliance courses\"\n",
        "    - \"Supplier ESG assessments completed for 95% of tier-1 suppliers\"\n",
        "    - \"Customer satisfaction rating: 4.6 out of 5.0\"\n",
        "    - \"Data breach incidents: 0 material breaches in 2023\"\n",
        "\n",
        "    ## ❌ NOT KPIs (DO NOT EXTRACT):\n",
        "    - \"Microsoft will require select suppliers to use carbon-free electricity by 2030\"\n",
        "    - \"The company plans to expand Sustainability Manager capabilities\"\n",
        "    - \"We are launching two new Circular Centers in 2023\"\n",
        "    - \"The organization established a new climate innovation fund\"\n",
        "    - \"Microsoft introduced enhanced data governance solutions\"\n",
        "    - \"Updated guidebook to include guidance on corporate responsibility\"\n",
        "    - \"Plans to publish new ESG strategy\"\n",
        "    - \"Implemented a new recycling program\"\n",
        "    - \"Conducted sustainability training sessions\"\n",
        "    - \"Launched employee wellness programs\"\n",
        "    - \"Committed to reducing emissions\"\n",
        "    - \"Focusing on environmental performance\"\n",
        "    - \"Established sustainability committee\"\n",
        "    - \"The company operates facilities in multiple regions\"\n",
        "    - \"Our supply chain includes thousands of vendors globally\"\n",
        "    - Any text without specific numbers, percentages, or quantifiable metrics\n",
        "    - Duplicate or repeated metrics (extract only once per time period)\n",
        "    - Any statement that describes business operations rather than performance outcomes\n",
        "\n",
        "    ## KPI Categories:\n",
        "    ### Environmental:\n",
        "    - **Carbon_Climate**: GHG emissions, carbon footprint, emission reductions, climate targets, scope 1/2/3 emissions, carbon intensity, carbon offsets, TCFD alignment\n",
        "    - **Energy**: Energy consumption, renewable energy percentage, energy efficiency, energy intensity, MWh, GWh, energy savings, fossil fuel usage\n",
        "    - **Water**: Water withdrawal, water consumption, water intensity, water recycling, water reuse, water stress, water discharge quality\n",
        "    - **Waste**: Waste generation, recycling rates, diversion percentages, hazardous waste, non-hazardous waste, zero waste to landfill, e-waste, incineration\n",
        "    - **Biodiversity**: Protected areas, species conservation, habitat restoration, biodiversity impact assessments, land use, ecosystem restoration\n",
        "    - **Circular_Economy**: Recycling rates, material recovery, circular design, raw materials usage, renewable materials, packaging waste\n",
        "    - **Materials**: Raw materials consumption, recycled content, sustainable materials, material intensity, sustainable sourcing\n",
        "\n",
        "    ### Social:\n",
        "    - **Workforce_Diversity**: Employee demographics, gender diversity, age diversity, ethnic diversity, disability inclusion, LGBTQ+ inclusion, workforce composition\n",
        "    - **Gender_Equality**: Women in leadership, gender pay ratio, parental leave return rates, gender representation, female employees percentage\n",
        "    - **Disability_Inclusion**: Employees with disabilities, accessibility compliance, inclusive workplace design, disability support programs\n",
        "    - **Health_Safety**: Lost Time Injury Frequency Rate (LTIFR), Total Recordable Incident Rate (TRIR), fatalities, workplace illness, safety training hours, PPE compliance, emergency drills\n",
        "    - **Employee_Wellbeing**: Employee satisfaction, retention rates, turnover rates, training hours, wellness programs, mental health services, work-life balance\n",
        "    - **Community_Engagement**: Corporate volunteering, social investment, community impact assessments, local hiring, stakeholder engagement activities\n",
        "    - **Human_Rights**: Child labor incidents, forced labor, human rights due diligence, freedom of association, grievance mechanisms, labor audits\n",
        "    - **Labor_Rights**: Collective bargaining coverage, labor complaints resolution, supplier labor audits, working conditions, fair wages\n",
        "    - **Customer_Safety**: Product safety incidents, customer satisfaction, accessibility features, safety recalls, quality metrics\n",
        "    - **Supply_Chain_Social**: Supplier assessments, sustainable sourcing, supplier code compliance, supply chain audits\n",
        "\n",
        "    ### Governance:\n",
        "    - **Board_Governance**: Board independence, board diversity, CEO-chair separation, board ESG expertise, board composition, director tenure\n",
        "    - **Executive_Compensation**: ESG-linked compensation, executive pay ratios, compensation disclosure, incentive structures\n",
        "    - **Ethics_Compliance**: Code of conduct training, corruption incidents, bribery cases, fines and penalties, whistleblower reports, anti-corruption assessments\n",
        "    - **Transparency_Disclosure**: ESG reporting coverage, third-party assurance, political contributions disclosure, GRI/SASB/TCFD compliance\n",
        "    - **Risk_Management**: Risk assessments, mitigation measures, climate risk disclosure, operational risk management\n",
        "    - **Cybersecurity_Data**: Cybersecurity breaches, data privacy policies, cybersecurity training, GDPR compliance, data protection measures\n",
        "    - **Supply_Chain_Governance**: Supplier ESG screening, supplier audits, procurement ESG clauses, vendor compliance rates\n",
        "\n",
        "    ## MANDATORY Requirements:\n",
        "    1. MUST contain specific numbers (e.g., 25%, 15,000, 2.5M, 8.5%, 0.3 per million hours)\n",
        "    2. MUST relate to measurable sustainability outcomes\n",
        "    3. MUST have time reference (year, period, or deadline)\n",
        "    4. MUST be performance-focused (results, not activities or descriptions)\n",
        "    5. MUST NOT be future plans or operational descriptions\n",
        "\n",
        "    ## Output Format:\n",
        "    Return a JSON array. Each KPI must contain:\n",
        "    ```json\n",
        "    {\n",
        "        \"kpi_text\": \"Complete original sentence with the quantifiable metric\",\n",
        "        \"kpi_theme\": \"Environmental/Social/Governance\",\n",
        "        \"kpi_category\": \"Specific category from above list\",\n",
        "        \"quantitative_value\": \"The specific number/percentage extracted\",\n",
        "        \"unit\": \"Unit of measurement (%, tonnes, employees, etc.)\",\n",
        "        \"time_period\": \"Time reference (2023, annual, by 2030, etc.)\",\n",
        "        \"target_or_actual\": \"Target/Actual/Both\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "    ## Additional Instructions:\n",
        "    - If a sentence includes a comparison value, such as a baseline, previous year, or other historical/target data (e.g., \"Compared to 32,395 MWh in 2020\"), extract it as a **separate KPI**.\n",
        "    - Do NOT store the comparison in any other field — just create another valid KPI from it.\n",
        "    - Avoid merging multiple numerical values into one KPI unless they are clearly part of the same metric (e.g., male: X, female: Y).\n",
        "\n",
        "    ## STRICT FILTERING:\n",
        "    - Return empty array [] if no quantifiable KPIs found\n",
        "    - Only extract text that contains specific measurable values\n",
        "    - Ignore all qualitative statements, plans, and descriptions\n",
        "    - Focus only on numerical performance data\n",
        "\n",
        "    Now analyze the following text for sustainability KPIs:\n",
        "\"\"\").strip()\n",
        "\n",
        "# 🔥 新增：增强的图像分析Prompt\n",
        "ENHANCED_IMAGE_KPI_SYSTEM_PROMPT = textwrap.dedent(\"\"\"\n",
        "    You are an expert data analyst specializing in extracting quantifiable KPI data from charts, graphs, and data visualizations in sustainability reports.\n",
        "\n",
        "    ## CRITICAL INSTRUCTION: ALWAYS EXTRACT NUMERICAL VALUES\n",
        "\n",
        "    **Your primary task is to extract the ACTUAL NUMBERS and PERCENTAGES visible in charts, not just descriptions.**\n",
        "\n",
        "    ## MISSION:\n",
        "    Extract ALL quantifiable data points from charts and graphs, including:\n",
        "    - Bar charts (vertical/horizontal)\n",
        "    - Pie charts and donut charts\n",
        "    - Line charts and trend graphs\n",
        "    - Stacked charts and combo charts\n",
        "    - Tables with numerical data\n",
        "    - Infographics with statistics\n",
        "    - Gauge charts and dashboards\n",
        "\n",
        "    ## DETAILED ANALYSIS INSTRUCTIONS:\n",
        "\n",
        "    ### For PIE CHARTS:\n",
        "    1. Read percentage labels on each slice\n",
        "    2. If no labels visible, estimate based on slice size\n",
        "    3. Identify what each slice represents (categories)\n",
        "    4. Extract each slice as separate KPI\n",
        "    5. **MUST read the percentage labels on each slice** - Look for numbers like 64%, 33%, 68%, 30%, etc.\n",
        "    6. **If percentages are visible on the chart, extract them exactly**\n",
        "    7. **If no labels visible, estimate based on slice size using these guidelines:**\n",
        "       - 90° slice = 25%\n",
        "       - 180° slice = 50%\n",
        "       - 270° slice = 75%\n",
        "       - Full circle = 100%\n",
        "    8. **Each slice MUST have a specific percentage value in the final output**\n",
        "\n",
        "    ### For BAR CHARTS:\n",
        "    1. Read Y-axis scale carefully (units, increments)\n",
        "    2. Estimate bar heights using grid lines and scale\n",
        "    3. Read X-axis labels (years, categories, regions)\n",
        "    4. Extract each bar as separate KPI\n",
        "    5. Pay attention to grouped/stacked bars\n",
        "\n",
        "    ### For LINE CHARTS:\n",
        "    1. Read data points at intersection of grid lines\n",
        "    2. Follow trend lines to extract values for each time period\n",
        "    3. Use Y-axis scale for value estimation\n",
        "    4. Extract each data point as separate KPI\n",
        "\n",
        "    ### For TABLES:\n",
        "    1. Read all numerical values in cells\n",
        "    2. Match values with row and column headers\n",
        "    3. Extract each cell with numerical data as KPI\n",
        "\n",
        "    ## MANDATORY VALUE EXTRACTION RULES:\n",
        "\n",
        "    **RULE 1**: Every KPI MUST contain a specific numerical value (percentage, amount, count, etc.)\n",
        "    **RULE 2**: For charts with categories, you MUST find and extract the quantitative values for each category\n",
        "    **RULE 3**: Never create KPIs without specific numbers - descriptions alone are incomplete\n",
        "    **RULE 4**: Include complete context: what + how much + when/where if available\n",
        "\n",
        "\n",
        "    ## VALUE ESTIMATION GUIDELINES:\n",
        "    - Use proportional analysis: if a bar reaches 80% of scale maximum, calculate 80% of max value\n",
        "    - For pie charts: estimate slice angles (90° = 25%, 180° = 50%, etc.)\n",
        "    - Cross-reference with any visible data labels or legends\n",
        "    - Be conservative but reasonably accurate in estimates\n",
        "\n",
        "    ## CHART IDENTIFICATION:\n",
        "    First identify the chart type, then apply appropriate extraction method.\n",
        "    Look for:\n",
        "    - Axes and scales\n",
        "    - Data labels and legends\n",
        "    - Grid lines for reference\n",
        "    - Color coding and patterns\n",
        "    - Title and subtitle information\n",
        "\n",
        "    ## OUTPUT FORMAT:\n",
        "    Return a JSON array. For each data point found:\n",
        "    ```json\n",
        "    {\n",
        "        \"kpi_text\": \"Complete description with the ACTUAL NUMERICAL VALUE included\",\n",
        "        \"kpi_theme\": \"Environmental/Social/Governance\",\n",
        "        \"kpi_category\": \"Specific category based on content\",\n",
        "        \"quantitative_value\": \"The exact number/percentage (e.g., '64', '33.5', '68')\",\n",
        "        \"unit\": \"% / tonnes / employees / MWh / USD / etc.\",\n",
        "        \"time_period\": \"2021/2020/2022/Year/period/etc if identifiable\",\n",
        "        \"target_or_actual\": \"Actual\",\n",
        "        \"chart_type\": \"pie_chart/bar_chart/line_chart/table/etc\",\n",
        "        \"estimation_confidence\": \"High/Medium/Low\",\n",
        "        \"chart_title\": \"Chart title if visible\",\n",
        "        \"data_source\": \"Legend or source if visible\"\n",
        "    }\n",
        "\n",
        "    ```\n",
        "    ## EXAMPLES of CORRECT vs INCORRECT extraction:\n",
        "\n",
        "    ### ❌ INCORRECT (incomplete - missing numerical values):\n",
        "    ```json\n",
        "    {\n",
        "        \"kpi_text\": \"Energy consumption by facility type\",\n",
        "        \"quantitative_value\": \"\",\n",
        "        \"unit\": \"%\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "    ### ✅ CORRECT (complete with specific values):\n",
        "    ```json\n",
        "    {\n",
        "        \"kpi_text\": \"Office buildings account for 45% of total energy consumption\",\n",
        "        \"quantitative_value\": \"45\",\n",
        "        \"unit\": \"%\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "    ### ❌ INCORRECT (category without value):\n",
        "    ```json\n",
        "    {\n",
        "        \"kpi_text\": \"Renewable energy percentage by region\",\n",
        "        \"quantitative_value\": \"\",\n",
        "        \"unit\": \"%\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "    ### ✅ CORRECT (specific regional data):\n",
        "    ```json\n",
        "    {\n",
        "        \"kpi_text\": \"North America achieved 78% renewable energy usage\",\n",
        "        \"quantitative_value\": \"78\",\n",
        "        \"unit\": \"%\"\n",
        "    }\n",
        "    ```\n",
        "    ## QUALITY ASSURANCE CHECKLIST:\n",
        "    Before returning results, verify:\n",
        "    - ✅ Every KPI contains a specific numerical value\n",
        "    - ✅ Chart categories are paired with their quantitative data\n",
        "    - ✅ KPI descriptions are complete and self-explanatory\n",
        "    - ✅ Units are correctly identified and specified\n",
        "    - ✅ Context (time, location, category) is preserved when available\n",
        "    - Each KPI must have a specific numerical value\n",
        "    - Context must be clear and self-contained\n",
        "    - Avoid extracting the same data point multiple times\n",
        "    - Focus on sustainability/ESG metrics when possible\n",
        "\n",
        "    ## VALUE ESTIMATION GUIDELINES:\n",
        "    - **High confidence**: Numbers clearly visible in image\n",
        "    - **Medium confidence**: Numbers estimated using chart scales/grid lines\n",
        "    - **Low confidence**: Values approximated from proportional analysis\n",
        "    - **If no numerical data is visible, return empty array []**\n",
        "\n",
        "    ## IMPORTANT NOTES:\n",
        "    - Extract ALL visible data points, not just main highlights\n",
        "    - Include context in descriptions (e.g., \"According to pie chart showing emission sources\")\n",
        "    - If values are not clearly visible, make reasonable estimates and mark confidence as \"Low\"\n",
        "    - Return empty array [] ONLY if image contains no charts/graphs with quantifiable data\n",
        "    - For multi-year data, create separate KPIs for each year\n",
        "    - Pay special attention to small text and numbers\n",
        "    - Focus on extracting actual performance data, not just identifying chart elements\n",
        "    - If you can see numbers in the image, you MUST extract them\n",
        "    - Pie chart percentages are usually the most important data points\n",
        "    - Return empty array [] ONLY if no numerical data is visible\n",
        "\n",
        "    Now analyze the provided image and extract ALL quantifiable KPI data points:\n",
        "\"\"\").strip()"
      ],
      "metadata": {
        "id": "IonQ4PK2aGzy"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ KPI提取函数 ============\n",
        "def extract_page_from_chunk(chunk: str) -> str:\n",
        "    \"\"\"Extract page information from chunk\"\"\"\n",
        "    # Look for PAGE_X_TEXT: format\n",
        "    page_matches = re.findall(r'PAGE_(\\d+)_TEXT:', chunk)\n",
        "    if page_matches:\n",
        "        pages = [int(p) for p in page_matches]\n",
        "        if len(pages) == 1:\n",
        "            return str(pages[0])\n",
        "        else:\n",
        "            return f\"{min(pages)}-{max(pages)}\"\n",
        "\n",
        "    # Look for TABLE_START_PAGE_X_\n",
        "    table_matches = re.findall(r'TABLE_START_PAGE_(\\d+)_', chunk)\n",
        "    if table_matches:\n",
        "        pages = [int(p) for p in table_matches]\n",
        "        if len(pages) == 1:\n",
        "            return str(pages[0])\n",
        "        else:\n",
        "            return f\"{min(pages)}-{max(pages)}\"\n",
        "\n",
        "    return \"Unknown\"\n",
        "\n",
        "def contains_procedural_language(text: str) -> bool:\n",
        "    \"\"\"Check if text contains procedural language\"\"\"\n",
        "    procedural_words = [\n",
        "        'introduced', 'established', 'set up', 'implemented', 'created',\n",
        "        'launched', 'formed', 'built', 'installed', 'deployed',\n",
        "        'additionally introduced', 'procedure for', 'standardization management'\n",
        "    ]\n",
        "    text_lower = text.lower()\n",
        "    return any(word in text_lower for word in procedural_words)\n",
        "\n",
        "def is_data_fragment(kpi_text: str) -> bool:\n",
        "    \"\"\"Check if text is a meaningless data fragment\"\"\"\n",
        "    text = kpi_text.strip()\n",
        "\n",
        "    # Filter pure numbers or simple percentages without context\n",
        "    if re.match(r'^\\d+\\.?\\d*%?$', text):\n",
        "        return True\n",
        "\n",
        "    # Filter very short text (less than 4 meaningful words)\n",
        "    meaningful_words = [word for word in text.split() if len(word) > 2 and not word.isdigit()]\n",
        "    if len(meaningful_words) < 3:\n",
        "        return True\n",
        "\n",
        "    # Filter text with only numbers and common connecting words\n",
        "    words = text.lower().split()\n",
        "    non_functional_words = [word for word in words if word not in ['in', 'of', 'the', 'and', 'or', 'to', 'for', 'with', 'by']]\n",
        "    if len(non_functional_words) < 3:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def standardize_kpi_universal(kpi_item: Dict) -> Dict:\n",
        "    \"\"\"Universal KPI data standardization\"\"\"\n",
        "    standardized = kpi_item.copy()\n",
        "\n",
        "    # Standardize numerical formats\n",
        "    quantitative_value = str(standardized.get('quantitative_value', '')).strip()\n",
        "    kpi_text = standardized.get('kpi_text', '').lower()\n",
        "\n",
        "    # Smart handling of percentage formats\n",
        "    if quantitative_value and quantitative_value.replace('.', '').replace('-', '').replace(',', '').isdigit():\n",
        "        # Check if original text suggests this is a percentage\n",
        "        percentage_indicators = ['percent', 'percentage', '%', 'rate', 'ratio', 'proportion', 'share']\n",
        "        if any(indicator in kpi_text for indicator in percentage_indicators):\n",
        "            if not quantitative_value.endswith('%'):\n",
        "                standardized['quantitative_value'] = quantitative_value + '%'\n",
        "                if not standardized.get('unit'):\n",
        "                    standardized['unit'] = '%'\n",
        "\n",
        "    # Ensure unit field consistency\n",
        "    if '%' in str(standardized.get('quantitative_value', '')):\n",
        "        standardized['unit'] = '%'\n",
        "\n",
        "    # Clean and normalize KPI text\n",
        "    kpi_text_original = standardized.get('kpi_text', '').strip()\n",
        "    # Remove extra spaces and newlines\n",
        "    kpi_text_cleaned = ' '.join(kpi_text_original.split())\n",
        "    standardized['kpi_text'] = kpi_text_cleaned\n",
        "\n",
        "    return standardized\n",
        "\n",
        "def generate_universal_metric_key(kpi_item: Dict) -> str:\n",
        "    \"\"\"Generate universal metric key for deduplication\"\"\"\n",
        "    try:\n",
        "        # Extract core elements\n",
        "        category = kpi_item.get('kpi_category', '').lower().strip()\n",
        "        value = str(kpi_item.get('quantitative_value', '')).replace('%', '').replace(',', '').strip()\n",
        "        time_period = kpi_item.get('time_period', '').lower().strip()\n",
        "        unit = kpi_item.get('unit', '').lower().strip()\n",
        "\n",
        "        # Extract key semantic information from KPI text\n",
        "        kpi_text = kpi_item.get('kpi_text', '').lower()\n",
        "\n",
        "        # Extract primary number (for more precise matching)\n",
        "        numbers_in_text = re.findall(r'\\d+\\.?\\d*', kpi_text)\n",
        "        primary_number = numbers_in_text[0] if numbers_in_text else value\n",
        "\n",
        "        # Generate semantic signature: extract keywords from text\n",
        "        # Remove common stop words\n",
        "        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'}\n",
        "\n",
        "        # Extract keywords (length>2 and not stop words)\n",
        "        words = re.findall(r'\\b\\w+\\b', kpi_text)\n",
        "        key_words = [word for word in words if len(word) > 2 and word not in stop_words and not word.isdigit()]\n",
        "\n",
        "        # Sort keywords to ensure consistency\n",
        "        key_words = sorted(set(key_words))[:5]  # Take at most 5 keywords\n",
        "        semantic_signature = '_'.join(key_words)\n",
        "\n",
        "        # Build universal metric key\n",
        "        key_components = []\n",
        "\n",
        "        if category:\n",
        "            key_components.append(f\"cat:{category}\")\n",
        "        if primary_number:\n",
        "            key_components.append(f\"val:{primary_number}\")\n",
        "        if time_period:\n",
        "            key_components.append(f\"time:{time_period}\")\n",
        "        if unit:\n",
        "            key_components.append(f\"unit:{unit}\")\n",
        "        if semantic_signature:\n",
        "            key_components.append(f\"sem:{semantic_signature}\")\n",
        "\n",
        "        # Generate final key\n",
        "        metric_key = \"|\".join(key_components)\n",
        "\n",
        "        # If all components are empty, use text hash\n",
        "        if not metric_key:\n",
        "            metric_key = f\"hash:{hash(kpi_text)}\"\n",
        "\n",
        "        return metric_key\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error generating universal metric key: {e}\")\n",
        "        # Fallback to text hash\n",
        "        return f\"fallback:{hash(kpi_item.get('kpi_text', ''))}\"\n",
        "\n",
        "def extract_kpi_from_chunk_universal(chunk: str) -> List[Dict]:\n",
        "    \"\"\"Universal KPI extraction function for various sustainability reports\"\"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=MODEL_NAME,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": UNIVERSAL_SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": f\"\"\"Extract ALL KPIs from this text. Requirements:\n",
        "\n",
        "1. Create COMPLETE, MEANINGFUL KPI descriptions with full context\n",
        "2. DO NOT extract standalone numbers without explanatory text\n",
        "3. Include all relevant context (time, location, metric type, etc.)\n",
        "4. Use consistent formatting for similar metrics\n",
        "5. Ensure each KPI is self-explanatory\n",
        "\n",
        "Text to analyze:\n",
        "{chunk}\"\"\"}\n",
        "            ],\n",
        "            temperature=0.0,\n",
        "            max_tokens=4000,\n",
        "            timeout=60\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Clean potential markdown formatting\n",
        "        if content.startswith('```json'):\n",
        "            content = content[7:]\n",
        "        if content.endswith('```'):\n",
        "            content = content[:-3]\n",
        "\n",
        "        if not content.strip().startswith(\"[\"):\n",
        "            logging.warning(f\"API response not JSON list: {content[:100]}...\")\n",
        "            return []\n",
        "\n",
        "        result = json.loads(content)\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            logging.warning(\"API response is not a list format\")\n",
        "            return []\n",
        "\n",
        "        # Extract page information\n",
        "        page_number = extract_page_from_chunk(chunk)\n",
        "\n",
        "        # Universal validation and deduplication logic\n",
        "        validated_result = []\n",
        "        seen_metrics = set()\n",
        "\n",
        "        for item in result:\n",
        "            if isinstance(item, dict) and 'kpi_text' in item and 'kpi_theme' in item:\n",
        "                if item['kpi_text'].strip() and item['kpi_theme'].strip():\n",
        "\n",
        "                    # Check procedural language\n",
        "                    if contains_procedural_language(item['kpi_text']):\n",
        "                        logging.debug(f\"Procedural statement filtered: {item['kpi_text'][:50]}...\")\n",
        "                        continue\n",
        "\n",
        "                    # Filter meaningless data fragments\n",
        "                    if is_data_fragment(item['kpi_text']):\n",
        "                        logging.debug(f\"Data fragment filtered: {item['kpi_text']}\")\n",
        "                        continue\n",
        "\n",
        "                    # Standardize KPI data\n",
        "                    standardized_item = standardize_kpi_universal(item)\n",
        "\n",
        "                    # Add page information\n",
        "                    standardized_item['source_page'] = page_number\n",
        "                    standardized_item['source_type'] = 'text'\n",
        "\n",
        "                    # Universal deduplication mechanism\n",
        "                    metric_key = generate_universal_metric_key(standardized_item)\n",
        "\n",
        "                    if metric_key not in seen_metrics:\n",
        "                        validated_result.append(standardized_item)\n",
        "                        seen_metrics.add(metric_key)\n",
        "                        logging.debug(f\"KPI extracted: {standardized_item['kpi_text'][:80]}...\")\n",
        "                    else:\n",
        "                        logging.debug(f\"Duplicate metric filtered: {standardized_item['kpi_text'][:50]}...\")\n",
        "\n",
        "        logging.info(f\"Chunk processed: {len(validated_result)} unique KPIs extracted\")\n",
        "        return validated_result\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        logging.warning(f\"JSON parsing failed: {e}\\nContent: {content[:300]}...\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logging.error(f\"API call failed: {e}\")\n",
        "        return []\n",
        "\n",
        "def post_process_kpis_universal(kpis: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"Universal KPI post-processing for various report types\"\"\"\n",
        "    if not kpis:\n",
        "        return kpis\n",
        "\n",
        "    # Step 1: Deduplication based on metric keys\n",
        "    unique_kpis_dict = {}\n",
        "\n",
        "    for kpi in kpis:\n",
        "        metric_key = generate_universal_metric_key(kpi)\n",
        "\n",
        "        if metric_key not in unique_kpis_dict:\n",
        "            unique_kpis_dict[metric_key] = kpi\n",
        "        else:\n",
        "            # If duplicate, keep the more complete KPI description\n",
        "            existing_kpi = unique_kpis_dict[metric_key]\n",
        "            current_kpi = kpi\n",
        "\n",
        "            # Compare KPI text completeness\n",
        "            if len(current_kpi.get('kpi_text', '')) > len(existing_kpi.get('kpi_text', '')):\n",
        "                unique_kpis_dict[metric_key] = current_kpi\n",
        "                logging.debug(f\"Replaced with more complete KPI: {current_kpi.get('kpi_text', '')[:50]}...\")\n",
        "            else:\n",
        "                logging.debug(f\"Kept existing KPI: {existing_kpi.get('kpi_text', '')[:50]}...\")\n",
        "\n",
        "    # Step 2: Text similarity-based secondary deduplication\n",
        "    final_kpis = list(unique_kpis_dict.values())\n",
        "\n",
        "    # Use text similarity to check remaining potential duplicates\n",
        "    final_unique_kpis = []\n",
        "\n",
        "    for current_kpi in final_kpis:\n",
        "        is_duplicate = False\n",
        "        current_text = current_kpi.get('kpi_text', '')\n",
        "\n",
        "        for existing_kpi in final_unique_kpis:\n",
        "            existing_text = existing_kpi.get('kpi_text', '')\n",
        "\n",
        "            # Calculate text similarity\n",
        "            similarity = calculate_text_similarity(current_text, existing_text)\n",
        "\n",
        "            # If similarity is very high, consider it duplicate\n",
        "            if similarity > 0.8:\n",
        "                is_duplicate = True\n",
        "                logging.debug(f\"Text similarity duplicate filtered: {current_text[:50]}...\")\n",
        "                break\n",
        "\n",
        "        if not is_duplicate:\n",
        "            final_unique_kpis.append(current_kpi)\n",
        "\n",
        "    logging.info(f\"Universal post-processing: {len(final_unique_kpis)}/{len(kpis)} KPIs retained\")\n",
        "    return final_unique_kpis\n",
        "\n",
        "def calculate_text_similarity(text1: str, text2: str) -> float:\n",
        "    \"\"\"Calculate similarity between two texts\"\"\"\n",
        "    # Normalize texts\n",
        "    norm1 = ' '.join(text1.lower().split())\n",
        "    norm2 = ' '.join(text2.lower().split())\n",
        "\n",
        "    # Word sets\n",
        "    words1 = set(norm1.split())\n",
        "    words2 = set(norm2.split())\n",
        "\n",
        "    if len(words1) == 0 or len(words2) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate intersection and union\n",
        "    intersection = len(words1.intersection(words2))\n",
        "    union = len(words1.union(words2))\n",
        "\n",
        "    return intersection / union if union > 0 else 0.0\n",
        "\n",
        "def validate_kpi_quality(kpis: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"Additional quality validation for extracted KPIs with relaxed filtering\"\"\"\n",
        "    if not ENABLE_QUALITY_VALIDATION:\n",
        "        return kpis\n",
        "\n",
        "    quality_kpis = []\n",
        "\n",
        "    for kpi in kpis:\n",
        "        kpi_text = kpi.get('kpi_text', '').lower()\n",
        "\n",
        "        # Exclude \"planned tone\" KPIs (not actual performance)\n",
        "        is_future_statement = any(word in kpi_text for word in [\n",
        "            'will', 'aim to', 'plan to', 'planning to', 'intend to',\n",
        "            'is expected to', 'is scheduled to', 'expects to', 'expected to',\n",
        "            'targeting', 'propose to', 'going to', 'shall', 'to be installed'\n",
        "        ])\n",
        "        if is_future_statement:\n",
        "            logging.debug(f\"KPI rejected (future plan): {kpi_text[:100]}...\")\n",
        "            continue\n",
        "\n",
        "        # Filter procedural language\n",
        "        if contains_procedural_language(kpi_text):\n",
        "            logging.debug(f\"KPI rejected (procedural language): {kpi_text[:100]}...\")\n",
        "            continue\n",
        "\n",
        "        # Filter for phrases like \"place name + percentage\" (not ESG KPIs, but distribution descriptions)\n",
        "        geo_percent_pattern = re.compile(r\"^[a-z\\s,:%-]+(?:\\s)?\\d{1,3}%$\")\n",
        "        if geo_percent_pattern.match(kpi_text.strip()) and len(kpi_text.strip().split()) <= 6:\n",
        "            logging.debug(f\"KPI rejected (geo+percent short form): {kpi_text}\")\n",
        "            continue\n",
        "\n",
        "        # Verb whitelist: must include action verbs\n",
        "        allowed_kpi_verbs = [\n",
        "            'reduce', 'reduced', 'achieve', 'achieved', 'improve', 'improved',\n",
        "            'diverted', 'trained', 'invested', 'decreased', 'increased',\n",
        "            'consumed', 'emitted', 'saved', 'reached', 'attained', 'completed',\n",
        "            'recorded', 'cut', 'lowered', 'targeted', 'complied', 'avoided',\n",
        "            'used', 'recycled', 'sourced', 'returned', 'measured', 'maintained',\n",
        "            'reported', 'accounted', 'utilized', 'were', 'was'  # Add state verbs\n",
        "        ]\n",
        "        if not any(verb in kpi_text for verb in allowed_kpi_verbs):\n",
        "            logging.debug(f\"KPI rejected (no action verb): {kpi_text[:100]}...\")\n",
        "            continue\n",
        "\n",
        "        # Greylist verbs (action words but not necessarily performance words) - remove problematic words\n",
        "        graylist_verbs = [\n",
        "            'launched',  # Keep some potentially useful words, but remove obvious procedural words\n",
        "            'formed', 'opened', 'started'\n",
        "        ]\n",
        "\n",
        "        contains_graylist = any(verb in kpi_text for verb in graylist_verbs)\n",
        "\n",
        "        # Check for quantitative indicators\n",
        "        has_numbers = any(char.isdigit() for char in kpi_text)\n",
        "        has_percentage = '%' in kpi_text\n",
        "\n",
        "        # Extended units and measurement indicators\n",
        "        has_units = any(unit in kpi_text for unit in [\n",
        "            'tonnes', 'tons', 'kg', 'mwh', 'kwh', 'gwh', 'litres', 'liters', 'gallons',\n",
        "            'employees', 'hours', 'million', 'billion', 'thousand', 'm³', 'co2e', 'tco2e',\n",
        "            'dollars', 'usd', 'eur', 'gbp', 'incidents', 'rate', 'ratio', 'intensity',\n",
        "            'frequency', 'recordable', 'fatalities', 'injuries', 'directors', 'board',\n",
        "            'workforce', 'leadership', 'diversity', 'inclusion', 'satisfaction', 'retention',\n",
        "            'turnover', 'training', 'safety', 'ltifr', 'trir', 'compliance', 'audit',\n",
        "            'assessment', 'screening', 'supplier', 'breach', 'violation', 'disclosure',\n",
        "            'assurance', 'coverage', 'participation', 'completion', 'investment',\n",
        "            'volunteering', 'engagement', 'grievance', 'whistleblower', 'compensation',\n",
        "            'people', 'staff', 'workers', 'positions', 'roles', 'headcount', 'fte',\n",
        "            'performance', 'score', 'index', 'metric', 'level', 'amount', 'value',\n",
        "            'average', 'median', 'total', 'sum', 'count', 'number', 'quantity'\n",
        "        ])\n",
        "\n",
        "        # More flexible time reference detection\n",
        "        has_time_ref = any(time_word in kpi_text for time_word in [\n",
        "            '2019', '2020', '2021', '2022', '2023', '2024', '2025', '2026', '2027', '2028', '2029', '2030',\n",
        "            '2031', '2032', '2033', '2034', '2035', '2040', '2045', '2050',\n",
        "            'annual', 'yearly', 'year', 'quarter', 'month', 'by', 'target', 'baseline', 'fy',\n",
        "            'per year', 'per annum', 'quarterly', 'monthly', 'daily', 'future', 'deadline',\n",
        "            'period', 'reporting', 'current', 'previous', 'next', 'last', 'this'\n",
        "        ])\n",
        "\n",
        "        # Enhanced sustainability context detection\n",
        "        has_sustainability_context = any(sus_word in kpi_text for sus_word in [\n",
        "            # Environmental keywords\n",
        "            'emission', 'carbon', 'energy', 'renewable', 'waste', 'water', 'recycl',\n",
        "            'environmental', 'ghg', 'scope', 'climate', 'biodiversity', 'circular',\n",
        "            'materials', 'intensity', 'consumption', 'efficiency', 'footprint',\n",
        "            'sustainable', 'sustainability', 'green', 'clean', 'eco', 'offset',\n",
        "            'tcfd', 'nature', 'habitat', 'ecosystem', 'pollution', 'discharge',\n",
        "            'electricity', 'gas', 'fuel', 'solar', 'wind', 'hydro', 'nuclear',\n",
        "\n",
        "            # Social keywords\n",
        "            'safety', 'training', 'employee', 'diversity', 'community', 'social',\n",
        "            'workforce', 'gender', 'women', 'female', 'male', 'disability', 'disabled',\n",
        "            'inclusion', 'equity', 'equality', 'lgbtq', 'minorities', 'ethnic',\n",
        "            'health', 'wellbeing', 'wellness', 'satisfaction', 'retention', 'turnover',\n",
        "            'injury', 'incident', 'fatality', 'ltifr', 'trir', 'recordable',\n",
        "            'human rights', 'labor', 'child labor', 'forced labor', 'slavery',\n",
        "            'freedom', 'association', 'collective bargaining', 'grievance',\n",
        "            'volunteering', 'investment', 'hiring', 'local', 'stakeholder',\n",
        "            'customer', 'supplier', 'supply chain', 'accessibility', 'parental',\n",
        "            'mental health', 'ppe', 'emergency', 'drill', 'compliance',\n",
        "            'people', 'staff', 'workers', 'employment', 'job', 'career',\n",
        "            'leadership', 'management', 'senior', 'executive', 'promotion',\n",
        "\n",
        "            # Governance keywords\n",
        "            'governance', 'board', 'director', 'independent', 'chair', 'ceo',\n",
        "            'executive', 'compensation', 'pay', 'ethics', 'compliance', 'corruption',\n",
        "            'bribery', 'code of conduct', 'whistleblower', 'transparency',\n",
        "            'disclosure', 'reporting', 'assurance', 'audit', 'risk', 'management',\n",
        "            'cybersecurity', 'data', 'privacy', 'gdpr', 'breach', 'policy',\n",
        "            'screening', 'assessment', 'due diligence', 'political', 'contribution',\n",
        "            'gri', 'sasb', 'oversight', 'expertise', 'separation', 'incentive',\n",
        "            'fine', 'penalty', 'violation', 'resolution', 'anti-corruption',\n",
        "\n",
        "            # General business performance that could be sustainability-related\n",
        "            'performance', 'quality', 'delivery', 'customer', 'service', 'product',\n",
        "            'operation', 'facility', 'site', 'location', 'region', 'business'\n",
        "        ])\n",
        "\n",
        "        # If it is a greylist verb sentence, but there is no performance content such as numbers, units, time, etc. → delete\n",
        "        if contains_graylist and not (has_numbers or has_units or has_percentage or has_time_ref or has_sustainability_context):\n",
        "            logging.debug(f\"KPI rejected (graylist verb, no quantitative data): {kpi_text[:100]}...\")\n",
        "            continue\n",
        "\n",
        "        # More lenient quality scoring - only require numbers and either units/percentage OR time reference OR sustainability context\n",
        "        basic_requirements = has_numbers and (has_percentage or has_units or has_time_ref or has_sustainability_context)\n",
        "\n",
        "        # Additional check for obvious ESG relevance\n",
        "        is_esg_relevant = any(esg_word in kpi_text for esg_word in [\n",
        "            'emission', 'carbon', 'energy', 'waste', 'water', 'renewable', 'employee',\n",
        "            'safety', 'training', 'diversity', 'governance', 'board', 'compliance',\n",
        "            'sustainability', 'environmental', 'social', 'ghg', 'co2', 'workforce',\n",
        "            'gender', 'health', 'injury', 'incident', 'ethics', 'transparency'\n",
        "        ])\n",
        "\n",
        "        if basic_requirements or is_esg_relevant:\n",
        "            quality_kpis.append(kpi)\n",
        "            logging.debug(f\"KPI accepted: {kpi_text[:100]}...\")\n",
        "        else:\n",
        "            logging.debug(f\"KPI filtered out for quality: {kpi_text[:100]}...\")\n",
        "\n",
        "    logging.info(f\"Quality validation: {len(quality_kpis)}/{len(kpis)} KPIs passed\")\n",
        "    return quality_kpis"
      ],
      "metadata": {
        "id": "sB_kDyLZaG1o"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ 图像处理函数 ============\n",
        "def extract_numeric_spans(page):\n",
        "    text_dict = page.get_text(\"dict\")\n",
        "    nums = []\n",
        "    for block in text_dict[\"blocks\"]:\n",
        "        for line in block.get(\"lines\", []):\n",
        "            for span in line.get(\"spans\", []):\n",
        "                s = span[\"text\"].strip()\n",
        "                if re.match(r\"[\\d,.]+%?$\", s):          # 纯数字或数字+%\n",
        "                    nums.append({\n",
        "                        \"text\": s,\n",
        "                        \"bbox\": span[\"bbox\"],           # (x0,y0,x1,y1)\n",
        "                        \"font\": span[\"size\"]\n",
        "                    })\n",
        "    return nums\n",
        "\n",
        "def extract_images_from_pdf_fixed(pdf_path: str) -> List[Dict]:\n",
        "    \"\"\"Extract images from PDF using PyMuPDF\"\"\"\n",
        "    images = []\n",
        "\n",
        "    try:\n",
        "        pdf_document = fitz.open(pdf_path)\n",
        "\n",
        "        for page_num in range(len(pdf_document)):\n",
        "            page = pdf_document[page_num]\n",
        "            image_list = page.get_images()\n",
        "\n",
        "            # 🔥 新增：同时提取页面截图作为备选\n",
        "            page_pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 高分辨率\n",
        "            page_img = Image.frombytes(\"RGB\", [page_pix.width, page_pix.height], page_pix.samples)\n",
        "\n",
        "            # 添加整页截图\n",
        "            images.append({\n",
        "                'image': page_img,\n",
        "                'page_number': page_num + 1,\n",
        "                'width': page_img.width,\n",
        "                'height': page_img.height,\n",
        "                'image_index': 'full_page',\n",
        "                'type': 'full_page'\n",
        "            })\n",
        "\n",
        "\n",
        "            for img_index, img in enumerate(image_list):\n",
        "                try:\n",
        "                    xref = img[0]\n",
        "                    base_image = pdf_document.extract_image(xref)\n",
        "                    image_bytes = base_image[\"image\"]\n",
        "\n",
        "                    image = Image.open(BytesIO(image_bytes))\n",
        "\n",
        "                    # Convert to RGB if needed\n",
        "                    if image.mode in ['RGBA', 'LA']:\n",
        "                        background = Image.new('RGB', image.size, (255, 255, 255))\n",
        "                        if image.mode == 'RGBA':\n",
        "                            background.paste(image, mask=image.split()[-1])\n",
        "                        else:\n",
        "                            background.paste(image)\n",
        "                        image = background\n",
        "                    elif image.mode != 'RGB':\n",
        "                        image = image.convert('RGB')\n",
        "\n",
        "                    # Filter small images\n",
        "                    if image.width >= 50 and image.height >= 50:\n",
        "                        images.append({\n",
        "                            'image': image,\n",
        "                            'page_number': page_num + 1,\n",
        "                            'width': image.width,\n",
        "                            'height': image.height,\n",
        "                            'image_index': img_index,\n",
        "                            'type': 'extracted'  # 🔥 新增类型标识\n",
        "                        })\n",
        "\n",
        "                except Exception as e:\n",
        "                    logging.warning(f\"Error extracting image {img_index} from page {page_num + 1}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        pdf_document.close()\n",
        "        logging.info(f\"Extracted {len(images)} images from PDF\")\n",
        "        return images\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error extracting images from PDF: {e}\")\n",
        "        return []\n",
        "\n",
        "def image_to_base64_fixed(image: Image.Image) -> str:\n",
        "    \"\"\"Convert image to base64 with error handling\"\"\"\n",
        "    try:\n",
        "        if image.mode not in ['RGB', 'L']:\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        # Resize large images\n",
        "        max_size = (1536, 1536)\n",
        "        if image.width > max_size[0] or image.height > max_size[1]:\n",
        "            # 计算缩放比例，保持长宽比\n",
        "            ratio = min(max_size[0]/image.width, max_size[1]/image.height)\n",
        "            new_size = (int(image.width * ratio), int(image.height * ratio))\n",
        "            image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "        buffered = BytesIO()\n",
        "        image.save(buffered, format=\"JPEG\", quality=95)\n",
        "        img_str = base64.b64encode(buffered.getvalue()).decode()\n",
        "\n",
        "        return img_str\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error converting image to base64: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "Vfegv4osaG3b"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "#  多裁剪 / 多分辨率生成器（支持裁剪参数为 0）\n",
        "# ------------------------------------------------------------\n",
        "from itertools import product\n",
        "\n",
        "def generate_image_variants(img: Image.Image,\n",
        "                            max_side_full: int = 1200,\n",
        "                            crop_size: int = 768,\n",
        "                            stride: int = 512) -> List[Tuple[Image.Image, str]]:\n",
        "    \"\"\"\n",
        "    返回 [(variant_image, variant_tag), ...]\n",
        "    variant_tag 取值: original / resized / crop_{row}_{col}\n",
        "    \"\"\"\n",
        "    variants = []\n",
        "\n",
        "    # 0) 原图\n",
        "    variants.append((img, \"original\"))\n",
        "\n",
        "    # 1) 缩放（若原图过大）\n",
        "    w, h = img.size\n",
        "    if max(w, h) > max_side_full:\n",
        "        scale = max_side_full / float(max(w, h))\n",
        "        resized = img.resize((int(w * scale), int(h * scale)), Image.Resampling.LANCZOS)\n",
        "        variants.append((resized, \"resized\"))\n",
        "    else:\n",
        "        resized = img  # 没缩放就保持原图\n",
        "        variants.append((resized, \"resized\"))  # 统一加上 resized 版本\n",
        "\n",
        "    # 2) 滑窗裁剪（裁剪尺寸或步长为 0 时跳过）\n",
        "    if crop_size > 0 and stride > 0:\n",
        "        base_img = variants[-1][0]\n",
        "        bw, bh = base_img.size\n",
        "        if bw > crop_size or bh > crop_size:\n",
        "            xs = list(range(0, max(bw - crop_size, 1), stride)) + [bw - crop_size]\n",
        "            ys = list(range(0, max(bh - crop_size, 1), stride)) + [bh - crop_size]\n",
        "            for r, c in product(range(len(ys)), range(len(xs))):\n",
        "                x, y = xs[c], ys[r]\n",
        "                crop = base_img.crop((x, y, x + crop_size, y + crop_size))\n",
        "                # 过滤纯色区域\n",
        "                if np.array(crop.convert('L')).std() < 5:\n",
        "                    continue\n",
        "                variants.append((crop, f\"crop_{r}_{c}\"))\n",
        "\n",
        "    return variants"
      ],
      "metadata": {
        "id": "ma9D4crJaG5z"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------\n",
        "# 📊 替代 plotclassifier 的图表识别函数（Hugging Face 模型）\n",
        "# ---------------------------------------------\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
        "import torch\n",
        "# 🔧 修复：使用CLIP模型进行图表识别\n",
        "def setup_chart_classifier():\n",
        "    \"\"\"设置图表分类器\"\"\"\n",
        "    try:\n",
        "        from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "        # 加载CLIP模型\n",
        "        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "        def is_chart_image_clip(image: Image.Image) -> bool:\n",
        "            \"\"\"使用CLIP判断是否是图表\"\"\"\n",
        "            try:\n",
        "                # 定义图表相关的文本描述\n",
        "                chart_labels = [\n",
        "                    \"a chart\", \"a graph\", \"a bar chart\", \"a pie chart\",\n",
        "                    \"a line graph\", \"a table\", \"data visualization\",\n",
        "                    \"statistics\", \"a diagram\", \"an infographic\"\n",
        "                ]\n",
        "\n",
        "                # 处理输入\n",
        "                inputs = processor(\n",
        "                    text=chart_labels,\n",
        "                    images=image,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True\n",
        "                )\n",
        "\n",
        "                # 获取预测结果\n",
        "                outputs = model(**inputs)\n",
        "                logits_per_image = outputs.logits_per_image\n",
        "                probs = logits_per_image.softmax(dim=1)\n",
        "\n",
        "                # 如果任意图表标签概率大于0.25，认为是图表\n",
        "                max_prob = probs.max().item()\n",
        "                is_chart = max_prob > 0.25\n",
        "\n",
        "                logging.debug(f\"CLIP图表识别: 最高概率={max_prob:.3f}, 结果={is_chart}\")\n",
        "                return is_chart\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"CLIP图表识别失败: {e}\")\n",
        "                # 降级到统计方法\n",
        "                gray = image.convert('L')\n",
        "                return np.array(gray).std() > 15\n",
        "\n",
        "        logging.info(\"✅ 使用CLIP模型进行图表识别\")\n",
        "        return is_chart_image_clip\n",
        "\n",
        "    except ImportError:\n",
        "        logging.warning(\"CLIP模型不可用，使用统计方法\")\n",
        "        def is_chart_image_stats(image: Image.Image) -> bool:\n",
        "            \"\"\"统计方法判断是否是图表\"\"\"\n",
        "            try:\n",
        "                gray = image.convert('L')\n",
        "                std_dev = np.array(gray).std()\n",
        "                return std_dev > 15\n",
        "            except:\n",
        "                return True\n",
        "\n",
        "        return is_chart_image_stats\n",
        "    except Exception as e:\n",
        "        logging.error(f\"设置图表分类器失败: {e}\")\n",
        "        def is_chart_image_fallback(image: Image.Image) -> bool:\n",
        "            return True  # 保守策略：有疑问就分析\n",
        "        return is_chart_image_fallback\n",
        "\n",
        "# 初始化图表分类器\n",
        "is_chart_image = setup_chart_classifier()\n",
        "\n",
        "\n",
        "def extract_kpi_from_image_fixed(image: Image.Image, page_number: int, image_type: str = 'extracted') -> List[Dict]:\n",
        "    \"\"\"Extract KPIs from image with improved error handling\"\"\"\n",
        "    try:\n",
        "        # 🔥 新增：预过滤：检查是否可能是图表\n",
        "        if not is_chart_image(image):\n",
        "            logging.debug(f\"Image on page {page_number} filtered out (not likely a chart)\")\n",
        "            return []\n",
        "\n",
        "        base64_image = image_to_base64_fixed(image)\n",
        "        if not base64_image:\n",
        "            return []\n",
        "\n",
        "        # 🔥 更改：使用增强的prompt\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": ENHANCED_IMAGE_KPI_SYSTEM_PROMPT  # 🔥 使用新的prompt\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\n",
        "                            \"type\": \"text\",\n",
        "                            # 🔥 新增：详细的用户指令\n",
        "                            \"text\": \"\"\"Analyze this image carefully for quantifiable performance data.\n",
        "\n",
        "IMPORTANT ANALYSIS PRINCIPLES:\n",
        "\n",
        "1. **Chart Type Recognition**:\n",
        "   - Stacked charts: Multiple colors/patterns layered in same position\n",
        "   - Grouped charts: Multiple elements side by side at same position\n",
        "   - Simple charts: One data point per position\n",
        "\n",
        "2. **Value Extraction Rules**:\n",
        "   - For STACKED charts: Read each layer separately, NOT the total height\n",
        "   - For GROUPED charts: Read each element individually\n",
        "   - For SIMPLE charts: Read data point values directly\n",
        "\n",
        "3. **Data Relevance Filter**:\n",
        "   ✅ EXTRACT: Performance outcomes, efficiency metrics, reduction rates, satisfaction scores, compliance rates\n",
        "   ❌ SKIP: Certification counts, project timelines, implementation schedules, organizational charts, process flows\n",
        "\n",
        "4. **Quality Standards**:\n",
        "   - Only extract clear, quantifiable performance indicators\n",
        "   - Each data point must have complete context\n",
        "   - If uncertain about values, don't estimate\n",
        "   - If chart shows mainly operational/administrative data, return empty array\n",
        "\n",
        "Please analyze this chart step by step:\n",
        "- First identify the chart type\n",
        "- Then determine if it contains performance KPIs\n",
        "- Finally extract all relevant performance data points\n",
        "\n",
        "Focus on measurable outcomes and achievements, not counts or processes.\"\"\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"type\": \"image_url\",\n",
        "                            \"image_url\": {\n",
        "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
        "                                \"detail\": \"high\"\n",
        "                            }\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.1,\n",
        "            max_tokens=4000,\n",
        "            timeout=60\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message.content.strip()\n",
        "\n",
        "        if not content:\n",
        "            return []\n",
        "\n",
        "        # Clean formatting\n",
        "        if content.startswith('```json'):\n",
        "            content = content[7:]\n",
        "        if content.endswith('```'):\n",
        "            content = content[:-3]\n",
        "\n",
        "        content = content.strip()\n",
        "\n",
        "        if not content.startswith(\"[\"):\n",
        "            logging.warning(f\"Image analysis response not JSON list: {content[:100]}...\")\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            result = json.loads(content)\n",
        "        except json.JSONDecodeError as e:\n",
        "            logging.warning(f\"JSON parsing failed for image analysis: {e}\")\n",
        "            return []\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            return []\n",
        "\n",
        "        # Process results\n",
        "        processed_result = []\n",
        "        for item in result:\n",
        "            if isinstance(item, dict) and 'kpi_text' in item:\n",
        "                if not item.get('kpi_text', '').strip():\n",
        "                    continue\n",
        "\n",
        "                item['source_page'] = page_number\n",
        "                item['source_type'] = 'image'\n",
        "                item['image_type'] = image_type  # 🔥 新增字段\n",
        "\n",
        "                # 🔥 更改：确保有chart标识\n",
        "                kpi_text = item['kpi_text']\n",
        "                if not any(marker in kpi_text.lower() for marker in ['chart', 'graph', 'table', 'figure']):\n",
        "                    chart_type = item.get('chart_type', 'chart')\n",
        "                    item['kpi_text'] = f\"[{chart_type.title()}] {kpi_text}\"\n",
        "\n",
        "                processed_result.append(item)\n",
        "\n",
        "        if processed_result:\n",
        "            logging.info(f\"✅ Extracted {len(processed_result)} KPIs from {image_type} on page {page_number}\")\n",
        "        else:\n",
        "            logging.debug(f\"❌ No KPIs found in {image_type} on page {page_number}\")\n",
        "\n",
        "        return processed_result\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error extracting KPIs from image: {e}\")\n",
        "        return []\n",
        "\n",
        "# 原函数：process_pdf_images_for_kpis_fixed\n",
        "# 完整替换为：\n",
        "\n",
        "def process_pdf_images_for_kpis_fixed(pdf_path: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    遍历 PDF 每一页：\n",
        "      • 对该页所有‘extracted’图像做多裁剪+Vision\n",
        "      • 若该页还没抓到 KPI，再对整页截图做 Vision\n",
        "    \"\"\"\n",
        "    logging.info(\"Starting page-by-page image KPI extraction …\")\n",
        "\n",
        "    images = extract_images_from_pdf_fixed(pdf_path)\n",
        "    if not images:\n",
        "        return []\n",
        "\n",
        "    # 把图像按页聚合\n",
        "    page_dict = {}\n",
        "    for info in images:\n",
        "        pg = info[\"page_number\"]\n",
        "        page_dict.setdefault(pg, {\"extracted\": [], \"full\": None})\n",
        "        if info[\"type\"] == \"extracted\":\n",
        "            page_dict[pg][\"extracted\"].append(info[\"image\"])\n",
        "        else:                    # full_page\n",
        "            page_dict[pg][\"full\"] = info[\"image\"]\n",
        "\n",
        "    all_image_kpis: List[Dict] = []\n",
        "\n",
        "    # —— 逐页处理 ——\n",
        "    for pg in sorted(page_dict.keys()):\n",
        "        logging.info(f\"\\n=== Page {pg} ===\")\n",
        "        page_kpis: List[Dict] = []\n",
        "\n",
        "        # ① 单独提取的图\n",
        "        for idx, img in enumerate(page_dict[pg][\"extracted\"]):\n",
        "            for var_img, var_tag in generate_image_variants(img, 1200, 768, 512):\n",
        "                kpis = extract_kpi_from_image_fixed(\n",
        "                    var_img, pg, f\"extracted_{var_tag}\"\n",
        "                )\n",
        "                for k in kpis:\n",
        "                    key = generate_universal_metric_key(k)\n",
        "                    if key not in {generate_universal_metric_key(x) for x in page_kpis}:\n",
        "                        page_kpis.append(k)\n",
        "                time.sleep(0.8)\n",
        "\n",
        "        # ② 若仍为空，再分析整页\n",
        "        if not page_kpis and page_dict[pg][\"full\"] is not None:\n",
        "            for var_img, var_tag in generate_image_variants(\n",
        "                    page_dict[pg][\"full\"], 1200, 0, 0):   # 只做 original/resized\n",
        "                kpis = extract_kpi_from_image_fixed(\n",
        "                    var_img, pg, f\"full_{var_tag}\"\n",
        "                )\n",
        "                for k in kpis:\n",
        "                    key = generate_universal_metric_key(k)\n",
        "                    if key not in {generate_universal_metric_key(x) for x in page_kpis}:\n",
        "                        page_kpis.append(k)\n",
        "                time.sleep(1.0)\n",
        "\n",
        "        logging.info(f\"  → Page {pg} KPI count: {len(page_kpis)}\")\n",
        "        all_image_kpis.extend(page_kpis)\n",
        "\n",
        "    logging.info(f\"Image KPI extraction finished: {len(all_image_kpis)} KPIs from {len(page_dict)} pages\")\n",
        "    return all_image_kpis"
      ],
      "metadata": {
        "id": "F5ZLpwL8Zok_"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "N7yh-bSwZonF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c9e95d-5571-4959-fff1-b29abb1a2296"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ 主处理函数 ============\n",
        "def process_sustainability_report_with_enhanced_images(pdf_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Main processing function with image analysis\"\"\"\n",
        "    logging.info(\"Starting enhanced PDF processing with image analysis...\")\n",
        "\n",
        "    # Step 1: Text and table extraction\n",
        "    logging.info(\"Step 1/5: Reading PDF text and tables...\")\n",
        "    full_text = pdf_to_text_and_tables(pdf_path)\n",
        "\n",
        "    camelot_tables = camelot_extra_tables_enhanced(pdf_path)\n",
        "    if camelot_tables:\n",
        "        full_text += \"\\n\\n\" + \"\\n\\n\".join(camelot_tables)\n",
        "\n",
        "    logging.info(\"Step 2/5: Chunking text...\")\n",
        "    chunks = split_into_chunks(full_text, MAX_TOKENS_CHUNK)\n",
        "\n",
        "    logging.info(\"Step 3/5: Extracting KPIs from text...\")\n",
        "    text_kpis = []\n",
        "    for idx, chunk in enumerate(chunks, 1):\n",
        "        logging.info(f\"Processing text chunk {idx}/{len(chunks)}\")\n",
        "        if chunk.strip():\n",
        "            chunk_kpis = extract_kpi_from_chunk_universal(chunk)\n",
        "            text_kpis.extend(chunk_kpis)\n",
        "            if idx < len(chunks):\n",
        "                time.sleep(SLEEP_SEC)\n",
        "\n",
        "    # Step 4: Image KPI extraction\n",
        "    logging.info(\"Step 4/5: Extracting KPIs from images...\")\n",
        "    image_kpis = process_pdf_images_for_kpis_fixed(pdf_path)\n",
        "\n",
        "    # Step 5: Combine and process\n",
        "    logging.info(\"Step 5/5: Combining and processing all KPIs...\")\n",
        "\n",
        "    for kpi in text_kpis:\n",
        "        if 'source_type' not in kpi:\n",
        "            kpi['source_type'] = 'text'\n",
        "\n",
        "    all_kpis = text_kpis + image_kpis\n",
        "    all_kpis = post_process_kpis_universal(all_kpis)\n",
        "\n",
        "    df_auto = pd.DataFrame(all_kpis)\n",
        "\n",
        "    if not df_auto.empty:\n",
        "        if 'source_type' not in df_auto.columns:\n",
        "            df_auto['source_type'] = 'text'\n",
        "\n",
        "        initial_count = len(df_auto)\n",
        "        df_auto = df_auto.drop_duplicates(subset=['kpi_text'], keep='first')\n",
        "        final_count = len(df_auto)\n",
        "\n",
        "        logging.info(f\"Removed {initial_count - final_count} duplicate KPIs\")\n",
        "\n",
        "        try:\n",
        "            df_auto = df_auto.sort_values(['source_type', 'kpi_theme', 'kpi_category'], na_position='last')\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "        text_kpi_count = len([kpi for kpi in all_kpis if kpi.get('source_type', 'text') != 'image'])\n",
        "        image_kpi_count = len([kpi for kpi in all_kpis if kpi.get('source_type') == 'image'])\n",
        "\n",
        "        logging.info(f\"KPI Summary: {text_kpi_count} from text/tables, {image_kpi_count} from images\")\n",
        "\n",
        "    return df_auto\n"
      ],
      "metadata": {
        "id": "Mr1OIB2BZosj"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ 结果保存和比较函数 ============\n",
        "def infer_stakeholder(row) -> str:\n",
        "    \"\"\"Infer affected stakeholders based on KPI theme and category\"\"\"\n",
        "    theme = row.get('kpi_theme', '').lower()\n",
        "    category = row.get('kpi_category', '').lower()\n",
        "    kpi_text = row.get('kpi_text', '').lower()\n",
        "\n",
        "    if theme == 'environmental':\n",
        "        return \"Environment, Community, Future Generations\"\n",
        "    elif theme == 'social':\n",
        "        if 'employee' in category or 'workforce' in category or 'gender' in category:\n",
        "            return \"Employees\"\n",
        "        elif 'customer' in category or 'safety' in category:\n",
        "            return \"Customers, Community\"\n",
        "        elif 'community' in category:\n",
        "            return \"Local Communities\"\n",
        "        elif 'supply' in category or 'supplier' in kpi_text:\n",
        "            return \"Suppliers, Business Partners\"\n",
        "        else:\n",
        "            return \"Employees, Community\"\n",
        "    elif theme == 'governance':\n",
        "        if 'board' in category:\n",
        "            return \"Shareholders, Investors\"\n",
        "        elif 'cyber' in category or 'data' in category:\n",
        "            return \"Customers, Employees, Business Partners\"\n",
        "        else:\n",
        "            return \"Shareholders, Investors, Stakeholders\"\n",
        "    else:\n",
        "        return \"All Stakeholders\"\n",
        "\n",
        "def save_results(df_auto: pd.DataFrame, output_path: str, pdf_path: str = \"\") -> None:\n",
        "    \"\"\"Save results to Excel file with proper formatting\"\"\"\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True)\n",
        "\n",
        "        if not df_auto.empty:\n",
        "            # Add metadata columns\n",
        "            pdf_filename = os.path.basename(pdf_path) if pdf_path else \"Unknown\"\n",
        "            df_auto['PDF file name'] = pdf_filename\n",
        "            df_auto['Title of the report'] = \"\"\n",
        "\n",
        "            if 'source_page' in df_auto.columns:\n",
        "                df_auto['Absolute Page Number'] = df_auto['source_page']\n",
        "                df_auto = df_auto.drop('source_page', axis=1)\n",
        "            else:\n",
        "                df_auto['Absolute Page Number'] = \"Unknown\"\n",
        "\n",
        "            df_auto['Impacted Stakeholder'] = df_auto.apply(infer_stakeholder, axis=1)\n",
        "\n",
        "            # Reorder columns\n",
        "            original_columns = [col for col in df_auto.columns if col not in\n",
        "                              ['PDF file name', 'Title of the report', 'Absolute Page Number', 'Impacted Stakeholder']]\n",
        "            new_column_order = ['PDF file name', 'Title of the report', 'Absolute Page Number', 'Impacted Stakeholder'] + original_columns\n",
        "            df_auto = df_auto[new_column_order]\n",
        "\n",
        "        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
        "            df_auto.to_excel(writer, sheet_name='Auto_KPIs', index=False)\n",
        "\n",
        "            if not df_auto.empty:\n",
        "                # Theme summary\n",
        "                theme_summary = df_auto.groupby('kpi_theme').size().reset_index(name='count')\n",
        "                theme_summary.to_excel(writer, sheet_name='Theme_Summary', index=False)\n",
        "\n",
        "                # Category summary\n",
        "                category_summary = df_auto.groupby(['kpi_theme', 'kpi_category']).size().reset_index(name='count')\n",
        "                category_summary.to_excel(writer, sheet_name='Category_Summary', index=False)\n",
        "\n",
        "        logging.info(f\"Results saved to {output_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving results: {e}\")\n",
        "\n",
        "def compare_with_manual_kpis(df_auto: pd.DataFrame, manual_xlsx_path: str) -> None:\n",
        "    \"\"\"Compare automatically extracted KPIs with manually annotated ones\"\"\"\n",
        "    if not os.path.exists(manual_xlsx_path):\n",
        "        logging.info(\"Manual KPI file not found, skipping comparison.\")\n",
        "        return\n",
        "\n",
        "    logging.info(\"Comparing with manual KPIs...\")\n",
        "\n",
        "    try:\n",
        "        df_manual = pd.read_excel(manual_xlsx_path)\n",
        "\n",
        "        if 'kpi_text' not in df_manual.columns:\n",
        "            logging.warning(\"Manual KPI file missing 'kpi_text' column\")\n",
        "            return\n",
        "\n",
        "        manual_kpis = set(df_manual['kpi_text'].astype(str).str.strip())\n",
        "        auto_kpis = set(df_auto['kpi_text'].astype(str).str.strip())\n",
        "\n",
        "        only_auto = auto_kpis - manual_kpis\n",
        "        only_manual = manual_kpis - auto_kpis\n",
        "        common = auto_kpis & manual_kpis\n",
        "\n",
        "        print(f\"\\n=== KPI Comparison Results ===\")\n",
        "        print(f\"Common KPIs: {len(common)}\")\n",
        "        print(f\"Only in automatic extraction: {len(only_auto)}\")\n",
        "        print(f\"Only in manual annotation: {len(only_manual)}\")\n",
        "\n",
        "        if only_auto:\n",
        "            print(f\"\\nKPIs found by model but not in manual annotation ({len(only_auto)}):\")\n",
        "            for kpi in sorted(only_auto):\n",
        "                if kpi.strip():\n",
        "                    print(f\"  - {kpi}\")\n",
        "\n",
        "        if only_manual:\n",
        "            print(f\"\\nKPIs in manual annotation but missed by model ({len(only_manual)}):\")\n",
        "            for kpi in sorted(only_manual):\n",
        "                if kpi.strip():\n",
        "                    print(f\"  - {kpi}\")\n",
        "\n",
        "        # Calculate metrics\n",
        "        if len(auto_kpis) > 0 and len(manual_kpis) > 0:\n",
        "            precision = len(common) / len(auto_kpis)\n",
        "            recall = len(common) / len(manual_kpis)\n",
        "            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "            print(f\"\\n=== Performance Metrics ===\")\n",
        "            print(f\"Precision: {precision:.3f}\")\n",
        "            print(f\"Recall: {recall:.3f}\")\n",
        "            print(f\"F1 Score: {f1_score:.3f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error comparing with manual KPIs: {e}\")"
      ],
      "metadata": {
        "id": "kucPzNTdZouV"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ 主执行函数 ============\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format=\"%(asctime)s - %(levelname)s: %(message)s\",\n",
        "        datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(PDF_PATH):\n",
        "            logging.error(f\"PDF file not found: {PDF_PATH}\")\n",
        "            return\n",
        "\n",
        "        # Process the PDF\n",
        "        df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
        "\n",
        "        # Save results\n",
        "        save_results(df_auto, EXPORT_AUTO_XLSX, PDF_PATH)\n",
        "\n",
        "        logging.info(f\"KPI extraction completed: {len(df_auto)} KPIs extracted\")\n",
        "\n",
        "        # Compare with manual annotations if available\n",
        "        if MANUAL_XLSX:\n",
        "            compare_with_manual_kpis(df_auto, MANUAL_XLSX)\n",
        "\n",
        "        # Display summary\n",
        "        if not df_auto.empty:\n",
        "            print(f\"\\n=== Extraction Summary ===\")\n",
        "            print(f\"Total KPIs extracted: {len(df_auto)}\")\n",
        "\n",
        "            # Source statistics\n",
        "            if 'source_type' in df_auto.columns:\n",
        "                source_counts = df_auto['source_type'].value_counts()\n",
        "                print(f\"From text/tables: {source_counts.get('text', 0)}\")\n",
        "                print(f\"From images/charts: {source_counts.get('image', 0)}\")\n",
        "\n",
        "            # Theme statistics\n",
        "            if 'kpi_theme' in df_auto.columns:\n",
        "                theme_counts = df_auto['kpi_theme'].value_counts()\n",
        "                print(f\"\\nKPI Distribution by Theme:\")\n",
        "                for theme, count in theme_counts.items():\n",
        "                    print(f\"  {theme}: {count}\")\n",
        "        else:\n",
        "            print(\"\\nNo KPIs were extracted from the document.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in main execution: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "uSueutkDZowK"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ 辅助功能函数 ============\n",
        "def install_dependencies():\n",
        "    \"\"\"Install required dependencies\"\"\"\n",
        "    try:\n",
        "        import subprocess\n",
        "        import sys\n",
        "\n",
        "        dependencies = [\n",
        "            \"openai\",\n",
        "            \"python-dotenv\",\n",
        "            \"pdfplumber\",\n",
        "            \"tiktoken\",\n",
        "            \"pandas\",\n",
        "            \"PyMuPDF\",\n",
        "            \"Pillow\",\n",
        "            \"openpyxl\"\n",
        "        ]\n",
        "\n",
        "        for dep in dependencies:\n",
        "            try:\n",
        "                __import__(dep.replace('-', '_'))\n",
        "                print(f\"✅ {dep} is already installed\")\n",
        "            except ImportError:\n",
        "                print(f\"Installing {dep}...\")\n",
        "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", dep])\n",
        "                print(f\"✅ Installed {dep}\")\n",
        "\n",
        "        # Optional Camelot installation\n",
        "        try:\n",
        "            import camelot\n",
        "            print(\"✅ Camelot is already installed\")\n",
        "        except ImportError:\n",
        "            print(\"Installing Camelot (optional)...\")\n",
        "            try:\n",
        "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"camelot-py[cv]\"])\n",
        "                print(\"✅ Installed Camelot\")\n",
        "            except:\n",
        "                print(\"⚠️ Camelot installation failed (optional dependency)\")\n",
        "\n",
        "        print(\"🎉 All dependencies checked/installed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error with dependencies: {e}\")\n",
        "\n",
        "def validate_environment():\n",
        "    \"\"\"Validate environment setup\"\"\"\n",
        "    issues = []\n",
        "\n",
        "    # Check API key\n",
        "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "        issues.append(\"OPENAI_API_KEY not found in environment variables\")\n",
        "\n",
        "    # Check PDF file\n",
        "    if not os.path.exists(PDF_PATH):\n",
        "        issues.append(f\"PDF file not found: {PDF_PATH}\")\n",
        "\n",
        "    # Check required imports\n",
        "    required_modules = ['openai', 'pdfplumber', 'pandas', 'tiktoken', 'PIL', 'fitz']\n",
        "    for module in required_modules:\n",
        "        try:\n",
        "            __import__(module)\n",
        "        except ImportError:\n",
        "            issues.append(f\"Required module '{module}' not installed\")\n",
        "\n",
        "    if issues:\n",
        "        print(\"❌ Environment validation failed:\")\n",
        "        for issue in issues:\n",
        "            print(f\"  - {issue}\")\n",
        "        return False\n",
        "    else:\n",
        "        print(\"✅ Environment validation passed\")\n",
        "        return True\n"
      ],
      "metadata": {
        "id": "qcCShhlea6ZK"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ 简化的执行接口 ============\n",
        "def run_kpi_extraction():\n",
        "    \"\"\"Simplified interface to run KPI extraction\"\"\"\n",
        "    print(\"🚀 Starting KPI extraction process...\")\n",
        "\n",
        "    # Validate environment\n",
        "    if not validate_environment():\n",
        "        print(\"Please fix the environment issues before running.\")\n",
        "        return\n",
        "\n",
        "    # Run main function\n",
        "    main()"
      ],
      "metadata": {
        "id": "Qw6B5H8ga6bl"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ 调试和测试功能 ============\n",
        "def test_text_extraction_only():\n",
        "    \"\"\"Test only text extraction without images\"\"\"\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    try:\n",
        "        # Extract text and tables\n",
        "        full_text = pdf_to_text_and_tables(PDF_PATH)\n",
        "        camelot_tables = camelot_extra_tables_enhanced(PDF_PATH)\n",
        "\n",
        "        if camelot_tables:\n",
        "            full_text += \"\\n\\n\" + \"\\n\\n\".join(camelot_tables)\n",
        "\n",
        "        # Chunk text\n",
        "        chunks = split_into_chunks(full_text, MAX_TOKENS_CHUNK)\n",
        "\n",
        "        # Extract KPIs from first few chunks\n",
        "        test_kpis = []\n",
        "        for idx, chunk in enumerate(chunks[:3]):  # Test first 3 chunks\n",
        "            chunk_kpis = extract_kpi_from_chunk_universal(chunk)\n",
        "            test_kpis.extend(chunk_kpis)\n",
        "            time.sleep(SLEEP_SEC)\n",
        "\n",
        "        print(f\"Test extraction completed: {len(test_kpis)} KPIs found in first 3 chunks\")\n",
        "\n",
        "        for i, kpi in enumerate(test_kpis[:5]):  # Show first 5\n",
        "            print(f\"{i+1}. {kpi.get('kpi_text', 'No text')}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Test failed: {e}\")\n",
        "\n",
        "def debug_single_image_analysis(image_path: str):\n",
        "    \"\"\"Test single image analysis functionality\"\"\"\n",
        "    try:\n",
        "        from PIL import Image\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        print(f\"Analyzing image: {image_path}\")\n",
        "        print(f\"Image size: {image.width}x{image.height}\")\n",
        "\n",
        "        kpis = extract_kpi_from_image_fixed(image, 1)\n",
        "\n",
        "        print(f\"\\n=== Analysis Results ===\")\n",
        "        print(f\"Found {len(kpis)} KPIs:\")\n",
        "\n",
        "        for i, kpi in enumerate(kpis, 1):\n",
        "            print(f\"\\n{i}. {kpi.get('kpi_text', 'No text')}\")\n",
        "            print(f\"   Value: {kpi.get('quantitative_value', 'No value')}\")\n",
        "            print(f\"   Confidence: {kpi.get('estimation_confidence', 'Not specified')}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in debug analysis: {e}\")\n",
        "\n",
        "def process_text_only():\n",
        "    \"\"\"Process only text and tables, skip images\"\"\"\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    try:\n",
        "        logging.info(\"Starting text-only processing...\")\n",
        "\n",
        "        # Step 1: Text and table extraction\n",
        "        full_text = pdf_to_text_and_tables(PDF_PATH)\n",
        "        camelot_tables = camelot_extra_tables_enhanced(PDF_PATH)\n",
        "\n",
        "        if camelot_tables:\n",
        "            full_text += \"\\n\\n\" + \"\\n\\n\".join(camelot_tables)\n",
        "\n",
        "        # Step 2: Chunking\n",
        "        chunks = split_into_chunks(full_text, MAX_TOKENS_CHUNK)\n",
        "\n",
        "        # Step 3: Extract KPIs\n",
        "        all_kpis = []\n",
        "        for idx, chunk in enumerate(chunks, 1):\n",
        "            logging.info(f\"Processing chunk {idx}/{len(chunks)}\")\n",
        "            if chunk.strip():\n",
        "                chunk_kpis = extract_kpi_from_chunk_universal(chunk)\n",
        "                all_kpis.extend(chunk_kpis)\n",
        "                if idx < len(chunks):\n",
        "                    time.sleep(SLEEP_SEC)\n",
        "\n",
        "        # Post-processing\n",
        "        all_kpis = post_process_kpis_universal(all_kpis)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        df_auto = pd.DataFrame(all_kpis)\n",
        "\n",
        "        if not df_auto.empty:\n",
        "            df_auto = df_auto.drop_duplicates(subset=['kpi_text'], keep='first')\n",
        "\n",
        "        # Save results\n",
        "        text_only_output = \"text_only_\" + EXPORT_AUTO_XLSX\n",
        "        save_results(df_auto, text_only_output, PDF_PATH)\n",
        "\n",
        "        print(f\"Text-only processing completed: {len(df_auto)} KPIs extracted\")\n",
        "\n",
        "        return df_auto\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Text-only processing failed: {e}\")\n",
        "        return pd.DataFrame()"
      ],
      "metadata": {
        "id": "Oku3umuia6dK"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ 兼容性函数 ============\n",
        "def extract_kpi_from_chunk(chunk: str) -> List[Dict]:\n",
        "    \"\"\"Backward compatibility function\"\"\"\n",
        "    return extract_kpi_from_chunk_universal(chunk)\n",
        "\n",
        "def process_sustainability_report(pdf_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Backward compatibility function for text-only processing\"\"\"\n",
        "    return process_text_only()\n",
        "\n",
        "def process_sustainability_report_with_images(pdf_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Backward compatibility function for full processing\"\"\"\n",
        "    return process_sustainability_report_with_enhanced_images(pdf_path)\n"
      ],
      "metadata": {
        "id": "uhyRnexga6fA"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ 使用示例 ============\n",
        "def example_usage():\n",
        "    \"\"\"Usage examples\"\"\"\n",
        "    print(\"=== KPI Extraction Tool Usage Examples ===\\n\")\n",
        "\n",
        "    print(\"1. Full extraction (text + images):\")\n",
        "    print(\"   df_results = process_sustainability_report_with_enhanced_images(PDF_PATH)\")\n",
        "    print(\"   save_results(df_results, EXPORT_AUTO_XLSX, PDF_PATH)\\n\")\n",
        "\n",
        "    print(\"2. Text-only extraction:\")\n",
        "    print(\"   df_results = process_text_only()\")\n",
        "    print(\"   # Results automatically saved\\n\")\n",
        "\n",
        "    print(\"3. Simple run:\")\n",
        "    print(\"   run_kpi_extraction()  # Complete pipeline with validation\\n\")\n",
        "\n",
        "    print(\"4. Debug single component:\")\n",
        "    print(\"   test_text_extraction_only()  # Test first 3 chunks\")\n",
        "    print(\"   debug_single_image_analysis('path/to/image.jpg')\\n\")\n",
        "\n",
        "    print(\"5. Install dependencies:\")\n",
        "    print(\"   install_dependencies()  # Install all required packages\\n\")\n"
      ],
      "metadata": {
        "id": "fldjMEkaa6g1"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 调试代码 - 直接复制粘贴到你的代码末尾\n",
        "# ============================================================================\n",
        "\n",
        "# 方法1: 检查所有图像提取和识别情况\n",
        "def debug_method_1_check_image_detection():\n",
        "    \"\"\"检查PDF中的所有图像是否被正确提取，以及图表分类器是否工作正常\"\"\"\n",
        "    print(\"=== 方法1: 检查图像提取和图表识别 ===\")\n",
        "\n",
        "    # 创建调试文件夹\n",
        "    import os\n",
        "    debug_folder = \"debug_images_method1\"\n",
        "    os.makedirs(debug_folder, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        # 提取所有图像\n",
        "        images = extract_images_from_pdf_fixed(PDF_PATH)\n",
        "        print(f\"从PDF中提取到 {len(images)} 个图像\")\n",
        "\n",
        "        chart_count = 0\n",
        "        non_chart_count = 0\n",
        "\n",
        "        for i, img_info in enumerate(images):\n",
        "            page_num = img_info['page_number']\n",
        "            img_type = img_info['type']\n",
        "            image = img_info['image']\n",
        "\n",
        "            # 检查是否被识别为图表\n",
        "            is_chart = is_chart_image(image)\n",
        "\n",
        "            # 保存图像，文件名包含识别结果\n",
        "            chart_status = \"CHART\" if is_chart else \"NOT_CHART\"\n",
        "            filename = f\"{debug_folder}/page_{page_num}_{img_type}_{chart_status}_{i}.jpg\"\n",
        "            image.save(filename)\n",
        "\n",
        "            print(f\"图像 {i+1}: 页面{page_num}, 类型{img_type}, 尺寸{image.width}x{image.height}, 图表识别:{is_chart}\")\n",
        "\n",
        "            if is_chart:\n",
        "                chart_count += 1\n",
        "            else:\n",
        "                non_chart_count += 1\n",
        "\n",
        "        print(f\"\\n总结:\")\n",
        "        print(f\"- 被识别为图表的图像: {chart_count}\")\n",
        "        print(f\"- 未被识别为图表的图像: {non_chart_count}\")\n",
        "        print(f\"- 所有图像已保存到 {debug_folder} 文件夹\")\n",
        "        print(f\"- 请手动检查 NOT_CHART 的图像，看是否包含你缺失的饼图\")\n",
        "\n",
        "        return images\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"方法1执行出错: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return []\n",
        "\n",
        "# 方法2: 临时禁用图表分类器\n",
        "def debug_method_2_bypass_chart_filter():\n",
        "    \"\"\"完全禁用图表分类器，强制处理所有图像\"\"\"\n",
        "    print(\"=== 方法2: 禁用图表分类器 ===\")\n",
        "\n",
        "    # 保存原始的图表分类器函数\n",
        "    global is_chart_image\n",
        "    original_chart_classifier = is_chart_image\n",
        "\n",
        "    # 创建新的分类器（总是返回True）\n",
        "    def bypass_chart_classifier(image):\n",
        "        print(f\"  🔓 强制处理图像 (尺寸: {image.width}x{image.height})\")\n",
        "        return True\n",
        "\n",
        "    # 临时替换分类器\n",
        "    is_chart_image = bypass_chart_classifier\n",
        "\n",
        "    try:\n",
        "        print(\"开始重新提取图像KPI（已禁用图表过滤）...\")\n",
        "\n",
        "        # 重新运行图像处理\n",
        "        image_kpis = process_pdf_images_for_kpis_fixed(PDF_PATH)\n",
        "\n",
        "        print(f\"禁用过滤器后提取到 {len(image_kpis)} 个图像KPI\")\n",
        "\n",
        "        # 显示结果\n",
        "        for i, kpi in enumerate(image_kpis):\n",
        "            print(f\"{i+1}. 页面{kpi.get('source_page', 'Unknown')}: {kpi.get('kpi_text', 'No text')[:100]}\")\n",
        "\n",
        "        return image_kpis\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"方法2执行出错: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return []\n",
        "\n",
        "    finally:\n",
        "        # 恢复原始分类器\n",
        "        is_chart_image = original_chart_classifier\n",
        "        print(\"已恢复原始图表分类器\")\n",
        "\n",
        "# 方法3: 手动测试特定图像\n",
        "def debug_method_3_manual_test():\n",
        "    \"\"\"手动选择图像进行测试\"\"\"\n",
        "    print(\"=== 方法3: 手动测试特定图像 ===\")\n",
        "\n",
        "    try:\n",
        "        images = extract_images_from_pdf_fixed(PDF_PATH)\n",
        "        print(f\"找到 {len(images)} 个图像\")\n",
        "\n",
        "        # 显示所有图像信息\n",
        "        for i, img_info in enumerate(images):\n",
        "            page_num = img_info['page_number']\n",
        "            img_type = img_info['type']\n",
        "            image = img_info['image']\n",
        "            is_chart = is_chart_image(image)\n",
        "\n",
        "            print(f\"{i+1}. 页面{page_num}, 类型{img_type}, 尺寸{image.width}x{image.height}, 图表:{is_chart}\")\n",
        "\n",
        "        # 让用户选择要测试的图像\n",
        "        while True:\n",
        "            try:\n",
        "                choice = input(f\"\\n请选择要测试的图像编号 (1-{len(images)}, 输入0退出): \")\n",
        "                if choice == '0':\n",
        "                    break\n",
        "\n",
        "                img_index = int(choice) - 1\n",
        "                if 0 <= img_index < len(images):\n",
        "                    img_info = images[img_index]\n",
        "                    page_num = img_info['page_number']\n",
        "                    image = img_info['image']\n",
        "\n",
        "                    print(f\"\\n测试图像 {choice} (页面 {page_num})\")\n",
        "\n",
        "                    # 保存这个图像供检查\n",
        "                    test_filename = f\"test_image_{choice}_page_{page_num}.jpg\"\n",
        "                    image.save(test_filename)\n",
        "                    print(f\"图像已保存为: {test_filename}\")\n",
        "\n",
        "                    # 测试提取\n",
        "                    kpis = extract_kpi_from_image_fixed(image, page_num, \"manual_test\")\n",
        "\n",
        "                    print(f\"提取结果: {len(kpis)} 个KPI\")\n",
        "                    for j, kpi in enumerate(kpis):\n",
        "                        print(f\"  KPI {j+1}: {kpi.get('kpi_text', 'No text')}\")\n",
        "\n",
        "                else:\n",
        "                    print(\"无效的选择\")\n",
        "\n",
        "            except ValueError:\n",
        "                print(\"请输入有效的数字\")\n",
        "            except KeyboardInterrupt:\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"测试出错: {e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"方法3执行出错: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# 方法4: 简化的提取测试\n",
        "def debug_method_4_simple_test():\n",
        "    \"\"\"使用简化的方法测试图像提取\"\"\"\n",
        "    print(\"=== 方法4: 简化提取测试 ===\")\n",
        "\n",
        "    simple_prompt = \"\"\"请分析这个图表，提取所有的数字数据。\n",
        "\n",
        "返回JSON格式，每个数据点包含：\n",
        "{\n",
        "  \"description\": \"数据描述\",\n",
        "  \"value\": \"数值\",\n",
        "  \"unit\": \"单位\"\n",
        "}\n",
        "\n",
        "如果是饼图，请提取每个扇形的百分比。\n",
        "如果是柱状图，请提取每个柱子的数值。\n",
        "如果是表格，请提取每个数字。\"\"\"\n",
        "\n",
        "    try:\n",
        "        images = extract_images_from_pdf_fixed(PDF_PATH)\n",
        "\n",
        "        for i, img_info in enumerate(images[:5]):  # 只测试前5个图像\n",
        "            page_num = img_info['page_number']\n",
        "            image = img_info['image']\n",
        "\n",
        "            print(f\"\\n🔍 简化测试图像 {i+1} (页面 {page_num})\")\n",
        "\n",
        "            try:\n",
        "                base64_image = image_to_base64_fixed(image)\n",
        "                if not base64_image:\n",
        "                    continue\n",
        "\n",
        "                response = client.chat.completions.create(\n",
        "                    model=\"gpt-4o\",\n",
        "                    messages=[{\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": [\n",
        "                            {\"type\": \"text\", \"text\": simple_prompt},\n",
        "                            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
        "                        ]\n",
        "                    }],\n",
        "                    temperature=0.0,\n",
        "                    max_tokens=2000,\n",
        "                    timeout=30\n",
        "                )\n",
        "\n",
        "                content = response.choices[0].message.content.strip()\n",
        "                print(f\"API响应: {content[:300]}...\")\n",
        "\n",
        "                # 检查是否包含你要找的数据\n",
        "                if \"property type\" in content.lower() or \"service type\" in content.lower():\n",
        "                    print(\"🎉 可能找到了缺失的饼图数据！\")\n",
        "                    print(f\"完整响应: {content}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ 简化测试失败: {e}\")\n",
        "\n",
        "            time.sleep(1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"方法4执行出错: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# 调试主控制函数\n",
        "def run_debugging_session():\n",
        "    \"\"\"调试会话主控制函数\"\"\"\n",
        "    print(\"🔧 KPI提取调试会话\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            choice = input(\"\"\"\\n选择调试方法：\n",
        "1 - 检查所有图像的提取和识别情况\n",
        "2 - 禁用图表分类器，强制处理所有图像\n",
        "3 - 手动选择图像进行测试\n",
        "4 - 简化API测试（测试前5个图像）\n",
        "0 - 退出调试\n",
        "\n",
        "请输入选择 (0-4): \"\"\")\n",
        "\n",
        "            if choice == \"1\":\n",
        "                debug_method_1_check_image_detection()\n",
        "            elif choice == \"2\":\n",
        "                debug_method_2_bypass_chart_filter()\n",
        "            elif choice == \"3\":\n",
        "                debug_method_3_manual_test()\n",
        "            elif choice == \"4\":\n",
        "                debug_method_4_simple_test()\n",
        "            elif choice == \"0\":\n",
        "                print(\"退出调试会话\")\n",
        "                break\n",
        "            else:\n",
        "                print(\"无效选择，请重新输入\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n用户中断，退出调试\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"调试会话出错: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "# ============================================================================\n",
        "# 单独的快速测试函数（如果你不想用交互式界面）\n",
        "# ============================================================================\n",
        "\n",
        "def quick_debug():\n",
        "    \"\"\"快速调试 - 直接运行方法1\"\"\"\n",
        "    print(\"🚀 快速调试模式\")\n",
        "    debug_method_1_check_image_detection()\n",
        "\n",
        "# ============================================================================\n",
        "# 使用方法\n",
        "# ============================================================================\n",
        "\n",
        "# 在你的代码最后，现在可以运行以下任意一个：\n",
        "\n",
        "# 选项1: 交互式调试（推荐）\n",
        "# run_debugging_session()\n",
        "\n",
        "# 选项2: 快速调试，直接检查图像\n",
        "# quick_debug()\n",
        "\n",
        "# 选项3: 直接运行特定方法\n",
        "# debug_method_1_check_image_detection()"
      ],
      "metadata": {
        "id": "zXuWkRTCgfWE"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 测试函数：验证新的提示词是否有效\n",
        "def test_improved_prompt():\n",
        "    \"\"\"测试改进的提示词是否能正确提取百分比\"\"\"\n",
        "    print(\"=== 测试改进的提示词 ===\")\n",
        "\n",
        "    try:\n",
        "        # 提取页面2的全页图像\n",
        "        images = extract_images_from_pdf_fixed(PDF_PATH)\n",
        "        page2_image = None\n",
        "\n",
        "        for img_info in images:\n",
        "            if img_info['page_number'] == 2 and img_info['type'] == 'full_page':\n",
        "                page2_image = img_info['image']\n",
        "                break\n",
        "\n",
        "        if page2_image is None:\n",
        "            print(\"❌ 找不到页面2图像\")\n",
        "            return\n",
        "\n",
        "        print(f\"✅ 找到页面2图像，尺寸: {page2_image.width}x{page2_image.height}\")\n",
        "\n",
        "        # 使用改进的提示词测试\n",
        "        base64_image = image_to_base64_fixed(page2_image)\n",
        "\n",
        "        print(\"🔄 使用改进的提示词调用API...\")\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": ENHANCED_IMAGE_KPI_SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": \"\"\"分析这个页面，重点关注两个饼图：\n",
        "\n",
        "1. 上方饼图：\"Energy Use by Property Type 2021\"\n",
        "2. 下方饼图：\"Energy Use by Service Type 2021\"\n",
        "\n",
        "请提取每个饼图中每个扇形的具体百分比数值。确保包含实际的数字，不只是描述。\"\"\"},\n",
        "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\", \"detail\": \"high\"}}\n",
        "                ]}\n",
        "            ],\n",
        "            temperature=0.0,\n",
        "            max_tokens=4000,\n",
        "            timeout=90\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message.content.strip()\n",
        "        print(\"\\n📋 API响应:\")\n",
        "        print(content[:500] + \"...\" if len(content) > 500 else content)\n",
        "\n",
        "        # 尝试解析JSON\n",
        "        try:\n",
        "            if content.startswith('```json'):\n",
        "                content = content[7:]\n",
        "            if content.endswith('```'):\n",
        "                content = content[:-3]\n",
        "            content = content.strip()\n",
        "\n",
        "            if content.startswith('['):\n",
        "                result = json.loads(content)\n",
        "                print(f\"\\n✅ 成功解析JSON，找到 {len(result)} 个KPI\")\n",
        "\n",
        "                # 检查是否提取了具体的百分比\n",
        "                found_percentages = []\n",
        "                for kpi in result:\n",
        "                    if isinstance(kpi, dict):\n",
        "                        kpi_text = kpi.get('kpi_text', '')\n",
        "                        quantitative_value = kpi.get('quantitative_value', '')\n",
        "\n",
        "                        print(f\"KPI: {kpi_text}\")\n",
        "                        print(f\"  数值: {quantitative_value}\")\n",
        "\n",
        "                        # 检查是否包含期望的百分比\n",
        "                        if any(target in kpi_text.lower() for target in ['64%', '33%', '68%', '30%', 'healthcare center', 'medical office', 'electricity', 'fuel']):\n",
        "                            found_percentages.append(kpi)\n",
        "                            print(f\"  🎯 找到目标数据!\")\n",
        "                        print()\n",
        "\n",
        "                if found_percentages:\n",
        "                    print(f\"🎉 成功提取了 {len(found_percentages)} 个包含具体百分比的KPI!\")\n",
        "                    return True\n",
        "                else:\n",
        "                    print(\"❌ 仍然没有提取到具体的百分比数值\")\n",
        "                    return False\n",
        "            else:\n",
        "                print(\"❌ API响应不是JSON格式\")\n",
        "                return False\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"❌ JSON解析失败: {e}\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 测试失败: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "# 快速修复函数：替换提示词并重新运行提取\n",
        "def quick_fix_and_rerun():\n",
        "    \"\"\"应用修复并重新运行完整的提取流程\"\"\"\n",
        "    print(\"🔧 应用修复并重新运行...\")\n",
        "\n",
        "    # 首先测试新提示词\n",
        "    if test_improved_prompt():\n",
        "        print(\"\\n✅ 新提示词测试成功!\")\n",
        "\n",
        "        # 重新运行完整的提取流程\n",
        "        print(\"\\n🔄 重新运行完整的KPI提取...\")\n",
        "        try:\n",
        "            df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
        "\n",
        "            # 保存结果\n",
        "            save_results(df_auto, \"fixed_\" + EXPORT_AUTO_XLSX, PDF_PATH)\n",
        "\n",
        "            print(f\"\\n🎉 修复完成! 总共提取了 {len(df_auto)} 个KPI\")\n",
        "            print(\"结果已保存到 fixed_\" + EXPORT_AUTO_XLSX)\n",
        "\n",
        "            # 显示包含饼图数据的KPI\n",
        "            pie_chart_kpis = df_auto[df_auto['kpi_text'].str.contains('pie|Pie', case=False, na=False)]\n",
        "            print(f\"\\n📊 饼图相关的KPI ({len(pie_chart_kpis)} 个):\")\n",
        "            for idx, row in pie_chart_kpis.iterrows():\n",
        "                print(f\"- {row['kpi_text']}\")\n",
        "\n",
        "            return df_auto\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ 重新运行失败: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(\"\\n❌ 新提示词测试失败，需要进一步调试\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "Yq5MEBJLop0V"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 通用测试函数\n",
        "def test_universal_prompt():\n",
        "    \"\"\"测试通用提示词的效果\"\"\"\n",
        "    print(\"=== 测试通用图像分析提示词 ===\")\n",
        "\n",
        "    try:\n",
        "        # 提取页面2的全页图像进行测试\n",
        "        images = extract_images_from_pdf_fixed(PDF_PATH)\n",
        "        page2_image = None\n",
        "\n",
        "        for img_info in images:\n",
        "            if img_info['page_number'] == 2 and img_info['type'] == 'full_page':\n",
        "                page2_image = img_info['image']\n",
        "                break\n",
        "\n",
        "        if page2_image is None:\n",
        "            print(\"❌ 找不到页面2图像\")\n",
        "            return False\n",
        "\n",
        "        print(f\"✅ 找到页面2图像，尺寸: {page2_image.width}x{page2_image.height}\")\n",
        "\n",
        "        # 使用通用提示词测试\n",
        "        base64_image = image_to_base64_fixed(page2_image)\n",
        "\n",
        "        print(\"🔄 使用通用提示词调用API...\")\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": ENHANCED_IMAGE_KPI_SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": \"\"\"请分析这个页面中的所有图表和表格，提取所有可量化的数据点。\n",
        "\n",
        "重点关注：\n",
        "- 饼图中每个扇形的具体百分比\n",
        "- 表格中的所有数值数据\n",
        "- 确保每个提取的KPI都包含具体的数字，不只是描述\n",
        "\n",
        "请确保提取完整的上下文信息。\"\"\"},\n",
        "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\", \"detail\": \"high\"}}\n",
        "                ]}\n",
        "            ],\n",
        "            temperature=0.0,\n",
        "            max_tokens=4000,\n",
        "            timeout=90\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message.content.strip()\n",
        "        print(f\"\\n📋 API响应长度: {len(content)} 字符\")\n",
        "\n",
        "        # 解析和验证结果\n",
        "        try:\n",
        "            if content.startswith('```json'):\n",
        "                content = content[7:]\n",
        "            if content.endswith('```'):\n",
        "                content = content[:-3]\n",
        "            content = content.strip()\n",
        "\n",
        "            if content.startswith('['):\n",
        "                result = json.loads(content)\n",
        "                print(f\"✅ 成功解析JSON，找到 {len(result)} 个KPI\")\n",
        "\n",
        "                # 分析提取质量\n",
        "                complete_kpis = 0\n",
        "                pie_chart_kpis = 0\n",
        "                table_kpis = 0\n",
        "\n",
        "                print(\"\\n📊 提取的KPI列表:\")\n",
        "                for i, kpi in enumerate(result, 1):\n",
        "                    if isinstance(kpi, dict):\n",
        "                        kpi_text = kpi.get('kpi_text', '')\n",
        "                        quantitative_value = kpi.get('quantitative_value', '')\n",
        "                        chart_type = kpi.get('chart_type', '')\n",
        "\n",
        "                        print(f\"{i:2d}. {kpi_text}\")\n",
        "                        print(f\"    数值: {quantitative_value} {kpi.get('unit', '')}\")\n",
        "                        print(f\"    类型: {chart_type}\")\n",
        "\n",
        "                        # 统计分析\n",
        "                        if quantitative_value and str(quantitative_value).strip():\n",
        "                            complete_kpis += 1\n",
        "\n",
        "                        if 'pie' in chart_type.lower():\n",
        "                            pie_chart_kpis += 1\n",
        "                        elif 'table' in chart_type.lower():\n",
        "                            table_kpis += 1\n",
        "\n",
        "                        print()\n",
        "\n",
        "                print(f\"📈 质量分析:\")\n",
        "                print(f\"  - 包含数值的KPI: {complete_kpis}/{len(result)} ({complete_kpis/len(result)*100:.1f}%)\")\n",
        "                print(f\"  - 饼图KPI: {pie_chart_kpis}\")\n",
        "                print(f\"  - 表格KPI: {table_kpis}\")\n",
        "\n",
        "                # 检查是否提取了目标数据\n",
        "                success_indicators = [\n",
        "                    any('64' in str(kpi.get('quantitative_value', '')) for kpi in result),\n",
        "                    any('33' in str(kpi.get('quantitative_value', '')) for kpi in result),\n",
        "                    any('68' in str(kpi.get('quantitative_value', '')) for kpi in result),\n",
        "                    any('30' in str(kpi.get('quantitative_value', '')) for kpi in result)\n",
        "                ]\n",
        "\n",
        "                if any(success_indicators):\n",
        "                    print(\"🎉 成功提取了目标饼图数据!\")\n",
        "                    return True\n",
        "                else:\n",
        "                    print(\"⚠️ 可能没有提取到期望的饼图百分比\")\n",
        "                    return False\n",
        "            else:\n",
        "                print(\"❌ API响应不是JSON格式\")\n",
        "                print(f\"响应内容: {content[:300]}...\")\n",
        "                return False\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"❌ JSON解析失败: {e}\")\n",
        "            print(f\"响应内容: {content[:300]}...\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 测试失败: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "# 应用通用修复\n",
        "def apply_universal_fix():\n",
        "    \"\"\"应用通用提示词修复并重新运行\"\"\"\n",
        "    print(\"🔧 应用通用提示词修复...\")\n",
        "\n",
        "    # 首先测试新提示词\n",
        "    print(\"第一步: 测试新的通用提示词...\")\n",
        "    if test_universal_prompt():\n",
        "        print(\"\\n✅ 通用提示词测试成功!\")\n",
        "\n",
        "        # 询问是否继续完整提取\n",
        "        try:\n",
        "            proceed = input(\"\\n是否继续运行完整的KPI提取? (y/n): \").lower()\n",
        "            if proceed == 'y':\n",
        "                print(\"\\n🔄 重新运行完整的KPI提取...\")\n",
        "                df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
        "\n",
        "                # 保存结果\n",
        "                output_file = \"universal_fixed_\" + EXPORT_AUTO_XLSX\n",
        "                save_results(df_auto, output_file, PDF_PATH)\n",
        "\n",
        "                print(f\"\\n🎉 修复完成! 总共提取了 {len(df_auto)} 个KPI\")\n",
        "                print(f\"结果已保存到 {output_file}\")\n",
        "\n",
        "                # 显示图像来源的KPI统计\n",
        "                if 'source_type' in df_auto.columns:\n",
        "                    image_kpis = df_auto[df_auto['source_type'] == 'image']\n",
        "                    print(f\"\\n📊 从图像中提取的KPI: {len(image_kpis)} 个\")\n",
        "\n",
        "                return df_auto\n",
        "            else:\n",
        "                print(\"已取消完整提取\")\n",
        "                return None\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n用户取消操作\")\n",
        "            return None\n",
        "\n",
        "    else:\n",
        "        print(\"\\n❌ 通用提示词测试失败\")\n",
        "        print(\"建议检查API响应或进一步调整提示词\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "7CYTzMwVqSbv"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ 执行入口 ============\n",
        "if __name__ == \"__main__\":\n",
        "    # Uncomment to install dependencies first\n",
        "    # install_dependencies()\n",
        "\n",
        "    # Uncomment to see usage examples\n",
        "    # example_usage()\n",
        "\n",
        "    # Run the main extraction\n",
        "    run_kpi_extraction()\n",
        "    #run_debugging_session()\n",
        "    #apply_universal_fix()"
      ],
      "metadata": {
        "id": "_ByxPUdia6io",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b8c0791-d568-422e-f959-d6d667aeb47b"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting KPI extraction process...\n",
            "✅ Environment validation passed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (0, 0, 595.0, 793.0)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (130.458984, 281.02200000000005, 488.56800000000055, 649.988)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (24.01875000000001, 626.215, 565.7600000000002, 758.1642608695653)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (17.619999999999997, 598.225, 567.4499999999999, 762.8299393939394)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (20.459999999999994, 218.20000000000002, 552.6464999999996, 599.0939393939393)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (24.020000000000003, 583.955, 569.6875000000001, 755.6360000000001)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (29.269, 357.42449999999997, 546.6355000000009, 606.349)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (17.619999999999997, 626.215, 562.7475000000002, 750.6956153846154)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (66.62, 403.738, 566.2579999999992, 612.1506153846153)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (101.997, 182.26, 572.2369999999987, 424.5196153846154)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (24.009999999999998, 595.7725, 574.7062499999998, 762.107875)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (127.01600000000005, 183.155, 455.5960000000003, 435.16445454545453)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:218: UserWarning: No tables found on page-31\n",
            "  if self._document_has_no_text():\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (24.020000000000003, 281.706, 319.82149999999984, 459.1270769230769)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:218: UserWarning: No tables found on page-31\n",
            "  if self._document_has_no_text():\n",
            "ERROR:root:API call failed: Request timed out.\n",
            "WARNING:root:API response not JSON list: The provided text does not contain any specific numbers, percentages, or measurable quantities that ...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It is s...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It is s...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts, graphs, or quantifiable performance data. It is a vi...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It is a...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is the cover of the \"Canon Environmental Report 1999\" and does not contain any ch...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is the cover of the \"Canon Environmental Report 1999\" and does not contain any ch...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a text-based message from the President of Canon Inc. It does not contain any ...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a text-based document listing operational sites surveyed for Canon's environme...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a page from the Canon Environmental Report. It primarily contains textual info...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is completely gray and does not contain any visible charts or data. Therefore, no...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is completely gray and does not contain any visible charts or data. Therefore, no...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is completely gray and does not contain any visible charts or data. Therefore, no...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any visible charts or graphs with quantifiable performance data....\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is completely gray and does not contain any visible charts or data. Therefore, no...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a text-based document outlining the \"Philosophy and Environment Assurance Guid...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It prim...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains a table outlining voluntary action plans related to environmental assurance activ...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains a table outlining voluntary action plans related to environmental assurance activ...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a text-based document outlining the environmental policy and organizational st...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is primarily a textual document outlining the organizational structure and commit...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains a textual description of an Environmental Management System rather than a chart o...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a text-based document describing the Environmental Management System of Canon....\n",
            "ERROR:root:Error extracting KPIs from image: Request timed out.\n",
            "ERROR:root:Error extracting KPIs from image: Request timed out.\n",
            "WARNING:root:Image analysis response not JSON list: The image contains a table detailing the control of chemical substances, categorized by control leve...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains a table listing different categories of chemical substances and their control lev...\n",
            "WARNING:root:Image analysis response not JSON list: The image is a table titled \"1998 Control Balance Sheet (PRTR)\" showing data related to the use and ...\n",
            "WARNING:root:Image analysis response not JSON list: The image is a table titled \"1998 Control Balance Sheet (PRTR)\" showing data on the use and emission...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains a bar chart titled \"Number of Audited Sites\" and a table titled \"1998 Auditing.\" ...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains tables with quantifiable performance data related to environmental analysis and g...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains tables with quantifiable data related to environmental analysis and green procure...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains tables and text related to social contributions and awards, but it does not inclu...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains tables with information about social contribution activities and awards, but it d...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a table listing issues and responses by Canon over various years. It does not ...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a table, not a chart or graph, and it primarily lists historical events, organ...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a text-based page from a Canon Environmental Report Questionnaire. It does not...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a survey form rather than a chart or graph. It contains questions about the re...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a survey form, not a chart or graph. It contains questions about the readabili...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any visible charts or graphs with quantifiable performance data....\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any visible charts, graphs, or data visualizations with quantifi...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Extraction Summary ===\n",
            "Total KPIs extracted: 566\n",
            "From text/tables: 412\n",
            "From images/charts: 154\n",
            "\n",
            "KPI Distribution by Theme:\n",
            "  Environmental: 508\n",
            "  Social: 27\n",
            "  Economic: 18\n",
            "  Governance: 13\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1IoxrHG3joQz9CaGgenAnsjP3sGaEFA9h",
      "authorship_tag": "ABX9TyPY1HPCNVN5vhrBMRhUHOLE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}