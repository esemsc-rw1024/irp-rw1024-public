{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esemsc-rw1024/irp-rw1024-public/blob/main/KPI_extraction_Jul25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "extract_sustainability_kpi.py\n",
        "==================================\n",
        "Automatically extract KPI sentences/table rows from Sustainability Report PDF\n",
        "and compare with manual KPI annotations\n",
        "--------------------------------------------------\n",
        "1. pdfplumber extracts text + tables\n",
        "2. Camelot supplements complex table parsing (optional)\n",
        "3. Chunking to control tokens\n",
        "4. OpenAI ChatCompletion API call (GPT-4o / GPT-4 / GPT-3.5)\n",
        "5. Aggregate, deduplicate, and export to auto_kpi.xlsx\n",
        "6. Compare with manual_kpi.xlsx for differences\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eDB9oRTEZoZw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "9bbcc21b-24f8-4ecf-9be8-796b7dd022a0"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nextract_sustainability_kpi.py\\n==================================\\nAutomatically extract KPI sentences/table rows from Sustainability Report PDF\\nand compare with manual KPI annotations\\n--------------------------------------------------\\n1. pdfplumber extracts text + tables\\n2. Camelot supplements complex table parsing (optional)\\n3. Chunking to control tokens\\n4. OpenAI ChatCompletion API call (GPT-4o / GPT-4 / GPT-3.5)\\n5. Aggregate, deduplicate, and export to auto_kpi.xlsx\\n6. Compare with manual_kpi.xlsx for differences\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "id": "LaGnafXLZxfg"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "3w2Bya8SZob3"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai python-dotenv pdfplumber tiktoken pandas\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install -y ghostscript\n",
        "!pip install \"camelot-py[cv]\"\n",
        "!pip install PyMuPDF Pillow\n",
        "!pip install -q transformers pillow torchvision"
      ],
      "metadata": {
        "id": "JSakj9TyZodt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac113630-1235-44fa-9b87-96f5db6a21c5"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.97.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.7)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ghostscript is already the newest version (9.55.0~dfsg1-0ubuntu5.12).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "Requirement already satisfied: camelot-py[cv] in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "\u001b[33mWARNING: camelot-py 1.0.0 does not provide the extra 'cv'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (8.2.1)\n",
            "Requirement already satisfied: chardet>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (5.2.0)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (2.0.2)\n",
            "Requirement already satisfied: openpyxl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (3.1.5)\n",
            "Requirement already satisfied: pdfminer-six>=20240706 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (20250506)\n",
            "Requirement already satisfied: pypdf<4.0,>=3.17 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (3.17.4)\n",
            "Requirement already satisfied: pandas>=2.2.2 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (2.2.2)\n",
            "Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (0.9.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.7.0.68 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (4.12.0.88)\n",
            "Requirement already satisfied: pypdfium2>=4 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (4.30.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl>=3.1.0->camelot-py[cv]) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.2->camelot-py[cv]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.2->camelot-py[cv]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.2->camelot-py[cv]) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer-six>=20240706->camelot-py[cv]) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer-six>=20240706->camelot-py[cv]) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[cv]) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.2->camelot-py[cv]) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[cv]) (2.22)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, time, textwrap, argparse, logging\n",
        "import pdfplumber, pandas as pd, tiktoken\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from typing import List, Dict, Optional, Set, Tuple\n",
        "from pathlib import Path\n",
        "from difflib import SequenceMatcher\n",
        "import base64\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import fitz  # PyMuPDF\n",
        "import numpy as np\n",
        "from transformers import CLIPProcessor, CLIPModel\n"
      ],
      "metadata": {
        "id": "rNQHu6_fZofh"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------- Configuration -----------------------------\n",
        "PDF_PATH          = \"/content/Global_Resource_Corp_eco1999e_2dcbgi89 (1).pdf\"\n",
        "MANUAL_XLSX       = \"manual_kpi.xlsx\"   # Leave empty if not available\n",
        "EXPORT_AUTO_XLSX  = \"auto_kpi.xlsx\"\n",
        "MODEL_NAME        = \"gpt-4o\"       # Adjust based on account availability\n",
        "MAX_TOKENS_CHUNK  = 1500               # Token limit per chunk\n",
        "SLEEP_SEC         = 0.6                # Rate limiting\n",
        "ENABLE_QUALITY_VALIDATION = True       # Enable additional quality checks\n",
        "# -----------------------------------------------------------------"
      ],
      "metadata": {
        "id": "L9B6ORg1Zohn"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ ‰øÆÂ§çÁöÑÂàùÂßãÂåñÈÉ®ÂàÜ ============\n",
        "def initialize_environment():\n",
        "    \"\"\"Initialize the environment and API client\"\"\"\n",
        "    # Load environment variables\n",
        "    load_dotenv(\"ruojia_api_key.env\")\n",
        "\n",
        "    # Initialize OpenAI client\n",
        "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"OPENAI_API_KEY not found in environment variables!\")\n",
        "\n",
        "    client = OpenAI(api_key=api_key)\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "    return client, enc\n",
        "\n",
        "# Initialize global variables\n",
        "client, enc = initialize_environment()"
      ],
      "metadata": {
        "id": "mi52QoSbZoja"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ ‰øÆÂ§çÁöÑPDFÊñáÊú¨ÊèêÂèñ ============\n",
        "def pdf_to_text_and_tables(path: str) -> str:\n",
        "    \"\"\"Extract text paragraphs and tables using pdfplumber.\"\"\"\n",
        "    all_chunks = []\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"PDF file not found: {path}\")\n",
        "\n",
        "    try:\n",
        "        with pdfplumber.open(path) as pdf:\n",
        "            logging.info(f\"Processing PDF with {len(pdf.pages)} pages...\")\n",
        "\n",
        "            for page_num, page in enumerate(pdf.pages, 1):\n",
        "                try:\n",
        "                    # Extract text\n",
        "                    text = page.extract_text() or \"\"\n",
        "                    if text.strip():\n",
        "                        all_chunks.append(f\"PAGE_{page_num}_TEXT:\\n{text}\")\n",
        "\n",
        "                    # Extract tables\n",
        "                    tables = page.extract_tables()\n",
        "                    for table_num, tb in enumerate(tables):\n",
        "                        if tb and len(tb) > 0:\n",
        "                            try:\n",
        "                                # Handle table headers safely\n",
        "                                if tb[0]:\n",
        "                                    headers = tb[0]\n",
        "                                else:\n",
        "                                    headers = [f\"Col_{i}\" for i in range(len(tb[1]) if len(tb) > 1 else 1)]\n",
        "\n",
        "                                rows = tb[1:] if len(tb) > 1 else []\n",
        "\n",
        "                                if rows:\n",
        "                                    df = pd.DataFrame(rows, columns=headers)\n",
        "                                    # Clean DataFrame\n",
        "                                    df = df.dropna(how='all')  # Remove empty rows\n",
        "                                    if not df.empty:\n",
        "                                        table_txt = f\"TABLE_START_PAGE_{page_num}_{table_num}\\n\" + df.to_csv(index=False) + \"\\nTABLE_END\"\n",
        "                                        all_chunks.append(table_txt)\n",
        "                            except Exception as e:\n",
        "                                logging.warning(f\"Error processing table on page {page_num}: {e}\")\n",
        "                                continue\n",
        "\n",
        "                except Exception as e:\n",
        "                    logging.warning(f\"Error processing page {page_num}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        return \"\\n\\n\".join(all_chunks)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error opening PDF file: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "IkSbr5pqaGsO"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ ‰øÆÂ§çÁöÑCamelotË°®Ê†ºÊèêÂèñ ============\n",
        "def generate_table_fingerprint(df: pd.DataFrame) -> str:\n",
        "    \"\"\"Generate table fingerprint for deduplication\"\"\"\n",
        "    try:\n",
        "        fingerprint_parts = []\n",
        "        fingerprint_parts.append(f\"shape_{df.shape[0]}x{df.shape[1]}\")\n",
        "\n",
        "        if not df.columns.empty:\n",
        "            col_names = [str(col).strip().lower().replace(' ', '') for col in df.columns]\n",
        "            col_fingerprint = '_'.join(sorted(col_names))\n",
        "            fingerprint_parts.append(f\"cols_{hash(col_fingerprint)}\")\n",
        "\n",
        "        if df.shape[0] > 0:\n",
        "            numeric_values = []\n",
        "            for col in df.columns:\n",
        "                for val in df[col].head(3):\n",
        "                    if pd.notna(val):\n",
        "                        numbers = re.findall(r'\\d+\\.?\\d*', str(val))\n",
        "                        numeric_values.extend(numbers)\n",
        "\n",
        "            if numeric_values:\n",
        "                numeric_fingerprint = hash('_'.join(sorted(numeric_values[:10])))\n",
        "                fingerprint_parts.append(f\"nums_{numeric_fingerprint}\")\n",
        "\n",
        "        return '_'.join(fingerprint_parts)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error generating table fingerprint: {e}\")\n",
        "        return str(hash(df.to_csv()))\n",
        "\n",
        "def clean_table_data_improved(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Improved table data cleaning\"\"\"\n",
        "    try:\n",
        "        cleaned_df = df.copy()\n",
        "        cleaned_df = cleaned_df.dropna(how='all')\n",
        "        cleaned_df = cleaned_df.dropna(axis=1, how='all')\n",
        "\n",
        "        for col in cleaned_df.columns:\n",
        "            if cleaned_df[col].dtype == 'object':\n",
        "                cleaned_df[col] = cleaned_df[col].astype(str).str.strip()\n",
        "                cleaned_df[col] = cleaned_df[col].replace(['nan', 'NaN', 'None'], '')\n",
        "\n",
        "        if not cleaned_df.empty:\n",
        "            new_columns = []\n",
        "            for i, col in enumerate(cleaned_df.columns):\n",
        "                col_str = str(col).strip()\n",
        "                if col_str in ['nan', 'NaN', 'None', ''] or pd.isna(col):\n",
        "                    new_columns.append(f'Column_{i}')\n",
        "                else:\n",
        "                    new_columns.append(col_str)\n",
        "            cleaned_df.columns = new_columns\n",
        "\n",
        "        cleaned_df = cleaned_df.reset_index(drop=True)\n",
        "        return cleaned_df\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error in table cleaning: {e}\")\n",
        "        return df\n",
        "\n",
        "def is_valid_table_improved(df: pd.DataFrame) -> bool:\n",
        "    \"\"\"Improved table validation\"\"\"\n",
        "    try:\n",
        "        if df.empty or df.shape[0] < 1 or df.shape[1] < 1:\n",
        "            return False\n",
        "\n",
        "        non_null_cells = 0\n",
        "        total_cells = df.shape[0] * df.shape[1]\n",
        "\n",
        "        for col in df.columns:\n",
        "            for val in df[col]:\n",
        "                if pd.notna(val) and str(val).strip() not in ['', 'nan', 'NaN', 'None']:\n",
        "                    non_null_cells += 1\n",
        "\n",
        "        if non_null_cells / total_cells < 0.2:\n",
        "            return False\n",
        "\n",
        "        has_meaningful_content = False\n",
        "        for col in df.columns:\n",
        "            text_content = ' '.join(df[col].dropna().astype(str))\n",
        "            if (any(char.isdigit() for char in text_content) or\n",
        "                '%' in text_content or\n",
        "                any(keyword in text_content.lower() for keyword in [\n",
        "                    'rate', 'percentage', 'total', 'number', 'emission', 'energy',\n",
        "                    'water', 'waste', 'employee', 'year', '2020', '2021', '2022', '2023'\n",
        "                ])):\n",
        "                has_meaningful_content = True\n",
        "                break\n",
        "\n",
        "        return has_meaningful_content\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error validating table: {e}\")\n",
        "        return True\n",
        "\n",
        "def format_table_output_improved(df: pd.DataFrame, table_id: str, parsing_report=None) -> str:\n",
        "    \"\"\"Improved table output formatting\"\"\"\n",
        "    try:\n",
        "        table_info = f\"TABLE_START_{table_id}\\n\"\n",
        "        table_info += f\"DIMENSIONS: {df.shape[0]} rows √ó {df.shape[1]} columns\\n\"\n",
        "\n",
        "        col_info = \"COLUMNS: \" + \" | \".join([f\"{i}:{col}\" for i, col in enumerate(df.columns)])\n",
        "        table_info += col_info + \"\\n\"\n",
        "\n",
        "        if df.shape[0] > 0:\n",
        "            preview_rows = min(2, df.shape[0])\n",
        "            table_info += f\"PREVIEW_FIRST_{preview_rows}_ROWS:\\n\"\n",
        "            for i in range(preview_rows):\n",
        "                row_preview = \" | \".join([str(df.iloc[i, j])[:20] for j in range(min(5, df.shape[1]))])\n",
        "                table_info += f\"  Row_{i}: {row_preview}\\n\"\n",
        "\n",
        "        if parsing_report:\n",
        "            try:\n",
        "                accuracy = getattr(parsing_report, 'accuracy', 'N/A')\n",
        "                if accuracy != 'N/A':\n",
        "                    table_info += f\"EXTRACTION_ACCURACY: {accuracy:.2f}\\n\"\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        table_info += \"TABLE_DATA_START\\n\"\n",
        "        table_csv = df.to_csv(index=False, na_rep='', quoting=1, escapechar='\\\\')\n",
        "        table_end = f\"TABLE_DATA_END\\nTABLE_END_{table_id}\\n\"\n",
        "\n",
        "        return table_info + table_csv + table_end\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error formatting table output: {e}\")\n",
        "        return f\"TABLE_START_{table_id}\\n{df.to_csv(index=False)}\\nTABLE_END_{table_id}\\n\"\n",
        "\n",
        "def camelot_extra_tables_enhanced(path: str) -> List[str]:\n",
        "    \"\"\"Enhanced table extraction using Camelot with better error handling\"\"\"\n",
        "    try:\n",
        "        import camelot\n",
        "    except ImportError:\n",
        "        logging.warning(\"Camelot not installed, skipping Camelot table parsing.\")\n",
        "        return []\n",
        "\n",
        "    extra_chunks = []\n",
        "    extracted_tables_fingerprints = set()\n",
        "\n",
        "    try:\n",
        "        logging.info(\"Starting Camelot table extraction...\")\n",
        "\n",
        "        # Stream mode extraction\n",
        "        try:\n",
        "            stream_tables = camelot.read_pdf(\n",
        "                path,\n",
        "                pages=\"all\",\n",
        "                flavor=\"stream\",\n",
        "                edge_tol=50,\n",
        "                row_tol=2,\n",
        "                column_tol=0\n",
        "            )\n",
        "\n",
        "            stream_count = 0\n",
        "            for i, table in enumerate(stream_tables):\n",
        "                if not table.df.empty and table.df.shape[0] > 0:\n",
        "                    table_fingerprint = generate_table_fingerprint(table.df)\n",
        "\n",
        "                    if table_fingerprint not in extracted_tables_fingerprints:\n",
        "                        cleaned_df = clean_table_data_improved(table.df)\n",
        "\n",
        "                        if is_valid_table_improved(cleaned_df):\n",
        "                            table_txt = format_table_output_improved(cleaned_df, f\"STREAM_{i}\", table.parsing_report)\n",
        "                            extra_chunks.append(table_txt)\n",
        "                            extracted_tables_fingerprints.add(table_fingerprint)\n",
        "                            stream_count += 1\n",
        "\n",
        "            logging.info(f\"Stream mode extracted {stream_count} valid tables\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Stream mode extraction failed: {e}\")\n",
        "\n",
        "        # Lattice mode extraction\n",
        "        try:\n",
        "            lattice_tables = camelot.read_pdf(\n",
        "                path,\n",
        "                pages=\"all\",\n",
        "                flavor=\"lattice\",\n",
        "                line_scale=15,\n",
        "                line_tol=2,\n",
        "                joint_tol=2\n",
        "            )\n",
        "\n",
        "            lattice_count = 0\n",
        "            for i, table in enumerate(lattice_tables):\n",
        "                if not table.df.empty and table.df.shape[0] > 0:\n",
        "                    table_fingerprint = generate_table_fingerprint(table.df)\n",
        "\n",
        "                    if table_fingerprint not in extracted_tables_fingerprints:\n",
        "                        cleaned_df = clean_table_data_improved(table.df)\n",
        "\n",
        "                        if is_valid_table_improved(cleaned_df):\n",
        "                            table_txt = format_table_output_improved(cleaned_df, f\"LATTICE_{i}\", table.parsing_report)\n",
        "                            extra_chunks.append(table_txt)\n",
        "                            extracted_tables_fingerprints.add(table_fingerprint)\n",
        "                            lattice_count += 1\n",
        "\n",
        "            logging.info(f\"Lattice mode extracted {lattice_count} additional unique tables\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Lattice mode extraction failed: {e}\")\n",
        "\n",
        "        total_extracted = len(extra_chunks)\n",
        "        logging.info(f\"Camelot extraction completed: {total_extracted} total unique tables extracted\")\n",
        "        return extra_chunks\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Camelot table extraction failed: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "amKWHBAHaGuk"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ ÊñáÊú¨ÂàÜÂùó ============\n",
        "def split_into_chunks(full_text: str, max_tokens: int) -> List[str]:\n",
        "    \"\"\"Split text into chunks based on token limit\"\"\"\n",
        "    paragraphs = [p for p in full_text.split(\"\\n\") if p.strip()]\n",
        "    chunks, current = [], []\n",
        "    current_tokens = 0\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        para_tokens = len(enc.encode(paragraph))\n",
        "\n",
        "        if current_tokens + para_tokens > max_tokens and current:\n",
        "            chunks.append(\"\\n\".join(current))\n",
        "            current = [paragraph]\n",
        "            current_tokens = para_tokens\n",
        "        else:\n",
        "            current.append(paragraph)\n",
        "            current_tokens += para_tokens\n",
        "\n",
        "    if current:\n",
        "        chunks.append(\"\\n\".join(current))\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "ZSLAWykuaGxs"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Á≥ªÁªüÊèêÁ§∫ËØç ============\n",
        "UNIVERSAL_SYSTEM_PROMPT = textwrap.dedent(\"\"\"\n",
        "    You are a professional ESG data analyst specializing in extracting Key Performance Indicators (KPIs) from sustainability reports.\n",
        "\n",
        "    ## CRITICAL: What is a KPI?\n",
        "    A KPI MUST contain SPECIFIC NUMBERS, PERCENTAGES, or MEASURABLE QUANTITIES that demonstrate actual performance or concrete targets.\n",
        "\n",
        "    ## IMPORTANT: Table Data Processing Rules\n",
        "    When processing table data:\n",
        "    1. Pay close attention to column headers to identify the correct time periods\n",
        "    2. Match data values with their corresponding year columns\n",
        "    3. If you see table format like \"Metric, 2021, 2022\" - the first number after metric belongs to 2021, second to 2022\n",
        "    4. Look for table headers that indicate year columns (e.g., \"2020\", \"2021\", \"2022\")\n",
        "    5. Extract each year's data as separate KPIs\n",
        "    6. Avoid extracting the same KPI multiple times - consolidate similar metrics\n",
        "\n",
        "    ## ENHANCED: Advanced Table Processing\n",
        "    7. **EXTRACT ALL DATA POINTS**: For each table cell containing a number, create a separate KPI\n",
        "    8. **REGIONAL/LOCATION DATA**: Pay special attention to location-specific data (countries, regions, cities)\n",
        "    9. **WORKFORCE DATA**: Extract all employee numbers, headcount data, and demographic information\n",
        "    10. **INCOMPLETE DATA**: Extract available data even if some cells are empty or missing\n",
        "    11. **TOTALS AND SUBTOTALS**: Always extract total values and aggregated numbers\n",
        "\n",
        "    ## ‚úÖ VALID KPI EXAMPLES:\n",
        "    - \"Achieved 89.4% reuse and recycle rate for cloud hardware in 2023\"\n",
        "    - \"Diverted over 18,537 metric tons of waste from landfills in 2023\"\n",
        "    - \"Reduced single-use plastics in product packaging to 2.7%\"\n",
        "    - \"Contracted 19 GW of new renewable energy across 16 countries in 2024\"\n",
        "    - \"Provided clean water access to over 1.5 million people in 2023\"\n",
        "    - \"Protected 15,849 acres of land‚Äîexceeding target by more than 30%\"\n",
        "    - \"Allocated 761 million toward innovative climate technologies\"\n",
        "    - \"Achieved 80% renewable energy operations by 2024\"\n",
        "    - \"Water replenishment projects estimated to provide over 25 million cubic meters\"\n",
        "    - \"Exceeded annual target to divert 75% of construction waste by reaching 85%\"\n",
        "    - \"Board independence: 78% of directors\"\n",
        "    - \"Women in senior leadership increased to 35% in 2023\"\n",
        "    - \"Employee engagement score: 87% in annual survey\"\n",
        "    - \"Reduced greenhouse gas emissions by 50% compared to 2019 baseline\"\n",
        "    - \"Zero workplace fatalities achieved for third consecutive year\"\n",
        "    - \"Training completion rate: 98% for mandatory compliance courses\"\n",
        "    - \"Supplier ESG assessments completed for 95% of tier-1 suppliers\"\n",
        "    - \"Customer satisfaction rating: 4.6 out of 5.0\"\n",
        "    - \"Data breach incidents: 0 material breaches in 2023\"\n",
        "\n",
        "    ## ‚ùå NOT KPIs (DO NOT EXTRACT):\n",
        "    - \"Microsoft will require select suppliers to use carbon-free electricity by 2030\"\n",
        "    - \"The company plans to expand Sustainability Manager capabilities\"\n",
        "    - \"We are launching two new Circular Centers in 2023\"\n",
        "    - \"The organization established a new climate innovation fund\"\n",
        "    - \"Microsoft introduced enhanced data governance solutions\"\n",
        "    - \"Updated guidebook to include guidance on corporate responsibility\"\n",
        "    - \"Plans to publish new ESG strategy\"\n",
        "    - \"Implemented a new recycling program\"\n",
        "    - \"Conducted sustainability training sessions\"\n",
        "    - \"Launched employee wellness programs\"\n",
        "    - \"Committed to reducing emissions\"\n",
        "    - \"Focusing on environmental performance\"\n",
        "    - \"Established sustainability committee\"\n",
        "    - \"The company operates facilities in multiple regions\"\n",
        "    - \"Our supply chain includes thousands of vendors globally\"\n",
        "    - Any text without specific numbers, percentages, or quantifiable metrics\n",
        "    - Duplicate or repeated metrics (extract only once per time period)\n",
        "    - Any statement that describes business operations rather than performance outcomes\n",
        "\n",
        "    ## KPI Categories:\n",
        "    ### Environmental:\n",
        "    - **Carbon_Climate**: GHG emissions, carbon footprint, emission reductions, climate targets, scope 1/2/3 emissions, carbon intensity, carbon offsets, TCFD alignment\n",
        "    - **Energy**: Energy consumption, renewable energy percentage, energy efficiency, energy intensity, MWh, GWh, energy savings, fossil fuel usage\n",
        "    - **Water**: Water withdrawal, water consumption, water intensity, water recycling, water reuse, water stress, water discharge quality\n",
        "    - **Waste**: Waste generation, recycling rates, diversion percentages, hazardous waste, non-hazardous waste, zero waste to landfill, e-waste, incineration\n",
        "    - **Biodiversity**: Protected areas, species conservation, habitat restoration, biodiversity impact assessments, land use, ecosystem restoration\n",
        "    - **Circular_Economy**: Recycling rates, material recovery, circular design, raw materials usage, renewable materials, packaging waste\n",
        "    - **Materials**: Raw materials consumption, recycled content, sustainable materials, material intensity, sustainable sourcing\n",
        "\n",
        "    ### Social:\n",
        "    - **Workforce_Diversity**: Employee demographics, gender diversity, age diversity, ethnic diversity, disability inclusion, LGBTQ+ inclusion, workforce composition\n",
        "    - **Gender_Equality**: Women in leadership, gender pay ratio, parental leave return rates, gender representation, female employees percentage\n",
        "    - **Disability_Inclusion**: Employees with disabilities, accessibility compliance, inclusive workplace design, disability support programs\n",
        "    - **Health_Safety**: Lost Time Injury Frequency Rate (LTIFR), Total Recordable Incident Rate (TRIR), fatalities, workplace illness, safety training hours, PPE compliance, emergency drills\n",
        "    - **Employee_Wellbeing**: Employee satisfaction, retention rates, turnover rates, training hours, wellness programs, mental health services, work-life balance\n",
        "    - **Community_Engagement**: Corporate volunteering, social investment, community impact assessments, local hiring, stakeholder engagement activities\n",
        "    - **Human_Rights**: Child labor incidents, forced labor, human rights due diligence, freedom of association, grievance mechanisms, labor audits\n",
        "    - **Labor_Rights**: Collective bargaining coverage, labor complaints resolution, supplier labor audits, working conditions, fair wages\n",
        "    - **Customer_Safety**: Product safety incidents, customer satisfaction, accessibility features, safety recalls, quality metrics\n",
        "    - **Supply_Chain_Social**: Supplier assessments, sustainable sourcing, supplier code compliance, supply chain audits\n",
        "\n",
        "    ### Governance:\n",
        "    - **Board_Governance**: Board independence, board diversity, CEO-chair separation, board ESG expertise, board composition, director tenure\n",
        "    - **Executive_Compensation**: ESG-linked compensation, executive pay ratios, compensation disclosure, incentive structures\n",
        "    - **Ethics_Compliance**: Code of conduct training, corruption incidents, bribery cases, fines and penalties, whistleblower reports, anti-corruption assessments\n",
        "    - **Transparency_Disclosure**: ESG reporting coverage, third-party assurance, political contributions disclosure, GRI/SASB/TCFD compliance\n",
        "    - **Risk_Management**: Risk assessments, mitigation measures, climate risk disclosure, operational risk management\n",
        "    - **Cybersecurity_Data**: Cybersecurity breaches, data privacy policies, cybersecurity training, GDPR compliance, data protection measures\n",
        "    - **Supply_Chain_Governance**: Supplier ESG screening, supplier audits, procurement ESG clauses, vendor compliance rates\n",
        "\n",
        "    ## MANDATORY Requirements:\n",
        "    1. MUST contain specific numbers (e.g., 25%, 15,000, 2.5M, 8.5%, 0.3 per million hours)\n",
        "    2. MUST relate to measurable sustainability outcomes\n",
        "    3. MUST have time reference (year, period, or deadline)\n",
        "    4. MUST be performance-focused (results, not activities or descriptions)\n",
        "    5. MUST NOT be future plans or operational descriptions\n",
        "\n",
        "    ## Output Format:\n",
        "    Return a JSON array. Each KPI must contain:\n",
        "    ```json\n",
        "    {\n",
        "        \"kpi_text\": \"Complete original sentence with the quantifiable metric\",\n",
        "        \"kpi_theme\": \"Environmental/Social/Governance\",\n",
        "        \"kpi_category\": \"Specific category from above list\",\n",
        "        \"quantitative_value\": \"The specific number/percentage extracted\",\n",
        "        \"unit\": \"Unit of measurement (%, tonnes, employees, etc.)\",\n",
        "        \"time_period\": \"Time reference (2023, annual, by 2030, etc.)\",\n",
        "        \"target_or_actual\": \"Target/Actual/Both\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "    ## Additional Instructions:\n",
        "    - If a sentence includes a comparison value, such as a baseline, previous year, or other historical/target data (e.g., \"Compared to 32,395 MWh in 2020\"), extract it as a **separate KPI**.\n",
        "    - Do NOT store the comparison in any other field ‚Äî just create another valid KPI from it.\n",
        "    - Avoid merging multiple numerical values into one KPI unless they are clearly part of the same metric (e.g., male: X, female: Y).\n",
        "\n",
        "    ## STRICT FILTERING:\n",
        "    - Return empty array [] if no quantifiable KPIs found\n",
        "    - Only extract text that contains specific measurable values\n",
        "    - Ignore all qualitative statements, plans, and descriptions\n",
        "    - Focus only on numerical performance data\n",
        "\n",
        "    Now analyze the following text for sustainability KPIs:\n",
        "\"\"\").strip()\n",
        "\n",
        "# üî• Êñ∞Â¢ûÔºöÂ¢ûÂº∫ÁöÑÂõæÂÉèÂàÜÊûêPrompt\n",
        "ENHANCED_IMAGE_KPI_SYSTEM_PROMPT = textwrap.dedent(\"\"\"\n",
        "    You are an expert data analyst specializing in extracting quantifiable KPI data from charts, graphs, and data visualizations in sustainability reports.\n",
        "\n",
        "    ## CRITICAL INSTRUCTION: ALWAYS EXTRACT NUMERICAL VALUES\n",
        "\n",
        "    **Your primary task is to extract the ACTUAL NUMBERS and PERCENTAGES visible in charts, not just descriptions.**\n",
        "\n",
        "    ## MISSION:\n",
        "    Extract ALL quantifiable data points from charts and graphs, including:\n",
        "    - Bar charts (vertical/horizontal)\n",
        "    - Pie charts and donut charts\n",
        "    - Line charts and trend graphs\n",
        "    - Stacked charts and combo charts\n",
        "    - Tables with numerical data\n",
        "    - Infographics with statistics\n",
        "    - Gauge charts and dashboards\n",
        "\n",
        "    ## DETAILED ANALYSIS INSTRUCTIONS:\n",
        "\n",
        "    ### For PIE CHARTS:\n",
        "    1. Read percentage labels on each slice\n",
        "    2. If no labels visible, estimate based on slice size\n",
        "    3. Identify what each slice represents (categories)\n",
        "    4. Extract each slice as separate KPI\n",
        "    5. **MUST read the percentage labels on each slice** - Look for numbers like 64%, 33%, 68%, 30%, etc.\n",
        "    6. **If percentages are visible on the chart, extract them exactly**\n",
        "    7. **If no labels visible, estimate based on slice size using these guidelines:**\n",
        "       - 90¬∞ slice = 25%\n",
        "       - 180¬∞ slice = 50%\n",
        "       - 270¬∞ slice = 75%\n",
        "       - Full circle = 100%\n",
        "    8. **Each slice MUST have a specific percentage value in the final output**\n",
        "\n",
        "    ### For BAR CHARTS:\n",
        "    1. Read Y-axis scale carefully (units, increments)\n",
        "    2. Estimate bar heights using grid lines and scale\n",
        "    3. Read X-axis labels (years, categories, regions)\n",
        "    4. Extract each bar as separate KPI\n",
        "    5. Pay attention to grouped/stacked bars\n",
        "\n",
        "    ### For LINE CHARTS:\n",
        "    1. Read data points at intersection of grid lines\n",
        "    2. Follow trend lines to extract values for each time period\n",
        "    3. Use Y-axis scale for value estimation\n",
        "    4. Extract each data point as separate KPI\n",
        "\n",
        "    ### For TABLES:\n",
        "    1. Read all numerical values in cells\n",
        "    2. Match values with row and column headers\n",
        "    3. Extract each cell with numerical data as KPI\n",
        "\n",
        "    ## MANDATORY VALUE EXTRACTION RULES:\n",
        "\n",
        "    **RULE 1**: Every KPI MUST contain a specific numerical value (percentage, amount, count, etc.)\n",
        "    **RULE 2**: For charts with categories, you MUST find and extract the quantitative values for each category\n",
        "    **RULE 3**: Never create KPIs without specific numbers - descriptions alone are incomplete\n",
        "    **RULE 4**: Include complete context: what + how much + when/where if available\n",
        "\n",
        "\n",
        "    ## VALUE ESTIMATION GUIDELINES:\n",
        "    - Use proportional analysis: if a bar reaches 80% of scale maximum, calculate 80% of max value\n",
        "    - For pie charts: estimate slice angles (90¬∞ = 25%, 180¬∞ = 50%, etc.)\n",
        "    - Cross-reference with any visible data labels or legends\n",
        "    - Be conservative but reasonably accurate in estimates\n",
        "\n",
        "    ## CHART IDENTIFICATION:\n",
        "    First identify the chart type, then apply appropriate extraction method.\n",
        "    Look for:\n",
        "    - Axes and scales\n",
        "    - Data labels and legends\n",
        "    - Grid lines for reference\n",
        "    - Color coding and patterns\n",
        "    - Title and subtitle information\n",
        "\n",
        "    ## OUTPUT FORMAT:\n",
        "    Return a JSON array. For each data point found:\n",
        "    ```json\n",
        "    {\n",
        "        \"kpi_text\": \"Complete description with the ACTUAL NUMERICAL VALUE included\",\n",
        "        \"kpi_theme\": \"Environmental/Social/Governance\",\n",
        "        \"kpi_category\": \"Specific category based on content\",\n",
        "        \"quantitative_value\": \"The exact number/percentage (e.g., '64', '33.5', '68')\",\n",
        "        \"unit\": \"% / tonnes / employees / MWh / USD / etc.\",\n",
        "        \"time_period\": \"2021/2020/2022/Year/period/etc if identifiable\",\n",
        "        \"target_or_actual\": \"Actual\",\n",
        "        \"chart_type\": \"pie_chart/bar_chart/line_chart/table/etc\",\n",
        "        \"estimation_confidence\": \"High/Medium/Low\",\n",
        "        \"chart_title\": \"Chart title if visible\",\n",
        "        \"data_source\": \"Legend or source if visible\"\n",
        "    }\n",
        "\n",
        "    ```\n",
        "    ## EXAMPLES of CORRECT vs INCORRECT extraction:\n",
        "\n",
        "    ### ‚ùå INCORRECT (incomplete - missing numerical values):\n",
        "    ```json\n",
        "    {\n",
        "        \"kpi_text\": \"Energy consumption by facility type\",\n",
        "        \"quantitative_value\": \"\",\n",
        "        \"unit\": \"%\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "    ### ‚úÖ CORRECT (complete with specific values):\n",
        "    ```json\n",
        "    {\n",
        "        \"kpi_text\": \"Office buildings account for 45% of total energy consumption\",\n",
        "        \"quantitative_value\": \"45\",\n",
        "        \"unit\": \"%\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "    ### ‚ùå INCORRECT (category without value):\n",
        "    ```json\n",
        "    {\n",
        "        \"kpi_text\": \"Renewable energy percentage by region\",\n",
        "        \"quantitative_value\": \"\",\n",
        "        \"unit\": \"%\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "    ### ‚úÖ CORRECT (specific regional data):\n",
        "    ```json\n",
        "    {\n",
        "        \"kpi_text\": \"North America achieved 78% renewable energy usage\",\n",
        "        \"quantitative_value\": \"78\",\n",
        "        \"unit\": \"%\"\n",
        "    }\n",
        "    ```\n",
        "    ## QUALITY ASSURANCE CHECKLIST:\n",
        "    Before returning results, verify:\n",
        "    - ‚úÖ Every KPI contains a specific numerical value\n",
        "    - ‚úÖ Chart categories are paired with their quantitative data\n",
        "    - ‚úÖ KPI descriptions are complete and self-explanatory\n",
        "    - ‚úÖ Units are correctly identified and specified\n",
        "    - ‚úÖ Context (time, location, category) is preserved when available\n",
        "    - Each KPI must have a specific numerical value\n",
        "    - Context must be clear and self-contained\n",
        "    - Avoid extracting the same data point multiple times\n",
        "    - Focus on sustainability/ESG metrics when possible\n",
        "\n",
        "    ## VALUE ESTIMATION GUIDELINES:\n",
        "    - **High confidence**: Numbers clearly visible in image\n",
        "    - **Medium confidence**: Numbers estimated using chart scales/grid lines\n",
        "    - **Low confidence**: Values approximated from proportional analysis\n",
        "    - **If no numerical data is visible, return empty array []**\n",
        "\n",
        "    ## IMPORTANT NOTES:\n",
        "    - Extract ALL visible data points, not just main highlights\n",
        "    - Include context in descriptions (e.g., \"According to pie chart showing emission sources\")\n",
        "    - If values are not clearly visible, make reasonable estimates and mark confidence as \"Low\"\n",
        "    - Return empty array [] ONLY if image contains no charts/graphs with quantifiable data\n",
        "    - For multi-year data, create separate KPIs for each year\n",
        "    - Pay special attention to small text and numbers\n",
        "    - Focus on extracting actual performance data, not just identifying chart elements\n",
        "    - If you can see numbers in the image, you MUST extract them\n",
        "    - Pie chart percentages are usually the most important data points\n",
        "    - Return empty array [] ONLY if no numerical data is visible\n",
        "\n",
        "    Now analyze the provided image and extract ALL quantifiable KPI data points:\n",
        "\"\"\").strip()"
      ],
      "metadata": {
        "id": "IonQ4PK2aGzy"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ KPIÊèêÂèñÂáΩÊï∞ ============\n",
        "def extract_page_from_chunk(chunk: str) -> str:\n",
        "    \"\"\"Extract page information from chunk\"\"\"\n",
        "    # Look for PAGE_X_TEXT: format\n",
        "    page_matches = re.findall(r'PAGE_(\\d+)_TEXT:', chunk)\n",
        "    if page_matches:\n",
        "        pages = [int(p) for p in page_matches]\n",
        "        if len(pages) == 1:\n",
        "            return str(pages[0])\n",
        "        else:\n",
        "            return f\"{min(pages)}-{max(pages)}\"\n",
        "\n",
        "    # Look for TABLE_START_PAGE_X_\n",
        "    table_matches = re.findall(r'TABLE_START_PAGE_(\\d+)_', chunk)\n",
        "    if table_matches:\n",
        "        pages = [int(p) for p in table_matches]\n",
        "        if len(pages) == 1:\n",
        "            return str(pages[0])\n",
        "        else:\n",
        "            return f\"{min(pages)}-{max(pages)}\"\n",
        "\n",
        "    return \"Unknown\"\n",
        "\n",
        "def contains_procedural_language(text: str) -> bool:\n",
        "    \"\"\"Check if text contains procedural language\"\"\"\n",
        "    procedural_words = [\n",
        "        'introduced', 'established', 'set up', 'implemented', 'created',\n",
        "        'launched', 'formed', 'built', 'installed', 'deployed',\n",
        "        'additionally introduced', 'procedure for', 'standardization management'\n",
        "    ]\n",
        "    text_lower = text.lower()\n",
        "    return any(word in text_lower for word in procedural_words)\n",
        "\n",
        "def is_data_fragment(kpi_text: str) -> bool:\n",
        "    \"\"\"Check if text is a meaningless data fragment\"\"\"\n",
        "    text = kpi_text.strip()\n",
        "\n",
        "    # Filter pure numbers or simple percentages without context\n",
        "    if re.match(r'^\\d+\\.?\\d*%?$', text):\n",
        "        return True\n",
        "\n",
        "    # Filter very short text (less than 4 meaningful words)\n",
        "    meaningful_words = [word for word in text.split() if len(word) > 2 and not word.isdigit()]\n",
        "    if len(meaningful_words) < 3:\n",
        "        return True\n",
        "\n",
        "    # Filter text with only numbers and common connecting words\n",
        "    words = text.lower().split()\n",
        "    non_functional_words = [word for word in words if word not in ['in', 'of', 'the', 'and', 'or', 'to', 'for', 'with', 'by']]\n",
        "    if len(non_functional_words) < 3:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def standardize_kpi_universal(kpi_item: Dict) -> Dict:\n",
        "    \"\"\"Universal KPI data standardization\"\"\"\n",
        "    standardized = kpi_item.copy()\n",
        "\n",
        "    # Standardize numerical formats\n",
        "    quantitative_value = str(standardized.get('quantitative_value', '')).strip()\n",
        "    kpi_text = standardized.get('kpi_text', '').lower()\n",
        "\n",
        "    # Smart handling of percentage formats\n",
        "    if quantitative_value and quantitative_value.replace('.', '').replace('-', '').replace(',', '').isdigit():\n",
        "        # Check if original text suggests this is a percentage\n",
        "        percentage_indicators = ['percent', 'percentage', '%', 'rate', 'ratio', 'proportion', 'share']\n",
        "        if any(indicator in kpi_text for indicator in percentage_indicators):\n",
        "            if not quantitative_value.endswith('%'):\n",
        "                standardized['quantitative_value'] = quantitative_value + '%'\n",
        "                if not standardized.get('unit'):\n",
        "                    standardized['unit'] = '%'\n",
        "\n",
        "    # Ensure unit field consistency\n",
        "    if '%' in str(standardized.get('quantitative_value', '')):\n",
        "        standardized['unit'] = '%'\n",
        "\n",
        "    # Clean and normalize KPI text\n",
        "    kpi_text_original = standardized.get('kpi_text', '').strip()\n",
        "    # Remove extra spaces and newlines\n",
        "    kpi_text_cleaned = ' '.join(kpi_text_original.split())\n",
        "    standardized['kpi_text'] = kpi_text_cleaned\n",
        "\n",
        "    return standardized\n",
        "\n",
        "def generate_universal_metric_key(kpi_item: Dict) -> str:\n",
        "    \"\"\"Generate universal metric key for deduplication\"\"\"\n",
        "    try:\n",
        "        # Extract core elements\n",
        "        category = kpi_item.get('kpi_category', '').lower().strip()\n",
        "        value = str(kpi_item.get('quantitative_value', '')).replace('%', '').replace(',', '').strip()\n",
        "        time_period = kpi_item.get('time_period', '').lower().strip()\n",
        "        unit = kpi_item.get('unit', '').lower().strip()\n",
        "\n",
        "        # Extract key semantic information from KPI text\n",
        "        kpi_text = kpi_item.get('kpi_text', '').lower()\n",
        "\n",
        "        # Extract primary number (for more precise matching)\n",
        "        numbers_in_text = re.findall(r'\\d+\\.?\\d*', kpi_text)\n",
        "        primary_number = numbers_in_text[0] if numbers_in_text else value\n",
        "\n",
        "        # Generate semantic signature: extract keywords from text\n",
        "        # Remove common stop words\n",
        "        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'}\n",
        "\n",
        "        # Extract keywords (length>2 and not stop words)\n",
        "        words = re.findall(r'\\b\\w+\\b', kpi_text)\n",
        "        key_words = [word for word in words if len(word) > 2 and word not in stop_words and not word.isdigit()]\n",
        "\n",
        "        # Sort keywords to ensure consistency\n",
        "        key_words = sorted(set(key_words))[:5]  # Take at most 5 keywords\n",
        "        semantic_signature = '_'.join(key_words)\n",
        "\n",
        "        # Build universal metric key\n",
        "        key_components = []\n",
        "\n",
        "        if category:\n",
        "            key_components.append(f\"cat:{category}\")\n",
        "        if primary_number:\n",
        "            key_components.append(f\"val:{primary_number}\")\n",
        "        if time_period:\n",
        "            key_components.append(f\"time:{time_period}\")\n",
        "        if unit:\n",
        "            key_components.append(f\"unit:{unit}\")\n",
        "        if semantic_signature:\n",
        "            key_components.append(f\"sem:{semantic_signature}\")\n",
        "\n",
        "        # Generate final key\n",
        "        metric_key = \"|\".join(key_components)\n",
        "\n",
        "        # If all components are empty, use text hash\n",
        "        if not metric_key:\n",
        "            metric_key = f\"hash:{hash(kpi_text)}\"\n",
        "\n",
        "        return metric_key\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error generating universal metric key: {e}\")\n",
        "        # Fallback to text hash\n",
        "        return f\"fallback:{hash(kpi_item.get('kpi_text', ''))}\"\n",
        "\n",
        "def extract_kpi_from_chunk_universal(chunk: str) -> List[Dict]:\n",
        "    \"\"\"Universal KPI extraction function for various sustainability reports\"\"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=MODEL_NAME,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": UNIVERSAL_SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": f\"\"\"Extract ALL KPIs from this text. Requirements:\n",
        "\n",
        "1. Create COMPLETE, MEANINGFUL KPI descriptions with full context\n",
        "2. DO NOT extract standalone numbers without explanatory text\n",
        "3. Include all relevant context (time, location, metric type, etc.)\n",
        "4. Use consistent formatting for similar metrics\n",
        "5. Ensure each KPI is self-explanatory\n",
        "\n",
        "Text to analyze:\n",
        "{chunk}\"\"\"}\n",
        "            ],\n",
        "            temperature=0.0,\n",
        "            max_tokens=4000,\n",
        "            timeout=60\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Clean potential markdown formatting\n",
        "        if content.startswith('```json'):\n",
        "            content = content[7:]\n",
        "        if content.endswith('```'):\n",
        "            content = content[:-3]\n",
        "\n",
        "        if not content.strip().startswith(\"[\"):\n",
        "            logging.warning(f\"API response not JSON list: {content[:100]}...\")\n",
        "            return []\n",
        "\n",
        "        result = json.loads(content)\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            logging.warning(\"API response is not a list format\")\n",
        "            return []\n",
        "\n",
        "        # Extract page information\n",
        "        page_number = extract_page_from_chunk(chunk)\n",
        "\n",
        "        # Universal validation and deduplication logic\n",
        "        validated_result = []\n",
        "        seen_metrics = set()\n",
        "\n",
        "        for item in result:\n",
        "            if isinstance(item, dict) and 'kpi_text' in item and 'kpi_theme' in item:\n",
        "                if item['kpi_text'].strip() and item['kpi_theme'].strip():\n",
        "\n",
        "                    # Check procedural language\n",
        "                    if contains_procedural_language(item['kpi_text']):\n",
        "                        logging.debug(f\"Procedural statement filtered: {item['kpi_text'][:50]}...\")\n",
        "                        continue\n",
        "\n",
        "                    # Filter meaningless data fragments\n",
        "                    if is_data_fragment(item['kpi_text']):\n",
        "                        logging.debug(f\"Data fragment filtered: {item['kpi_text']}\")\n",
        "                        continue\n",
        "\n",
        "                    # Standardize KPI data\n",
        "                    standardized_item = standardize_kpi_universal(item)\n",
        "\n",
        "                    # Add page information\n",
        "                    standardized_item['source_page'] = page_number\n",
        "                    standardized_item['source_type'] = 'text'\n",
        "\n",
        "                    # Universal deduplication mechanism\n",
        "                    metric_key = generate_universal_metric_key(standardized_item)\n",
        "\n",
        "                    if metric_key not in seen_metrics:\n",
        "                        validated_result.append(standardized_item)\n",
        "                        seen_metrics.add(metric_key)\n",
        "                        logging.debug(f\"KPI extracted: {standardized_item['kpi_text'][:80]}...\")\n",
        "                    else:\n",
        "                        logging.debug(f\"Duplicate metric filtered: {standardized_item['kpi_text'][:50]}...\")\n",
        "\n",
        "        logging.info(f\"Chunk processed: {len(validated_result)} unique KPIs extracted\")\n",
        "        return validated_result\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        logging.warning(f\"JSON parsing failed: {e}\\nContent: {content[:300]}...\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logging.error(f\"API call failed: {e}\")\n",
        "        return []\n",
        "\n",
        "def post_process_kpis_universal(kpis: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"Universal KPI post-processing for various report types\"\"\"\n",
        "    if not kpis:\n",
        "        return kpis\n",
        "\n",
        "    # Step 1: Deduplication based on metric keys\n",
        "    unique_kpis_dict = {}\n",
        "\n",
        "    for kpi in kpis:\n",
        "        metric_key = generate_universal_metric_key(kpi)\n",
        "\n",
        "        if metric_key not in unique_kpis_dict:\n",
        "            unique_kpis_dict[metric_key] = kpi\n",
        "        else:\n",
        "            # If duplicate, keep the more complete KPI description\n",
        "            existing_kpi = unique_kpis_dict[metric_key]\n",
        "            current_kpi = kpi\n",
        "\n",
        "            # Compare KPI text completeness\n",
        "            if len(current_kpi.get('kpi_text', '')) > len(existing_kpi.get('kpi_text', '')):\n",
        "                unique_kpis_dict[metric_key] = current_kpi\n",
        "                logging.debug(f\"Replaced with more complete KPI: {current_kpi.get('kpi_text', '')[:50]}...\")\n",
        "            else:\n",
        "                logging.debug(f\"Kept existing KPI: {existing_kpi.get('kpi_text', '')[:50]}...\")\n",
        "\n",
        "    # Step 2: Text similarity-based secondary deduplication\n",
        "    final_kpis = list(unique_kpis_dict.values())\n",
        "\n",
        "    # Use text similarity to check remaining potential duplicates\n",
        "    final_unique_kpis = []\n",
        "\n",
        "    for current_kpi in final_kpis:\n",
        "        is_duplicate = False\n",
        "        current_text = current_kpi.get('kpi_text', '')\n",
        "\n",
        "        for existing_kpi in final_unique_kpis:\n",
        "            existing_text = existing_kpi.get('kpi_text', '')\n",
        "\n",
        "            # Calculate text similarity\n",
        "            similarity = calculate_text_similarity(current_text, existing_text)\n",
        "\n",
        "            # If similarity is very high, consider it duplicate\n",
        "            if similarity > 0.8:\n",
        "                is_duplicate = True\n",
        "                logging.debug(f\"Text similarity duplicate filtered: {current_text[:50]}...\")\n",
        "                break\n",
        "\n",
        "        if not is_duplicate:\n",
        "            final_unique_kpis.append(current_kpi)\n",
        "\n",
        "    logging.info(f\"Universal post-processing: {len(final_unique_kpis)}/{len(kpis)} KPIs retained\")\n",
        "    return final_unique_kpis\n",
        "\n",
        "def calculate_text_similarity(text1: str, text2: str) -> float:\n",
        "    \"\"\"Calculate similarity between two texts\"\"\"\n",
        "    # Normalize texts\n",
        "    norm1 = ' '.join(text1.lower().split())\n",
        "    norm2 = ' '.join(text2.lower().split())\n",
        "\n",
        "    # Word sets\n",
        "    words1 = set(norm1.split())\n",
        "    words2 = set(norm2.split())\n",
        "\n",
        "    if len(words1) == 0 or len(words2) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate intersection and union\n",
        "    intersection = len(words1.intersection(words2))\n",
        "    union = len(words1.union(words2))\n",
        "\n",
        "    return intersection / union if union > 0 else 0.0\n",
        "\n",
        "def validate_kpi_quality(kpis: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"Additional quality validation for extracted KPIs with relaxed filtering\"\"\"\n",
        "    if not ENABLE_QUALITY_VALIDATION:\n",
        "        return kpis\n",
        "\n",
        "    quality_kpis = []\n",
        "\n",
        "    for kpi in kpis:\n",
        "        kpi_text = kpi.get('kpi_text', '').lower()\n",
        "\n",
        "        # Exclude \"planned tone\" KPIs (not actual performance)\n",
        "        is_future_statement = any(word in kpi_text for word in [\n",
        "            'will', 'aim to', 'plan to', 'planning to', 'intend to',\n",
        "            'is expected to', 'is scheduled to', 'expects to', 'expected to',\n",
        "            'targeting', 'propose to', 'going to', 'shall', 'to be installed'\n",
        "        ])\n",
        "        if is_future_statement:\n",
        "            logging.debug(f\"KPI rejected (future plan): {kpi_text[:100]}...\")\n",
        "            continue\n",
        "\n",
        "        # Filter procedural language\n",
        "        if contains_procedural_language(kpi_text):\n",
        "            logging.debug(f\"KPI rejected (procedural language): {kpi_text[:100]}...\")\n",
        "            continue\n",
        "\n",
        "        # Filter for phrases like \"place name + percentage\" (not ESG KPIs, but distribution descriptions)\n",
        "        geo_percent_pattern = re.compile(r\"^[a-z\\s,:%-]+(?:\\s)?\\d{1,3}%$\")\n",
        "        if geo_percent_pattern.match(kpi_text.strip()) and len(kpi_text.strip().split()) <= 6:\n",
        "            logging.debug(f\"KPI rejected (geo+percent short form): {kpi_text}\")\n",
        "            continue\n",
        "\n",
        "        # Verb whitelist: must include action verbs\n",
        "        allowed_kpi_verbs = [\n",
        "            'reduce', 'reduced', 'achieve', 'achieved', 'improve', 'improved',\n",
        "            'diverted', 'trained', 'invested', 'decreased', 'increased',\n",
        "            'consumed', 'emitted', 'saved', 'reached', 'attained', 'completed',\n",
        "            'recorded', 'cut', 'lowered', 'targeted', 'complied', 'avoided',\n",
        "            'used', 'recycled', 'sourced', 'returned', 'measured', 'maintained',\n",
        "            'reported', 'accounted', 'utilized', 'were', 'was'  # Add state verbs\n",
        "        ]\n",
        "        if not any(verb in kpi_text for verb in allowed_kpi_verbs):\n",
        "            logging.debug(f\"KPI rejected (no action verb): {kpi_text[:100]}...\")\n",
        "            continue\n",
        "\n",
        "        # Greylist verbs (action words but not necessarily performance words) - remove problematic words\n",
        "        graylist_verbs = [\n",
        "            'launched',  # Keep some potentially useful words, but remove obvious procedural words\n",
        "            'formed', 'opened', 'started'\n",
        "        ]\n",
        "\n",
        "        contains_graylist = any(verb in kpi_text for verb in graylist_verbs)\n",
        "\n",
        "        # Check for quantitative indicators\n",
        "        has_numbers = any(char.isdigit() for char in kpi_text)\n",
        "        has_percentage = '%' in kpi_text\n",
        "\n",
        "        # Extended units and measurement indicators\n",
        "        has_units = any(unit in kpi_text for unit in [\n",
        "            'tonnes', 'tons', 'kg', 'mwh', 'kwh', 'gwh', 'litres', 'liters', 'gallons',\n",
        "            'employees', 'hours', 'million', 'billion', 'thousand', 'm¬≥', 'co2e', 'tco2e',\n",
        "            'dollars', 'usd', 'eur', 'gbp', 'incidents', 'rate', 'ratio', 'intensity',\n",
        "            'frequency', 'recordable', 'fatalities', 'injuries', 'directors', 'board',\n",
        "            'workforce', 'leadership', 'diversity', 'inclusion', 'satisfaction', 'retention',\n",
        "            'turnover', 'training', 'safety', 'ltifr', 'trir', 'compliance', 'audit',\n",
        "            'assessment', 'screening', 'supplier', 'breach', 'violation', 'disclosure',\n",
        "            'assurance', 'coverage', 'participation', 'completion', 'investment',\n",
        "            'volunteering', 'engagement', 'grievance', 'whistleblower', 'compensation',\n",
        "            'people', 'staff', 'workers', 'positions', 'roles', 'headcount', 'fte',\n",
        "            'performance', 'score', 'index', 'metric', 'level', 'amount', 'value',\n",
        "            'average', 'median', 'total', 'sum', 'count', 'number', 'quantity'\n",
        "        ])\n",
        "\n",
        "        # More flexible time reference detection\n",
        "        has_time_ref = any(time_word in kpi_text for time_word in [\n",
        "            '2019', '2020', '2021', '2022', '2023', '2024', '2025', '2026', '2027', '2028', '2029', '2030',\n",
        "            '2031', '2032', '2033', '2034', '2035', '2040', '2045', '2050',\n",
        "            'annual', 'yearly', 'year', 'quarter', 'month', 'by', 'target', 'baseline', 'fy',\n",
        "            'per year', 'per annum', 'quarterly', 'monthly', 'daily', 'future', 'deadline',\n",
        "            'period', 'reporting', 'current', 'previous', 'next', 'last', 'this'\n",
        "        ])\n",
        "\n",
        "        # Enhanced sustainability context detection\n",
        "        has_sustainability_context = any(sus_word in kpi_text for sus_word in [\n",
        "            # Environmental keywords\n",
        "            'emission', 'carbon', 'energy', 'renewable', 'waste', 'water', 'recycl',\n",
        "            'environmental', 'ghg', 'scope', 'climate', 'biodiversity', 'circular',\n",
        "            'materials', 'intensity', 'consumption', 'efficiency', 'footprint',\n",
        "            'sustainable', 'sustainability', 'green', 'clean', 'eco', 'offset',\n",
        "            'tcfd', 'nature', 'habitat', 'ecosystem', 'pollution', 'discharge',\n",
        "            'electricity', 'gas', 'fuel', 'solar', 'wind', 'hydro', 'nuclear',\n",
        "\n",
        "            # Social keywords\n",
        "            'safety', 'training', 'employee', 'diversity', 'community', 'social',\n",
        "            'workforce', 'gender', 'women', 'female', 'male', 'disability', 'disabled',\n",
        "            'inclusion', 'equity', 'equality', 'lgbtq', 'minorities', 'ethnic',\n",
        "            'health', 'wellbeing', 'wellness', 'satisfaction', 'retention', 'turnover',\n",
        "            'injury', 'incident', 'fatality', 'ltifr', 'trir', 'recordable',\n",
        "            'human rights', 'labor', 'child labor', 'forced labor', 'slavery',\n",
        "            'freedom', 'association', 'collective bargaining', 'grievance',\n",
        "            'volunteering', 'investment', 'hiring', 'local', 'stakeholder',\n",
        "            'customer', 'supplier', 'supply chain', 'accessibility', 'parental',\n",
        "            'mental health', 'ppe', 'emergency', 'drill', 'compliance',\n",
        "            'people', 'staff', 'workers', 'employment', 'job', 'career',\n",
        "            'leadership', 'management', 'senior', 'executive', 'promotion',\n",
        "\n",
        "            # Governance keywords\n",
        "            'governance', 'board', 'director', 'independent', 'chair', 'ceo',\n",
        "            'executive', 'compensation', 'pay', 'ethics', 'compliance', 'corruption',\n",
        "            'bribery', 'code of conduct', 'whistleblower', 'transparency',\n",
        "            'disclosure', 'reporting', 'assurance', 'audit', 'risk', 'management',\n",
        "            'cybersecurity', 'data', 'privacy', 'gdpr', 'breach', 'policy',\n",
        "            'screening', 'assessment', 'due diligence', 'political', 'contribution',\n",
        "            'gri', 'sasb', 'oversight', 'expertise', 'separation', 'incentive',\n",
        "            'fine', 'penalty', 'violation', 'resolution', 'anti-corruption',\n",
        "\n",
        "            # General business performance that could be sustainability-related\n",
        "            'performance', 'quality', 'delivery', 'customer', 'service', 'product',\n",
        "            'operation', 'facility', 'site', 'location', 'region', 'business'\n",
        "        ])\n",
        "\n",
        "        # If it is a greylist verb sentence, but there is no performance content such as numbers, units, time, etc. ‚Üí delete\n",
        "        if contains_graylist and not (has_numbers or has_units or has_percentage or has_time_ref or has_sustainability_context):\n",
        "            logging.debug(f\"KPI rejected (graylist verb, no quantitative data): {kpi_text[:100]}...\")\n",
        "            continue\n",
        "\n",
        "        # More lenient quality scoring - only require numbers and either units/percentage OR time reference OR sustainability context\n",
        "        basic_requirements = has_numbers and (has_percentage or has_units or has_time_ref or has_sustainability_context)\n",
        "\n",
        "        # Additional check for obvious ESG relevance\n",
        "        is_esg_relevant = any(esg_word in kpi_text for esg_word in [\n",
        "            'emission', 'carbon', 'energy', 'waste', 'water', 'renewable', 'employee',\n",
        "            'safety', 'training', 'diversity', 'governance', 'board', 'compliance',\n",
        "            'sustainability', 'environmental', 'social', 'ghg', 'co2', 'workforce',\n",
        "            'gender', 'health', 'injury', 'incident', 'ethics', 'transparency'\n",
        "        ])\n",
        "\n",
        "        if basic_requirements or is_esg_relevant:\n",
        "            quality_kpis.append(kpi)\n",
        "            logging.debug(f\"KPI accepted: {kpi_text[:100]}...\")\n",
        "        else:\n",
        "            logging.debug(f\"KPI filtered out for quality: {kpi_text[:100]}...\")\n",
        "\n",
        "    logging.info(f\"Quality validation: {len(quality_kpis)}/{len(kpis)} KPIs passed\")\n",
        "    return quality_kpis"
      ],
      "metadata": {
        "id": "sB_kDyLZaG1o"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ ÂõæÂÉèÂ§ÑÁêÜÂáΩÊï∞ ============\n",
        "def extract_numeric_spans(page):\n",
        "    text_dict = page.get_text(\"dict\")\n",
        "    nums = []\n",
        "    for block in text_dict[\"blocks\"]:\n",
        "        for line in block.get(\"lines\", []):\n",
        "            for span in line.get(\"spans\", []):\n",
        "                s = span[\"text\"].strip()\n",
        "                if re.match(r\"[\\d,.]+%?$\", s):          # Á∫ØÊï∞Â≠óÊàñÊï∞Â≠ó+%\n",
        "                    nums.append({\n",
        "                        \"text\": s,\n",
        "                        \"bbox\": span[\"bbox\"],           # (x0,y0,x1,y1)\n",
        "                        \"font\": span[\"size\"]\n",
        "                    })\n",
        "    return nums\n",
        "\n",
        "def extract_images_from_pdf_fixed(pdf_path: str) -> List[Dict]:\n",
        "    \"\"\"Extract images from PDF using PyMuPDF\"\"\"\n",
        "    images = []\n",
        "\n",
        "    try:\n",
        "        pdf_document = fitz.open(pdf_path)\n",
        "\n",
        "        for page_num in range(len(pdf_document)):\n",
        "            page = pdf_document[page_num]\n",
        "            image_list = page.get_images()\n",
        "\n",
        "            # üî• Êñ∞Â¢ûÔºöÂêåÊó∂ÊèêÂèñÈ°µÈù¢Êà™Âõæ‰Ωú‰∏∫Â§áÈÄâ\n",
        "            page_pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # È´òÂàÜËæ®Áéá\n",
        "            page_img = Image.frombytes(\"RGB\", [page_pix.width, page_pix.height], page_pix.samples)\n",
        "\n",
        "            # Ê∑ªÂä†Êï¥È°µÊà™Âõæ\n",
        "            images.append({\n",
        "                'image': page_img,\n",
        "                'page_number': page_num + 1,\n",
        "                'width': page_img.width,\n",
        "                'height': page_img.height,\n",
        "                'image_index': 'full_page',\n",
        "                'type': 'full_page'\n",
        "            })\n",
        "\n",
        "\n",
        "            for img_index, img in enumerate(image_list):\n",
        "                try:\n",
        "                    xref = img[0]\n",
        "                    base_image = pdf_document.extract_image(xref)\n",
        "                    image_bytes = base_image[\"image\"]\n",
        "\n",
        "                    image = Image.open(BytesIO(image_bytes))\n",
        "\n",
        "                    # Convert to RGB if needed\n",
        "                    if image.mode in ['RGBA', 'LA']:\n",
        "                        background = Image.new('RGB', image.size, (255, 255, 255))\n",
        "                        if image.mode == 'RGBA':\n",
        "                            background.paste(image, mask=image.split()[-1])\n",
        "                        else:\n",
        "                            background.paste(image)\n",
        "                        image = background\n",
        "                    elif image.mode != 'RGB':\n",
        "                        image = image.convert('RGB')\n",
        "\n",
        "                    # Filter small images\n",
        "                    if image.width >= 50 and image.height >= 50:\n",
        "                        images.append({\n",
        "                            'image': image,\n",
        "                            'page_number': page_num + 1,\n",
        "                            'width': image.width,\n",
        "                            'height': image.height,\n",
        "                            'image_index': img_index,\n",
        "                            'type': 'extracted'  # üî• Êñ∞Â¢ûÁ±ªÂûãÊ†áËØÜ\n",
        "                        })\n",
        "\n",
        "                except Exception as e:\n",
        "                    logging.warning(f\"Error extracting image {img_index} from page {page_num + 1}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        pdf_document.close()\n",
        "        logging.info(f\"Extracted {len(images)} images from PDF\")\n",
        "        return images\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error extracting images from PDF: {e}\")\n",
        "        return []\n",
        "\n",
        "def image_to_base64_fixed(image: Image.Image) -> str:\n",
        "    \"\"\"Convert image to base64 with error handling\"\"\"\n",
        "    try:\n",
        "        if image.mode not in ['RGB', 'L']:\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        # Resize large images\n",
        "        max_size = (1536, 1536)\n",
        "        if image.width > max_size[0] or image.height > max_size[1]:\n",
        "            # ËÆ°ÁÆóÁº©ÊîæÊØî‰æãÔºå‰øùÊåÅÈïøÂÆΩÊØî\n",
        "            ratio = min(max_size[0]/image.width, max_size[1]/image.height)\n",
        "            new_size = (int(image.width * ratio), int(image.height * ratio))\n",
        "            image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "        buffered = BytesIO()\n",
        "        image.save(buffered, format=\"JPEG\", quality=95)\n",
        "        img_str = base64.b64encode(buffered.getvalue()).decode()\n",
        "\n",
        "        return img_str\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error converting image to base64: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "Vfegv4osaG3b"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "#  Â§öË£ÅÂâ™ / Â§öÂàÜËæ®ÁéáÁîüÊàêÂô®ÔºàÊîØÊåÅË£ÅÂâ™ÂèÇÊï∞‰∏∫ 0Ôºâ\n",
        "# ------------------------------------------------------------\n",
        "from itertools import product\n",
        "\n",
        "def generate_image_variants(img: Image.Image,\n",
        "                            max_side_full: int = 1200,\n",
        "                            crop_size: int = 768,\n",
        "                            stride: int = 512) -> List[Tuple[Image.Image, str]]:\n",
        "    \"\"\"\n",
        "    ËøîÂõû [(variant_image, variant_tag), ...]\n",
        "    variant_tag ÂèñÂÄº: original / resized / crop_{row}_{col}\n",
        "    \"\"\"\n",
        "    variants = []\n",
        "\n",
        "    # 0) ÂéüÂõæ\n",
        "    variants.append((img, \"original\"))\n",
        "\n",
        "    # 1) Áº©ÊîæÔºàËã•ÂéüÂõæËøáÂ§ßÔºâ\n",
        "    w, h = img.size\n",
        "    if max(w, h) > max_side_full:\n",
        "        scale = max_side_full / float(max(w, h))\n",
        "        resized = img.resize((int(w * scale), int(h * scale)), Image.Resampling.LANCZOS)\n",
        "        variants.append((resized, \"resized\"))\n",
        "    else:\n",
        "        resized = img  # Ê≤°Áº©ÊîæÂ∞±‰øùÊåÅÂéüÂõæ\n",
        "        variants.append((resized, \"resized\"))  # Áªü‰∏ÄÂä†‰∏ä resized ÁâàÊú¨\n",
        "\n",
        "    # 2) ÊªëÁ™óË£ÅÂâ™ÔºàË£ÅÂâ™Â∞∫ÂØ∏ÊàñÊ≠•Èïø‰∏∫ 0 Êó∂Ë∑≥ËøáÔºâ\n",
        "    if crop_size > 0 and stride > 0:\n",
        "        base_img = variants[-1][0]\n",
        "        bw, bh = base_img.size\n",
        "        if bw > crop_size or bh > crop_size:\n",
        "            xs = list(range(0, max(bw - crop_size, 1), stride)) + [bw - crop_size]\n",
        "            ys = list(range(0, max(bh - crop_size, 1), stride)) + [bh - crop_size]\n",
        "            for r, c in product(range(len(ys)), range(len(xs))):\n",
        "                x, y = xs[c], ys[r]\n",
        "                crop = base_img.crop((x, y, x + crop_size, y + crop_size))\n",
        "                # ËøáÊª§Á∫ØËâ≤Âå∫Âüü\n",
        "                if np.array(crop.convert('L')).std() < 5:\n",
        "                    continue\n",
        "                variants.append((crop, f\"crop_{r}_{c}\"))\n",
        "\n",
        "    return variants"
      ],
      "metadata": {
        "id": "ma9D4crJaG5z"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------\n",
        "# üìä Êõø‰ª£ plotclassifier ÁöÑÂõæË°®ËØÜÂà´ÂáΩÊï∞ÔºàHugging Face Ê®°ÂûãÔºâ\n",
        "# ---------------------------------------------\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
        "import torch\n",
        "# üîß ‰øÆÂ§çÔºö‰ΩøÁî®CLIPÊ®°ÂûãËøõË°åÂõæË°®ËØÜÂà´\n",
        "def setup_chart_classifier():\n",
        "    \"\"\"ËÆæÁΩÆÂõæË°®ÂàÜÁ±ªÂô®\"\"\"\n",
        "    try:\n",
        "        from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "        # Âä†ËΩΩCLIPÊ®°Âûã\n",
        "        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "        def is_chart_image_clip(image: Image.Image) -> bool:\n",
        "            \"\"\"‰ΩøÁî®CLIPÂà§Êñ≠ÊòØÂê¶ÊòØÂõæË°®\"\"\"\n",
        "            try:\n",
        "                # ÂÆö‰πâÂõæË°®Áõ∏ÂÖ≥ÁöÑÊñáÊú¨ÊèèËø∞\n",
        "                chart_labels = [\n",
        "                    \"a chart\", \"a graph\", \"a bar chart\", \"a pie chart\",\n",
        "                    \"a line graph\", \"a table\", \"data visualization\",\n",
        "                    \"statistics\", \"a diagram\", \"an infographic\"\n",
        "                ]\n",
        "\n",
        "                # Â§ÑÁêÜËæìÂÖ•\n",
        "                inputs = processor(\n",
        "                    text=chart_labels,\n",
        "                    images=image,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True\n",
        "                )\n",
        "\n",
        "                # Ëé∑ÂèñÈ¢ÑÊµãÁªìÊûú\n",
        "                outputs = model(**inputs)\n",
        "                logits_per_image = outputs.logits_per_image\n",
        "                probs = logits_per_image.softmax(dim=1)\n",
        "\n",
        "                # Â¶ÇÊûú‰ªªÊÑèÂõæË°®Ê†áÁ≠æÊ¶ÇÁéáÂ§ß‰∫é0.25ÔºåËÆ§‰∏∫ÊòØÂõæË°®\n",
        "                max_prob = probs.max().item()\n",
        "                is_chart = max_prob > 0.25\n",
        "\n",
        "                logging.debug(f\"CLIPÂõæË°®ËØÜÂà´: ÊúÄÈ´òÊ¶ÇÁéá={max_prob:.3f}, ÁªìÊûú={is_chart}\")\n",
        "                return is_chart\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"CLIPÂõæË°®ËØÜÂà´Â§±Ë¥•: {e}\")\n",
        "                # ÈôçÁ∫ßÂà∞ÁªüËÆ°ÊñπÊ≥ï\n",
        "                gray = image.convert('L')\n",
        "                return np.array(gray).std() > 15\n",
        "\n",
        "        logging.info(\"‚úÖ ‰ΩøÁî®CLIPÊ®°ÂûãËøõË°åÂõæË°®ËØÜÂà´\")\n",
        "        return is_chart_image_clip\n",
        "\n",
        "    except ImportError:\n",
        "        logging.warning(\"CLIPÊ®°Âûã‰∏çÂèØÁî®Ôºå‰ΩøÁî®ÁªüËÆ°ÊñπÊ≥ï\")\n",
        "        def is_chart_image_stats(image: Image.Image) -> bool:\n",
        "            \"\"\"ÁªüËÆ°ÊñπÊ≥ïÂà§Êñ≠ÊòØÂê¶ÊòØÂõæË°®\"\"\"\n",
        "            try:\n",
        "                gray = image.convert('L')\n",
        "                std_dev = np.array(gray).std()\n",
        "                return std_dev > 15\n",
        "            except:\n",
        "                return True\n",
        "\n",
        "        return is_chart_image_stats\n",
        "    except Exception as e:\n",
        "        logging.error(f\"ËÆæÁΩÆÂõæË°®ÂàÜÁ±ªÂô®Â§±Ë¥•: {e}\")\n",
        "        def is_chart_image_fallback(image: Image.Image) -> bool:\n",
        "            return True  # ‰øùÂÆàÁ≠ñÁï•ÔºöÊúâÁñëÈóÆÂ∞±ÂàÜÊûê\n",
        "        return is_chart_image_fallback\n",
        "\n",
        "# ÂàùÂßãÂåñÂõæË°®ÂàÜÁ±ªÂô®\n",
        "is_chart_image = setup_chart_classifier()\n",
        "\n",
        "\n",
        "def extract_kpi_from_image_fixed(image: Image.Image, page_number: int, image_type: str = 'extracted') -> List[Dict]:\n",
        "    \"\"\"Extract KPIs from image with improved error handling\"\"\"\n",
        "    try:\n",
        "        # üî• Êñ∞Â¢ûÔºöÈ¢ÑËøáÊª§ÔºöÊ£ÄÊü•ÊòØÂê¶ÂèØËÉΩÊòØÂõæË°®\n",
        "        if not is_chart_image(image):\n",
        "            logging.debug(f\"Image on page {page_number} filtered out (not likely a chart)\")\n",
        "            return []\n",
        "\n",
        "        base64_image = image_to_base64_fixed(image)\n",
        "        if not base64_image:\n",
        "            return []\n",
        "\n",
        "        # üî• Êõ¥ÊîπÔºö‰ΩøÁî®Â¢ûÂº∫ÁöÑprompt\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": ENHANCED_IMAGE_KPI_SYSTEM_PROMPT  # üî• ‰ΩøÁî®Êñ∞ÁöÑprompt\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\n",
        "                            \"type\": \"text\",\n",
        "                            # üî• Êñ∞Â¢ûÔºöËØ¶ÁªÜÁöÑÁî®Êà∑Êåá‰ª§\n",
        "                            \"text\": \"\"\"Analyze this image carefully for quantifiable performance data.\n",
        "\n",
        "IMPORTANT ANALYSIS PRINCIPLES:\n",
        "\n",
        "1. **Chart Type Recognition**:\n",
        "   - Stacked charts: Multiple colors/patterns layered in same position\n",
        "   - Grouped charts: Multiple elements side by side at same position\n",
        "   - Simple charts: One data point per position\n",
        "\n",
        "2. **Value Extraction Rules**:\n",
        "   - For STACKED charts: Read each layer separately, NOT the total height\n",
        "   - For GROUPED charts: Read each element individually\n",
        "   - For SIMPLE charts: Read data point values directly\n",
        "\n",
        "3. **Data Relevance Filter**:\n",
        "   ‚úÖ EXTRACT: Performance outcomes, efficiency metrics, reduction rates, satisfaction scores, compliance rates\n",
        "   ‚ùå SKIP: Certification counts, project timelines, implementation schedules, organizational charts, process flows\n",
        "\n",
        "4. **Quality Standards**:\n",
        "   - Only extract clear, quantifiable performance indicators\n",
        "   - Each data point must have complete context\n",
        "   - If uncertain about values, don't estimate\n",
        "   - If chart shows mainly operational/administrative data, return empty array\n",
        "\n",
        "Please analyze this chart step by step:\n",
        "- First identify the chart type\n",
        "- Then determine if it contains performance KPIs\n",
        "- Finally extract all relevant performance data points\n",
        "\n",
        "Focus on measurable outcomes and achievements, not counts or processes.\"\"\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"type\": \"image_url\",\n",
        "                            \"image_url\": {\n",
        "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
        "                                \"detail\": \"high\"\n",
        "                            }\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.1,\n",
        "            max_tokens=4000,\n",
        "            timeout=60\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message.content.strip()\n",
        "\n",
        "        if not content:\n",
        "            return []\n",
        "\n",
        "        # Clean formatting\n",
        "        if content.startswith('```json'):\n",
        "            content = content[7:]\n",
        "        if content.endswith('```'):\n",
        "            content = content[:-3]\n",
        "\n",
        "        content = content.strip()\n",
        "\n",
        "        if not content.startswith(\"[\"):\n",
        "            logging.warning(f\"Image analysis response not JSON list: {content[:100]}...\")\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            result = json.loads(content)\n",
        "        except json.JSONDecodeError as e:\n",
        "            logging.warning(f\"JSON parsing failed for image analysis: {e}\")\n",
        "            return []\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            return []\n",
        "\n",
        "        # Process results\n",
        "        processed_result = []\n",
        "        for item in result:\n",
        "            if isinstance(item, dict) and 'kpi_text' in item:\n",
        "                if not item.get('kpi_text', '').strip():\n",
        "                    continue\n",
        "\n",
        "                item['source_page'] = page_number\n",
        "                item['source_type'] = 'image'\n",
        "                item['image_type'] = image_type  # üî• Êñ∞Â¢ûÂ≠óÊÆµ\n",
        "\n",
        "                # üî• Êõ¥ÊîπÔºöÁ°Æ‰øùÊúâchartÊ†áËØÜ\n",
        "                kpi_text = item['kpi_text']\n",
        "                if not any(marker in kpi_text.lower() for marker in ['chart', 'graph', 'table', 'figure']):\n",
        "                    chart_type = item.get('chart_type', 'chart')\n",
        "                    item['kpi_text'] = f\"[{chart_type.title()}] {kpi_text}\"\n",
        "\n",
        "                processed_result.append(item)\n",
        "\n",
        "        if processed_result:\n",
        "            logging.info(f\"‚úÖ Extracted {len(processed_result)} KPIs from {image_type} on page {page_number}\")\n",
        "        else:\n",
        "            logging.debug(f\"‚ùå No KPIs found in {image_type} on page {page_number}\")\n",
        "\n",
        "        return processed_result\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error extracting KPIs from image: {e}\")\n",
        "        return []\n",
        "\n",
        "# ÂéüÂáΩÊï∞Ôºöprocess_pdf_images_for_kpis_fixed\n",
        "# ÂÆåÊï¥ÊõøÊç¢‰∏∫Ôºö\n",
        "\n",
        "def process_pdf_images_for_kpis_fixed(pdf_path: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    ÈÅçÂéÜ PDF ÊØè‰∏ÄÈ°µÔºö\n",
        "      ‚Ä¢ ÂØπËØ•È°µÊâÄÊúâ‚Äòextracted‚ÄôÂõæÂÉèÂÅöÂ§öË£ÅÂâ™+Vision\n",
        "      ‚Ä¢ Ëã•ËØ•È°µËøòÊ≤°ÊäìÂà∞ KPIÔºåÂÜçÂØπÊï¥È°µÊà™ÂõæÂÅö Vision\n",
        "    \"\"\"\n",
        "    logging.info(\"Starting page-by-page image KPI extraction ‚Ä¶\")\n",
        "\n",
        "    images = extract_images_from_pdf_fixed(pdf_path)\n",
        "    if not images:\n",
        "        return []\n",
        "\n",
        "    # ÊääÂõæÂÉèÊåâÈ°µËÅöÂêà\n",
        "    page_dict = {}\n",
        "    for info in images:\n",
        "        pg = info[\"page_number\"]\n",
        "        page_dict.setdefault(pg, {\"extracted\": [], \"full\": None})\n",
        "        if info[\"type\"] == \"extracted\":\n",
        "            page_dict[pg][\"extracted\"].append(info[\"image\"])\n",
        "        else:                    # full_page\n",
        "            page_dict[pg][\"full\"] = info[\"image\"]\n",
        "\n",
        "    all_image_kpis: List[Dict] = []\n",
        "\n",
        "    # ‚Äî‚Äî ÈÄêÈ°µÂ§ÑÁêÜ ‚Äî‚Äî\n",
        "    for pg in sorted(page_dict.keys()):\n",
        "        logging.info(f\"\\n=== Page {pg} ===\")\n",
        "        page_kpis: List[Dict] = []\n",
        "\n",
        "        # ‚ë† ÂçïÁã¨ÊèêÂèñÁöÑÂõæ\n",
        "        for idx, img in enumerate(page_dict[pg][\"extracted\"]):\n",
        "            for var_img, var_tag in generate_image_variants(img, 1200, 768, 512):\n",
        "                kpis = extract_kpi_from_image_fixed(\n",
        "                    var_img, pg, f\"extracted_{var_tag}\"\n",
        "                )\n",
        "                for k in kpis:\n",
        "                    key = generate_universal_metric_key(k)\n",
        "                    if key not in {generate_universal_metric_key(x) for x in page_kpis}:\n",
        "                        page_kpis.append(k)\n",
        "                time.sleep(0.8)\n",
        "\n",
        "        # ‚ë° Ëã•‰ªç‰∏∫Á©∫ÔºåÂÜçÂàÜÊûêÊï¥È°µ\n",
        "        if not page_kpis and page_dict[pg][\"full\"] is not None:\n",
        "            for var_img, var_tag in generate_image_variants(\n",
        "                    page_dict[pg][\"full\"], 1200, 0, 0):   # Âè™ÂÅö original/resized\n",
        "                kpis = extract_kpi_from_image_fixed(\n",
        "                    var_img, pg, f\"full_{var_tag}\"\n",
        "                )\n",
        "                for k in kpis:\n",
        "                    key = generate_universal_metric_key(k)\n",
        "                    if key not in {generate_universal_metric_key(x) for x in page_kpis}:\n",
        "                        page_kpis.append(k)\n",
        "                time.sleep(1.0)\n",
        "\n",
        "        logging.info(f\"  ‚Üí Page {pg} KPI count: {len(page_kpis)}\")\n",
        "        all_image_kpis.extend(page_kpis)\n",
        "\n",
        "    logging.info(f\"Image KPI extraction finished: {len(all_image_kpis)} KPIs from {len(page_dict)} pages\")\n",
        "    return all_image_kpis"
      ],
      "metadata": {
        "id": "F5ZLpwL8Zok_"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "N7yh-bSwZonF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c9e95d-5571-4959-fff1-b29abb1a2296"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ ‰∏ªÂ§ÑÁêÜÂáΩÊï∞ ============\n",
        "def process_sustainability_report_with_enhanced_images(pdf_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Main processing function with image analysis\"\"\"\n",
        "    logging.info(\"Starting enhanced PDF processing with image analysis...\")\n",
        "\n",
        "    # Step 1: Text and table extraction\n",
        "    logging.info(\"Step 1/5: Reading PDF text and tables...\")\n",
        "    full_text = pdf_to_text_and_tables(pdf_path)\n",
        "\n",
        "    camelot_tables = camelot_extra_tables_enhanced(pdf_path)\n",
        "    if camelot_tables:\n",
        "        full_text += \"\\n\\n\" + \"\\n\\n\".join(camelot_tables)\n",
        "\n",
        "    logging.info(\"Step 2/5: Chunking text...\")\n",
        "    chunks = split_into_chunks(full_text, MAX_TOKENS_CHUNK)\n",
        "\n",
        "    logging.info(\"Step 3/5: Extracting KPIs from text...\")\n",
        "    text_kpis = []\n",
        "    for idx, chunk in enumerate(chunks, 1):\n",
        "        logging.info(f\"Processing text chunk {idx}/{len(chunks)}\")\n",
        "        if chunk.strip():\n",
        "            chunk_kpis = extract_kpi_from_chunk_universal(chunk)\n",
        "            text_kpis.extend(chunk_kpis)\n",
        "            if idx < len(chunks):\n",
        "                time.sleep(SLEEP_SEC)\n",
        "\n",
        "    # Step 4: Image KPI extraction\n",
        "    logging.info(\"Step 4/5: Extracting KPIs from images...\")\n",
        "    image_kpis = process_pdf_images_for_kpis_fixed(pdf_path)\n",
        "\n",
        "    # Step 5: Combine and process\n",
        "    logging.info(\"Step 5/5: Combining and processing all KPIs...\")\n",
        "\n",
        "    for kpi in text_kpis:\n",
        "        if 'source_type' not in kpi:\n",
        "            kpi['source_type'] = 'text'\n",
        "\n",
        "    all_kpis = text_kpis + image_kpis\n",
        "    all_kpis = post_process_kpis_universal(all_kpis)\n",
        "\n",
        "    df_auto = pd.DataFrame(all_kpis)\n",
        "\n",
        "    if not df_auto.empty:\n",
        "        if 'source_type' not in df_auto.columns:\n",
        "            df_auto['source_type'] = 'text'\n",
        "\n",
        "        initial_count = len(df_auto)\n",
        "        df_auto = df_auto.drop_duplicates(subset=['kpi_text'], keep='first')\n",
        "        final_count = len(df_auto)\n",
        "\n",
        "        logging.info(f\"Removed {initial_count - final_count} duplicate KPIs\")\n",
        "\n",
        "        try:\n",
        "            df_auto = df_auto.sort_values(['source_type', 'kpi_theme', 'kpi_category'], na_position='last')\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "        text_kpi_count = len([kpi for kpi in all_kpis if kpi.get('source_type', 'text') != 'image'])\n",
        "        image_kpi_count = len([kpi for kpi in all_kpis if kpi.get('source_type') == 'image'])\n",
        "\n",
        "        logging.info(f\"KPI Summary: {text_kpi_count} from text/tables, {image_kpi_count} from images\")\n",
        "\n",
        "    return df_auto\n"
      ],
      "metadata": {
        "id": "Mr1OIB2BZosj"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ ÁªìÊûú‰øùÂ≠òÂíåÊØîËæÉÂáΩÊï∞ ============\n",
        "def infer_stakeholder(row) -> str:\n",
        "    \"\"\"Infer affected stakeholders based on KPI theme and category\"\"\"\n",
        "    theme = row.get('kpi_theme', '').lower()\n",
        "    category = row.get('kpi_category', '').lower()\n",
        "    kpi_text = row.get('kpi_text', '').lower()\n",
        "\n",
        "    if theme == 'environmental':\n",
        "        return \"Environment, Community, Future Generations\"\n",
        "    elif theme == 'social':\n",
        "        if 'employee' in category or 'workforce' in category or 'gender' in category:\n",
        "            return \"Employees\"\n",
        "        elif 'customer' in category or 'safety' in category:\n",
        "            return \"Customers, Community\"\n",
        "        elif 'community' in category:\n",
        "            return \"Local Communities\"\n",
        "        elif 'supply' in category or 'supplier' in kpi_text:\n",
        "            return \"Suppliers, Business Partners\"\n",
        "        else:\n",
        "            return \"Employees, Community\"\n",
        "    elif theme == 'governance':\n",
        "        if 'board' in category:\n",
        "            return \"Shareholders, Investors\"\n",
        "        elif 'cyber' in category or 'data' in category:\n",
        "            return \"Customers, Employees, Business Partners\"\n",
        "        else:\n",
        "            return \"Shareholders, Investors, Stakeholders\"\n",
        "    else:\n",
        "        return \"All Stakeholders\"\n",
        "\n",
        "def save_results(df_auto: pd.DataFrame, output_path: str, pdf_path: str = \"\") -> None:\n",
        "    \"\"\"Save results to Excel file with proper formatting\"\"\"\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True)\n",
        "\n",
        "        if not df_auto.empty:\n",
        "            # Add metadata columns\n",
        "            pdf_filename = os.path.basename(pdf_path) if pdf_path else \"Unknown\"\n",
        "            df_auto['PDF file name'] = pdf_filename\n",
        "            df_auto['Title of the report'] = \"\"\n",
        "\n",
        "            if 'source_page' in df_auto.columns:\n",
        "                df_auto['Absolute Page Number'] = df_auto['source_page']\n",
        "                df_auto = df_auto.drop('source_page', axis=1)\n",
        "            else:\n",
        "                df_auto['Absolute Page Number'] = \"Unknown\"\n",
        "\n",
        "            df_auto['Impacted Stakeholder'] = df_auto.apply(infer_stakeholder, axis=1)\n",
        "\n",
        "            # Reorder columns\n",
        "            original_columns = [col for col in df_auto.columns if col not in\n",
        "                              ['PDF file name', 'Title of the report', 'Absolute Page Number', 'Impacted Stakeholder']]\n",
        "            new_column_order = ['PDF file name', 'Title of the report', 'Absolute Page Number', 'Impacted Stakeholder'] + original_columns\n",
        "            df_auto = df_auto[new_column_order]\n",
        "\n",
        "        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
        "            df_auto.to_excel(writer, sheet_name='Auto_KPIs', index=False)\n",
        "\n",
        "            if not df_auto.empty:\n",
        "                # Theme summary\n",
        "                theme_summary = df_auto.groupby('kpi_theme').size().reset_index(name='count')\n",
        "                theme_summary.to_excel(writer, sheet_name='Theme_Summary', index=False)\n",
        "\n",
        "                # Category summary\n",
        "                category_summary = df_auto.groupby(['kpi_theme', 'kpi_category']).size().reset_index(name='count')\n",
        "                category_summary.to_excel(writer, sheet_name='Category_Summary', index=False)\n",
        "\n",
        "        logging.info(f\"Results saved to {output_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving results: {e}\")\n",
        "\n",
        "def compare_with_manual_kpis(df_auto: pd.DataFrame, manual_xlsx_path: str) -> None:\n",
        "    \"\"\"Compare automatically extracted KPIs with manually annotated ones\"\"\"\n",
        "    if not os.path.exists(manual_xlsx_path):\n",
        "        logging.info(\"Manual KPI file not found, skipping comparison.\")\n",
        "        return\n",
        "\n",
        "    logging.info(\"Comparing with manual KPIs...\")\n",
        "\n",
        "    try:\n",
        "        df_manual = pd.read_excel(manual_xlsx_path)\n",
        "\n",
        "        if 'kpi_text' not in df_manual.columns:\n",
        "            logging.warning(\"Manual KPI file missing 'kpi_text' column\")\n",
        "            return\n",
        "\n",
        "        manual_kpis = set(df_manual['kpi_text'].astype(str).str.strip())\n",
        "        auto_kpis = set(df_auto['kpi_text'].astype(str).str.strip())\n",
        "\n",
        "        only_auto = auto_kpis - manual_kpis\n",
        "        only_manual = manual_kpis - auto_kpis\n",
        "        common = auto_kpis & manual_kpis\n",
        "\n",
        "        print(f\"\\n=== KPI Comparison Results ===\")\n",
        "        print(f\"Common KPIs: {len(common)}\")\n",
        "        print(f\"Only in automatic extraction: {len(only_auto)}\")\n",
        "        print(f\"Only in manual annotation: {len(only_manual)}\")\n",
        "\n",
        "        if only_auto:\n",
        "            print(f\"\\nKPIs found by model but not in manual annotation ({len(only_auto)}):\")\n",
        "            for kpi in sorted(only_auto):\n",
        "                if kpi.strip():\n",
        "                    print(f\"  - {kpi}\")\n",
        "\n",
        "        if only_manual:\n",
        "            print(f\"\\nKPIs in manual annotation but missed by model ({len(only_manual)}):\")\n",
        "            for kpi in sorted(only_manual):\n",
        "                if kpi.strip():\n",
        "                    print(f\"  - {kpi}\")\n",
        "\n",
        "        # Calculate metrics\n",
        "        if len(auto_kpis) > 0 and len(manual_kpis) > 0:\n",
        "            precision = len(common) / len(auto_kpis)\n",
        "            recall = len(common) / len(manual_kpis)\n",
        "            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "            print(f\"\\n=== Performance Metrics ===\")\n",
        "            print(f\"Precision: {precision:.3f}\")\n",
        "            print(f\"Recall: {recall:.3f}\")\n",
        "            print(f\"F1 Score: {f1_score:.3f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error comparing with manual KPIs: {e}\")"
      ],
      "metadata": {
        "id": "kucPzNTdZouV"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ ‰∏ªÊâßË°åÂáΩÊï∞ ============\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format=\"%(asctime)s - %(levelname)s: %(message)s\",\n",
        "        datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(PDF_PATH):\n",
        "            logging.error(f\"PDF file not found: {PDF_PATH}\")\n",
        "            return\n",
        "\n",
        "        # Process the PDF\n",
        "        df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
        "\n",
        "        # Save results\n",
        "        save_results(df_auto, EXPORT_AUTO_XLSX, PDF_PATH)\n",
        "\n",
        "        logging.info(f\"KPI extraction completed: {len(df_auto)} KPIs extracted\")\n",
        "\n",
        "        # Compare with manual annotations if available\n",
        "        if MANUAL_XLSX:\n",
        "            compare_with_manual_kpis(df_auto, MANUAL_XLSX)\n",
        "\n",
        "        # Display summary\n",
        "        if not df_auto.empty:\n",
        "            print(f\"\\n=== Extraction Summary ===\")\n",
        "            print(f\"Total KPIs extracted: {len(df_auto)}\")\n",
        "\n",
        "            # Source statistics\n",
        "            if 'source_type' in df_auto.columns:\n",
        "                source_counts = df_auto['source_type'].value_counts()\n",
        "                print(f\"From text/tables: {source_counts.get('text', 0)}\")\n",
        "                print(f\"From images/charts: {source_counts.get('image', 0)}\")\n",
        "\n",
        "            # Theme statistics\n",
        "            if 'kpi_theme' in df_auto.columns:\n",
        "                theme_counts = df_auto['kpi_theme'].value_counts()\n",
        "                print(f\"\\nKPI Distribution by Theme:\")\n",
        "                for theme, count in theme_counts.items():\n",
        "                    print(f\"  {theme}: {count}\")\n",
        "        else:\n",
        "            print(\"\\nNo KPIs were extracted from the document.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in main execution: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "uSueutkDZowK"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ ËæÖÂä©ÂäüËÉΩÂáΩÊï∞ ============\n",
        "def install_dependencies():\n",
        "    \"\"\"Install required dependencies\"\"\"\n",
        "    try:\n",
        "        import subprocess\n",
        "        import sys\n",
        "\n",
        "        dependencies = [\n",
        "            \"openai\",\n",
        "            \"python-dotenv\",\n",
        "            \"pdfplumber\",\n",
        "            \"tiktoken\",\n",
        "            \"pandas\",\n",
        "            \"PyMuPDF\",\n",
        "            \"Pillow\",\n",
        "            \"openpyxl\"\n",
        "        ]\n",
        "\n",
        "        for dep in dependencies:\n",
        "            try:\n",
        "                __import__(dep.replace('-', '_'))\n",
        "                print(f\"‚úÖ {dep} is already installed\")\n",
        "            except ImportError:\n",
        "                print(f\"Installing {dep}...\")\n",
        "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", dep])\n",
        "                print(f\"‚úÖ Installed {dep}\")\n",
        "\n",
        "        # Optional Camelot installation\n",
        "        try:\n",
        "            import camelot\n",
        "            print(\"‚úÖ Camelot is already installed\")\n",
        "        except ImportError:\n",
        "            print(\"Installing Camelot (optional)...\")\n",
        "            try:\n",
        "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"camelot-py[cv]\"])\n",
        "                print(\"‚úÖ Installed Camelot\")\n",
        "            except:\n",
        "                print(\"‚ö†Ô∏è Camelot installation failed (optional dependency)\")\n",
        "\n",
        "        print(\"üéâ All dependencies checked/installed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error with dependencies: {e}\")\n",
        "\n",
        "def validate_environment():\n",
        "    \"\"\"Validate environment setup\"\"\"\n",
        "    issues = []\n",
        "\n",
        "    # Check API key\n",
        "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "        issues.append(\"OPENAI_API_KEY not found in environment variables\")\n",
        "\n",
        "    # Check PDF file\n",
        "    if not os.path.exists(PDF_PATH):\n",
        "        issues.append(f\"PDF file not found: {PDF_PATH}\")\n",
        "\n",
        "    # Check required imports\n",
        "    required_modules = ['openai', 'pdfplumber', 'pandas', 'tiktoken', 'PIL', 'fitz']\n",
        "    for module in required_modules:\n",
        "        try:\n",
        "            __import__(module)\n",
        "        except ImportError:\n",
        "            issues.append(f\"Required module '{module}' not installed\")\n",
        "\n",
        "    if issues:\n",
        "        print(\"‚ùå Environment validation failed:\")\n",
        "        for issue in issues:\n",
        "            print(f\"  - {issue}\")\n",
        "        return False\n",
        "    else:\n",
        "        print(\"‚úÖ Environment validation passed\")\n",
        "        return True\n"
      ],
      "metadata": {
        "id": "qcCShhlea6ZK"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ ÁÆÄÂåñÁöÑÊâßË°åÊé•Âè£ ============\n",
        "def run_kpi_extraction():\n",
        "    \"\"\"Simplified interface to run KPI extraction\"\"\"\n",
        "    print(\"üöÄ Starting KPI extraction process...\")\n",
        "\n",
        "    # Validate environment\n",
        "    if not validate_environment():\n",
        "        print(\"Please fix the environment issues before running.\")\n",
        "        return\n",
        "\n",
        "    # Run main function\n",
        "    main()"
      ],
      "metadata": {
        "id": "Qw6B5H8ga6bl"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Ë∞ÉËØïÂíåÊµãËØïÂäüËÉΩ ============\n",
        "def test_text_extraction_only():\n",
        "    \"\"\"Test only text extraction without images\"\"\"\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    try:\n",
        "        # Extract text and tables\n",
        "        full_text = pdf_to_text_and_tables(PDF_PATH)\n",
        "        camelot_tables = camelot_extra_tables_enhanced(PDF_PATH)\n",
        "\n",
        "        if camelot_tables:\n",
        "            full_text += \"\\n\\n\" + \"\\n\\n\".join(camelot_tables)\n",
        "\n",
        "        # Chunk text\n",
        "        chunks = split_into_chunks(full_text, MAX_TOKENS_CHUNK)\n",
        "\n",
        "        # Extract KPIs from first few chunks\n",
        "        test_kpis = []\n",
        "        for idx, chunk in enumerate(chunks[:3]):  # Test first 3 chunks\n",
        "            chunk_kpis = extract_kpi_from_chunk_universal(chunk)\n",
        "            test_kpis.extend(chunk_kpis)\n",
        "            time.sleep(SLEEP_SEC)\n",
        "\n",
        "        print(f\"Test extraction completed: {len(test_kpis)} KPIs found in first 3 chunks\")\n",
        "\n",
        "        for i, kpi in enumerate(test_kpis[:5]):  # Show first 5\n",
        "            print(f\"{i+1}. {kpi.get('kpi_text', 'No text')}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Test failed: {e}\")\n",
        "\n",
        "def debug_single_image_analysis(image_path: str):\n",
        "    \"\"\"Test single image analysis functionality\"\"\"\n",
        "    try:\n",
        "        from PIL import Image\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        print(f\"Analyzing image: {image_path}\")\n",
        "        print(f\"Image size: {image.width}x{image.height}\")\n",
        "\n",
        "        kpis = extract_kpi_from_image_fixed(image, 1)\n",
        "\n",
        "        print(f\"\\n=== Analysis Results ===\")\n",
        "        print(f\"Found {len(kpis)} KPIs:\")\n",
        "\n",
        "        for i, kpi in enumerate(kpis, 1):\n",
        "            print(f\"\\n{i}. {kpi.get('kpi_text', 'No text')}\")\n",
        "            print(f\"   Value: {kpi.get('quantitative_value', 'No value')}\")\n",
        "            print(f\"   Confidence: {kpi.get('estimation_confidence', 'Not specified')}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in debug analysis: {e}\")\n",
        "\n",
        "def process_text_only():\n",
        "    \"\"\"Process only text and tables, skip images\"\"\"\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    try:\n",
        "        logging.info(\"Starting text-only processing...\")\n",
        "\n",
        "        # Step 1: Text and table extraction\n",
        "        full_text = pdf_to_text_and_tables(PDF_PATH)\n",
        "        camelot_tables = camelot_extra_tables_enhanced(PDF_PATH)\n",
        "\n",
        "        if camelot_tables:\n",
        "            full_text += \"\\n\\n\" + \"\\n\\n\".join(camelot_tables)\n",
        "\n",
        "        # Step 2: Chunking\n",
        "        chunks = split_into_chunks(full_text, MAX_TOKENS_CHUNK)\n",
        "\n",
        "        # Step 3: Extract KPIs\n",
        "        all_kpis = []\n",
        "        for idx, chunk in enumerate(chunks, 1):\n",
        "            logging.info(f\"Processing chunk {idx}/{len(chunks)}\")\n",
        "            if chunk.strip():\n",
        "                chunk_kpis = extract_kpi_from_chunk_universal(chunk)\n",
        "                all_kpis.extend(chunk_kpis)\n",
        "                if idx < len(chunks):\n",
        "                    time.sleep(SLEEP_SEC)\n",
        "\n",
        "        # Post-processing\n",
        "        all_kpis = post_process_kpis_universal(all_kpis)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        df_auto = pd.DataFrame(all_kpis)\n",
        "\n",
        "        if not df_auto.empty:\n",
        "            df_auto = df_auto.drop_duplicates(subset=['kpi_text'], keep='first')\n",
        "\n",
        "        # Save results\n",
        "        text_only_output = \"text_only_\" + EXPORT_AUTO_XLSX\n",
        "        save_results(df_auto, text_only_output, PDF_PATH)\n",
        "\n",
        "        print(f\"Text-only processing completed: {len(df_auto)} KPIs extracted\")\n",
        "\n",
        "        return df_auto\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Text-only processing failed: {e}\")\n",
        "        return pd.DataFrame()"
      ],
      "metadata": {
        "id": "Oku3umuia6dK"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ ÂÖºÂÆπÊÄßÂáΩÊï∞ ============\n",
        "def extract_kpi_from_chunk(chunk: str) -> List[Dict]:\n",
        "    \"\"\"Backward compatibility function\"\"\"\n",
        "    return extract_kpi_from_chunk_universal(chunk)\n",
        "\n",
        "def process_sustainability_report(pdf_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Backward compatibility function for text-only processing\"\"\"\n",
        "    return process_text_only()\n",
        "\n",
        "def process_sustainability_report_with_images(pdf_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Backward compatibility function for full processing\"\"\"\n",
        "    return process_sustainability_report_with_enhanced_images(pdf_path)\n"
      ],
      "metadata": {
        "id": "uhyRnexga6fA"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ ‰ΩøÁî®Á§∫‰æã ============\n",
        "def example_usage():\n",
        "    \"\"\"Usage examples\"\"\"\n",
        "    print(\"=== KPI Extraction Tool Usage Examples ===\\n\")\n",
        "\n",
        "    print(\"1. Full extraction (text + images):\")\n",
        "    print(\"   df_results = process_sustainability_report_with_enhanced_images(PDF_PATH)\")\n",
        "    print(\"   save_results(df_results, EXPORT_AUTO_XLSX, PDF_PATH)\\n\")\n",
        "\n",
        "    print(\"2. Text-only extraction:\")\n",
        "    print(\"   df_results = process_text_only()\")\n",
        "    print(\"   # Results automatically saved\\n\")\n",
        "\n",
        "    print(\"3. Simple run:\")\n",
        "    print(\"   run_kpi_extraction()  # Complete pipeline with validation\\n\")\n",
        "\n",
        "    print(\"4. Debug single component:\")\n",
        "    print(\"   test_text_extraction_only()  # Test first 3 chunks\")\n",
        "    print(\"   debug_single_image_analysis('path/to/image.jpg')\\n\")\n",
        "\n",
        "    print(\"5. Install dependencies:\")\n",
        "    print(\"   install_dependencies()  # Install all required packages\\n\")\n"
      ],
      "metadata": {
        "id": "fldjMEkaa6g1"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Ë∞ÉËØï‰ª£Á†Å - Áõ¥Êé•Â§çÂà∂Á≤òË¥¥Âà∞‰Ω†ÁöÑ‰ª£Á†ÅÊú´Â∞æ\n",
        "# ============================================================================\n",
        "\n",
        "# ÊñπÊ≥ï1: Ê£ÄÊü•ÊâÄÊúâÂõæÂÉèÊèêÂèñÂíåËØÜÂà´ÊÉÖÂÜµ\n",
        "def debug_method_1_check_image_detection():\n",
        "    \"\"\"Ê£ÄÊü•PDF‰∏≠ÁöÑÊâÄÊúâÂõæÂÉèÊòØÂê¶Ë¢´Ê≠£Á°ÆÊèêÂèñÔºå‰ª•ÂèäÂõæË°®ÂàÜÁ±ªÂô®ÊòØÂê¶Â∑•‰ΩúÊ≠£Â∏∏\"\"\"\n",
        "    print(\"=== ÊñπÊ≥ï1: Ê£ÄÊü•ÂõæÂÉèÊèêÂèñÂíåÂõæË°®ËØÜÂà´ ===\")\n",
        "\n",
        "    # ÂàõÂª∫Ë∞ÉËØïÊñá‰ª∂Â§π\n",
        "    import os\n",
        "    debug_folder = \"debug_images_method1\"\n",
        "    os.makedirs(debug_folder, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        # ÊèêÂèñÊâÄÊúâÂõæÂÉè\n",
        "        images = extract_images_from_pdf_fixed(PDF_PATH)\n",
        "        print(f\"‰ªéPDF‰∏≠ÊèêÂèñÂà∞ {len(images)} ‰∏™ÂõæÂÉè\")\n",
        "\n",
        "        chart_count = 0\n",
        "        non_chart_count = 0\n",
        "\n",
        "        for i, img_info in enumerate(images):\n",
        "            page_num = img_info['page_number']\n",
        "            img_type = img_info['type']\n",
        "            image = img_info['image']\n",
        "\n",
        "            # Ê£ÄÊü•ÊòØÂê¶Ë¢´ËØÜÂà´‰∏∫ÂõæË°®\n",
        "            is_chart = is_chart_image(image)\n",
        "\n",
        "            # ‰øùÂ≠òÂõæÂÉèÔºåÊñá‰ª∂ÂêçÂåÖÂê´ËØÜÂà´ÁªìÊûú\n",
        "            chart_status = \"CHART\" if is_chart else \"NOT_CHART\"\n",
        "            filename = f\"{debug_folder}/page_{page_num}_{img_type}_{chart_status}_{i}.jpg\"\n",
        "            image.save(filename)\n",
        "\n",
        "            print(f\"ÂõæÂÉè {i+1}: È°µÈù¢{page_num}, Á±ªÂûã{img_type}, Â∞∫ÂØ∏{image.width}x{image.height}, ÂõæË°®ËØÜÂà´:{is_chart}\")\n",
        "\n",
        "            if is_chart:\n",
        "                chart_count += 1\n",
        "            else:\n",
        "                non_chart_count += 1\n",
        "\n",
        "        print(f\"\\nÊÄªÁªì:\")\n",
        "        print(f\"- Ë¢´ËØÜÂà´‰∏∫ÂõæË°®ÁöÑÂõæÂÉè: {chart_count}\")\n",
        "        print(f\"- Êú™Ë¢´ËØÜÂà´‰∏∫ÂõæË°®ÁöÑÂõæÂÉè: {non_chart_count}\")\n",
        "        print(f\"- ÊâÄÊúâÂõæÂÉèÂ∑≤‰øùÂ≠òÂà∞ {debug_folder} Êñá‰ª∂Â§π\")\n",
        "        print(f\"- ËØ∑ÊâãÂä®Ê£ÄÊü• NOT_CHART ÁöÑÂõæÂÉèÔºåÁúãÊòØÂê¶ÂåÖÂê´‰Ω†Áº∫Â§±ÁöÑÈ•ºÂõæ\")\n",
        "\n",
        "        return images\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ÊñπÊ≥ï1ÊâßË°åÂá∫Èîô: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return []\n",
        "\n",
        "# ÊñπÊ≥ï2: ‰∏¥Êó∂Á¶ÅÁî®ÂõæË°®ÂàÜÁ±ªÂô®\n",
        "def debug_method_2_bypass_chart_filter():\n",
        "    \"\"\"ÂÆåÂÖ®Á¶ÅÁî®ÂõæË°®ÂàÜÁ±ªÂô®ÔºåÂº∫Âà∂Â§ÑÁêÜÊâÄÊúâÂõæÂÉè\"\"\"\n",
        "    print(\"=== ÊñπÊ≥ï2: Á¶ÅÁî®ÂõæË°®ÂàÜÁ±ªÂô® ===\")\n",
        "\n",
        "    # ‰øùÂ≠òÂéüÂßãÁöÑÂõæË°®ÂàÜÁ±ªÂô®ÂáΩÊï∞\n",
        "    global is_chart_image\n",
        "    original_chart_classifier = is_chart_image\n",
        "\n",
        "    # ÂàõÂª∫Êñ∞ÁöÑÂàÜÁ±ªÂô®ÔºàÊÄªÊòØËøîÂõûTrueÔºâ\n",
        "    def bypass_chart_classifier(image):\n",
        "        print(f\"  üîì Âº∫Âà∂Â§ÑÁêÜÂõæÂÉè (Â∞∫ÂØ∏: {image.width}x{image.height})\")\n",
        "        return True\n",
        "\n",
        "    # ‰∏¥Êó∂ÊõøÊç¢ÂàÜÁ±ªÂô®\n",
        "    is_chart_image = bypass_chart_classifier\n",
        "\n",
        "    try:\n",
        "        print(\"ÂºÄÂßãÈáçÊñ∞ÊèêÂèñÂõæÂÉèKPIÔºàÂ∑≤Á¶ÅÁî®ÂõæË°®ËøáÊª§Ôºâ...\")\n",
        "\n",
        "        # ÈáçÊñ∞ËøêË°åÂõæÂÉèÂ§ÑÁêÜ\n",
        "        image_kpis = process_pdf_images_for_kpis_fixed(PDF_PATH)\n",
        "\n",
        "        print(f\"Á¶ÅÁî®ËøáÊª§Âô®ÂêéÊèêÂèñÂà∞ {len(image_kpis)} ‰∏™ÂõæÂÉèKPI\")\n",
        "\n",
        "        # ÊòæÁ§∫ÁªìÊûú\n",
        "        for i, kpi in enumerate(image_kpis):\n",
        "            print(f\"{i+1}. È°µÈù¢{kpi.get('source_page', 'Unknown')}: {kpi.get('kpi_text', 'No text')[:100]}\")\n",
        "\n",
        "        return image_kpis\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ÊñπÊ≥ï2ÊâßË°åÂá∫Èîô: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return []\n",
        "\n",
        "    finally:\n",
        "        # ÊÅ¢Â§çÂéüÂßãÂàÜÁ±ªÂô®\n",
        "        is_chart_image = original_chart_classifier\n",
        "        print(\"Â∑≤ÊÅ¢Â§çÂéüÂßãÂõæË°®ÂàÜÁ±ªÂô®\")\n",
        "\n",
        "# ÊñπÊ≥ï3: ÊâãÂä®ÊµãËØïÁâπÂÆöÂõæÂÉè\n",
        "def debug_method_3_manual_test():\n",
        "    \"\"\"ÊâãÂä®ÈÄâÊã©ÂõæÂÉèËøõË°åÊµãËØï\"\"\"\n",
        "    print(\"=== ÊñπÊ≥ï3: ÊâãÂä®ÊµãËØïÁâπÂÆöÂõæÂÉè ===\")\n",
        "\n",
        "    try:\n",
        "        images = extract_images_from_pdf_fixed(PDF_PATH)\n",
        "        print(f\"ÊâæÂà∞ {len(images)} ‰∏™ÂõæÂÉè\")\n",
        "\n",
        "        # ÊòæÁ§∫ÊâÄÊúâÂõæÂÉè‰ø°ÊÅØ\n",
        "        for i, img_info in enumerate(images):\n",
        "            page_num = img_info['page_number']\n",
        "            img_type = img_info['type']\n",
        "            image = img_info['image']\n",
        "            is_chart = is_chart_image(image)\n",
        "\n",
        "            print(f\"{i+1}. È°µÈù¢{page_num}, Á±ªÂûã{img_type}, Â∞∫ÂØ∏{image.width}x{image.height}, ÂõæË°®:{is_chart}\")\n",
        "\n",
        "        # ËÆ©Áî®Êà∑ÈÄâÊã©Ë¶ÅÊµãËØïÁöÑÂõæÂÉè\n",
        "        while True:\n",
        "            try:\n",
        "                choice = input(f\"\\nËØ∑ÈÄâÊã©Ë¶ÅÊµãËØïÁöÑÂõæÂÉèÁºñÂè∑ (1-{len(images)}, ËæìÂÖ•0ÈÄÄÂá∫): \")\n",
        "                if choice == '0':\n",
        "                    break\n",
        "\n",
        "                img_index = int(choice) - 1\n",
        "                if 0 <= img_index < len(images):\n",
        "                    img_info = images[img_index]\n",
        "                    page_num = img_info['page_number']\n",
        "                    image = img_info['image']\n",
        "\n",
        "                    print(f\"\\nÊµãËØïÂõæÂÉè {choice} (È°µÈù¢ {page_num})\")\n",
        "\n",
        "                    # ‰øùÂ≠òËøô‰∏™ÂõæÂÉè‰æõÊ£ÄÊü•\n",
        "                    test_filename = f\"test_image_{choice}_page_{page_num}.jpg\"\n",
        "                    image.save(test_filename)\n",
        "                    print(f\"ÂõæÂÉèÂ∑≤‰øùÂ≠ò‰∏∫: {test_filename}\")\n",
        "\n",
        "                    # ÊµãËØïÊèêÂèñ\n",
        "                    kpis = extract_kpi_from_image_fixed(image, page_num, \"manual_test\")\n",
        "\n",
        "                    print(f\"ÊèêÂèñÁªìÊûú: {len(kpis)} ‰∏™KPI\")\n",
        "                    for j, kpi in enumerate(kpis):\n",
        "                        print(f\"  KPI {j+1}: {kpi.get('kpi_text', 'No text')}\")\n",
        "\n",
        "                else:\n",
        "                    print(\"Êó†ÊïàÁöÑÈÄâÊã©\")\n",
        "\n",
        "            except ValueError:\n",
        "                print(\"ËØ∑ËæìÂÖ•ÊúâÊïàÁöÑÊï∞Â≠ó\")\n",
        "            except KeyboardInterrupt:\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"ÊµãËØïÂá∫Èîô: {e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ÊñπÊ≥ï3ÊâßË°åÂá∫Èîô: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# ÊñπÊ≥ï4: ÁÆÄÂåñÁöÑÊèêÂèñÊµãËØï\n",
        "def debug_method_4_simple_test():\n",
        "    \"\"\"‰ΩøÁî®ÁÆÄÂåñÁöÑÊñπÊ≥ïÊµãËØïÂõæÂÉèÊèêÂèñ\"\"\"\n",
        "    print(\"=== ÊñπÊ≥ï4: ÁÆÄÂåñÊèêÂèñÊµãËØï ===\")\n",
        "\n",
        "    simple_prompt = \"\"\"ËØ∑ÂàÜÊûêËøô‰∏™ÂõæË°®ÔºåÊèêÂèñÊâÄÊúâÁöÑÊï∞Â≠óÊï∞ÊçÆ„ÄÇ\n",
        "\n",
        "ËøîÂõûJSONÊ†ºÂºèÔºåÊØè‰∏™Êï∞ÊçÆÁÇπÂåÖÂê´Ôºö\n",
        "{\n",
        "  \"description\": \"Êï∞ÊçÆÊèèËø∞\",\n",
        "  \"value\": \"Êï∞ÂÄº\",\n",
        "  \"unit\": \"Âçï‰Ωç\"\n",
        "}\n",
        "\n",
        "Â¶ÇÊûúÊòØÈ•ºÂõæÔºåËØ∑ÊèêÂèñÊØè‰∏™ÊâáÂΩ¢ÁöÑÁôæÂàÜÊØî„ÄÇ\n",
        "Â¶ÇÊûúÊòØÊü±Áä∂ÂõæÔºåËØ∑ÊèêÂèñÊØè‰∏™Êü±Â≠êÁöÑÊï∞ÂÄº„ÄÇ\n",
        "Â¶ÇÊûúÊòØË°®Ê†ºÔºåËØ∑ÊèêÂèñÊØè‰∏™Êï∞Â≠ó„ÄÇ\"\"\"\n",
        "\n",
        "    try:\n",
        "        images = extract_images_from_pdf_fixed(PDF_PATH)\n",
        "\n",
        "        for i, img_info in enumerate(images[:5]):  # Âè™ÊµãËØïÂâç5‰∏™ÂõæÂÉè\n",
        "            page_num = img_info['page_number']\n",
        "            image = img_info['image']\n",
        "\n",
        "            print(f\"\\nüîç ÁÆÄÂåñÊµãËØïÂõæÂÉè {i+1} (È°µÈù¢ {page_num})\")\n",
        "\n",
        "            try:\n",
        "                base64_image = image_to_base64_fixed(image)\n",
        "                if not base64_image:\n",
        "                    continue\n",
        "\n",
        "                response = client.chat.completions.create(\n",
        "                    model=\"gpt-4o\",\n",
        "                    messages=[{\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": [\n",
        "                            {\"type\": \"text\", \"text\": simple_prompt},\n",
        "                            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
        "                        ]\n",
        "                    }],\n",
        "                    temperature=0.0,\n",
        "                    max_tokens=2000,\n",
        "                    timeout=30\n",
        "                )\n",
        "\n",
        "                content = response.choices[0].message.content.strip()\n",
        "                print(f\"APIÂìçÂ∫î: {content[:300]}...\")\n",
        "\n",
        "                # Ê£ÄÊü•ÊòØÂê¶ÂåÖÂê´‰Ω†Ë¶ÅÊâæÁöÑÊï∞ÊçÆ\n",
        "                if \"property type\" in content.lower() or \"service type\" in content.lower():\n",
        "                    print(\"üéâ ÂèØËÉΩÊâæÂà∞‰∫ÜÁº∫Â§±ÁöÑÈ•ºÂõæÊï∞ÊçÆÔºÅ\")\n",
        "                    print(f\"ÂÆåÊï¥ÂìçÂ∫î: {content}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå ÁÆÄÂåñÊµãËØïÂ§±Ë¥•: {e}\")\n",
        "\n",
        "            time.sleep(1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ÊñπÊ≥ï4ÊâßË°åÂá∫Èîô: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Ë∞ÉËØï‰∏ªÊéßÂà∂ÂáΩÊï∞\n",
        "def run_debugging_session():\n",
        "    \"\"\"Ë∞ÉËØï‰ºöËØù‰∏ªÊéßÂà∂ÂáΩÊï∞\"\"\"\n",
        "    print(\"üîß KPIÊèêÂèñË∞ÉËØï‰ºöËØù\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            choice = input(\"\"\"\\nÈÄâÊã©Ë∞ÉËØïÊñπÊ≥ïÔºö\n",
        "1 - Ê£ÄÊü•ÊâÄÊúâÂõæÂÉèÁöÑÊèêÂèñÂíåËØÜÂà´ÊÉÖÂÜµ\n",
        "2 - Á¶ÅÁî®ÂõæË°®ÂàÜÁ±ªÂô®ÔºåÂº∫Âà∂Â§ÑÁêÜÊâÄÊúâÂõæÂÉè\n",
        "3 - ÊâãÂä®ÈÄâÊã©ÂõæÂÉèËøõË°åÊµãËØï\n",
        "4 - ÁÆÄÂåñAPIÊµãËØïÔºàÊµãËØïÂâç5‰∏™ÂõæÂÉèÔºâ\n",
        "0 - ÈÄÄÂá∫Ë∞ÉËØï\n",
        "\n",
        "ËØ∑ËæìÂÖ•ÈÄâÊã© (0-4): \"\"\")\n",
        "\n",
        "            if choice == \"1\":\n",
        "                debug_method_1_check_image_detection()\n",
        "            elif choice == \"2\":\n",
        "                debug_method_2_bypass_chart_filter()\n",
        "            elif choice == \"3\":\n",
        "                debug_method_3_manual_test()\n",
        "            elif choice == \"4\":\n",
        "                debug_method_4_simple_test()\n",
        "            elif choice == \"0\":\n",
        "                print(\"ÈÄÄÂá∫Ë∞ÉËØï‰ºöËØù\")\n",
        "                break\n",
        "            else:\n",
        "                print(\"Êó†ÊïàÈÄâÊã©ÔºåËØ∑ÈáçÊñ∞ËæìÂÖ•\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nÁî®Êà∑‰∏≠Êñ≠ÔºåÈÄÄÂá∫Ë∞ÉËØï\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Ë∞ÉËØï‰ºöËØùÂá∫Èîô: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "# ============================================================================\n",
        "# ÂçïÁã¨ÁöÑÂø´ÈÄüÊµãËØïÂáΩÊï∞ÔºàÂ¶ÇÊûú‰Ω†‰∏çÊÉ≥Áî®‰∫§‰∫íÂºèÁïåÈù¢Ôºâ\n",
        "# ============================================================================\n",
        "\n",
        "def quick_debug():\n",
        "    \"\"\"Âø´ÈÄüË∞ÉËØï - Áõ¥Êé•ËøêË°åÊñπÊ≥ï1\"\"\"\n",
        "    print(\"üöÄ Âø´ÈÄüË∞ÉËØïÊ®°Âºè\")\n",
        "    debug_method_1_check_image_detection()\n",
        "\n",
        "# ============================================================================\n",
        "# ‰ΩøÁî®ÊñπÊ≥ï\n",
        "# ============================================================================\n",
        "\n",
        "# Âú®‰Ω†ÁöÑ‰ª£Á†ÅÊúÄÂêéÔºåÁé∞Âú®ÂèØ‰ª•ËøêË°å‰ª•‰∏ã‰ªªÊÑè‰∏Ä‰∏™Ôºö\n",
        "\n",
        "# ÈÄâÈ°π1: ‰∫§‰∫íÂºèË∞ÉËØïÔºàÊé®ËçêÔºâ\n",
        "# run_debugging_session()\n",
        "\n",
        "# ÈÄâÈ°π2: Âø´ÈÄüË∞ÉËØïÔºåÁõ¥Êé•Ê£ÄÊü•ÂõæÂÉè\n",
        "# quick_debug()\n",
        "\n",
        "# ÈÄâÈ°π3: Áõ¥Êé•ËøêË°åÁâπÂÆöÊñπÊ≥ï\n",
        "# debug_method_1_check_image_detection()"
      ],
      "metadata": {
        "id": "zXuWkRTCgfWE"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ÊµãËØïÂáΩÊï∞ÔºöÈ™åËØÅÊñ∞ÁöÑÊèêÁ§∫ËØçÊòØÂê¶ÊúâÊïà\n",
        "def test_improved_prompt():\n",
        "    \"\"\"ÊµãËØïÊîπËøõÁöÑÊèêÁ§∫ËØçÊòØÂê¶ËÉΩÊ≠£Á°ÆÊèêÂèñÁôæÂàÜÊØî\"\"\"\n",
        "    print(\"=== ÊµãËØïÊîπËøõÁöÑÊèêÁ§∫ËØç ===\")\n",
        "\n",
        "    try:\n",
        "        # ÊèêÂèñÈ°µÈù¢2ÁöÑÂÖ®È°µÂõæÂÉè\n",
        "        images = extract_images_from_pdf_fixed(PDF_PATH)\n",
        "        page2_image = None\n",
        "\n",
        "        for img_info in images:\n",
        "            if img_info['page_number'] == 2 and img_info['type'] == 'full_page':\n",
        "                page2_image = img_info['image']\n",
        "                break\n",
        "\n",
        "        if page2_image is None:\n",
        "            print(\"‚ùå Êâæ‰∏çÂà∞È°µÈù¢2ÂõæÂÉè\")\n",
        "            return\n",
        "\n",
        "        print(f\"‚úÖ ÊâæÂà∞È°µÈù¢2ÂõæÂÉèÔºåÂ∞∫ÂØ∏: {page2_image.width}x{page2_image.height}\")\n",
        "\n",
        "        # ‰ΩøÁî®ÊîπËøõÁöÑÊèêÁ§∫ËØçÊµãËØï\n",
        "        base64_image = image_to_base64_fixed(page2_image)\n",
        "\n",
        "        print(\"üîÑ ‰ΩøÁî®ÊîπËøõÁöÑÊèêÁ§∫ËØçË∞ÉÁî®API...\")\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": ENHANCED_IMAGE_KPI_SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": \"\"\"ÂàÜÊûêËøô‰∏™È°µÈù¢ÔºåÈáçÁÇπÂÖ≥Ê≥®‰∏§‰∏™È•ºÂõæÔºö\n",
        "\n",
        "1. ‰∏äÊñπÈ•ºÂõæÔºö\"Energy Use by Property Type 2021\"\n",
        "2. ‰∏ãÊñπÈ•ºÂõæÔºö\"Energy Use by Service Type 2021\"\n",
        "\n",
        "ËØ∑ÊèêÂèñÊØè‰∏™È•ºÂõæ‰∏≠ÊØè‰∏™ÊâáÂΩ¢ÁöÑÂÖ∑‰ΩìÁôæÂàÜÊØîÊï∞ÂÄº„ÄÇÁ°Æ‰øùÂåÖÂê´ÂÆûÈôÖÁöÑÊï∞Â≠óÔºå‰∏çÂè™ÊòØÊèèËø∞„ÄÇ\"\"\"},\n",
        "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\", \"detail\": \"high\"}}\n",
        "                ]}\n",
        "            ],\n",
        "            temperature=0.0,\n",
        "            max_tokens=4000,\n",
        "            timeout=90\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message.content.strip()\n",
        "        print(\"\\nüìã APIÂìçÂ∫î:\")\n",
        "        print(content[:500] + \"...\" if len(content) > 500 else content)\n",
        "\n",
        "        # Â∞ùËØïËß£ÊûêJSON\n",
        "        try:\n",
        "            if content.startswith('```json'):\n",
        "                content = content[7:]\n",
        "            if content.endswith('```'):\n",
        "                content = content[:-3]\n",
        "            content = content.strip()\n",
        "\n",
        "            if content.startswith('['):\n",
        "                result = json.loads(content)\n",
        "                print(f\"\\n‚úÖ ÊàêÂäüËß£ÊûêJSONÔºåÊâæÂà∞ {len(result)} ‰∏™KPI\")\n",
        "\n",
        "                # Ê£ÄÊü•ÊòØÂê¶ÊèêÂèñ‰∫ÜÂÖ∑‰ΩìÁöÑÁôæÂàÜÊØî\n",
        "                found_percentages = []\n",
        "                for kpi in result:\n",
        "                    if isinstance(kpi, dict):\n",
        "                        kpi_text = kpi.get('kpi_text', '')\n",
        "                        quantitative_value = kpi.get('quantitative_value', '')\n",
        "\n",
        "                        print(f\"KPI: {kpi_text}\")\n",
        "                        print(f\"  Êï∞ÂÄº: {quantitative_value}\")\n",
        "\n",
        "                        # Ê£ÄÊü•ÊòØÂê¶ÂåÖÂê´ÊúüÊúõÁöÑÁôæÂàÜÊØî\n",
        "                        if any(target in kpi_text.lower() for target in ['64%', '33%', '68%', '30%', 'healthcare center', 'medical office', 'electricity', 'fuel']):\n",
        "                            found_percentages.append(kpi)\n",
        "                            print(f\"  üéØ ÊâæÂà∞ÁõÆÊ†áÊï∞ÊçÆ!\")\n",
        "                        print()\n",
        "\n",
        "                if found_percentages:\n",
        "                    print(f\"üéâ ÊàêÂäüÊèêÂèñ‰∫Ü {len(found_percentages)} ‰∏™ÂåÖÂê´ÂÖ∑‰ΩìÁôæÂàÜÊØîÁöÑKPI!\")\n",
        "                    return True\n",
        "                else:\n",
        "                    print(\"‚ùå ‰ªçÁÑ∂Ê≤°ÊúâÊèêÂèñÂà∞ÂÖ∑‰ΩìÁöÑÁôæÂàÜÊØîÊï∞ÂÄº\")\n",
        "                    return False\n",
        "            else:\n",
        "                print(\"‚ùå APIÂìçÂ∫î‰∏çÊòØJSONÊ†ºÂºè\")\n",
        "                return False\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"‚ùå JSONËß£ÊûêÂ§±Ë¥•: {e}\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ÊµãËØïÂ§±Ë¥•: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "# Âø´ÈÄü‰øÆÂ§çÂáΩÊï∞ÔºöÊõøÊç¢ÊèêÁ§∫ËØçÂπ∂ÈáçÊñ∞ËøêË°åÊèêÂèñ\n",
        "def quick_fix_and_rerun():\n",
        "    \"\"\"Â∫îÁî®‰øÆÂ§çÂπ∂ÈáçÊñ∞ËøêË°åÂÆåÊï¥ÁöÑÊèêÂèñÊµÅÁ®ã\"\"\"\n",
        "    print(\"üîß Â∫îÁî®‰øÆÂ§çÂπ∂ÈáçÊñ∞ËøêË°å...\")\n",
        "\n",
        "    # È¶ñÂÖàÊµãËØïÊñ∞ÊèêÁ§∫ËØç\n",
        "    if test_improved_prompt():\n",
        "        print(\"\\n‚úÖ Êñ∞ÊèêÁ§∫ËØçÊµãËØïÊàêÂäü!\")\n",
        "\n",
        "        # ÈáçÊñ∞ËøêË°åÂÆåÊï¥ÁöÑÊèêÂèñÊµÅÁ®ã\n",
        "        print(\"\\nüîÑ ÈáçÊñ∞ËøêË°åÂÆåÊï¥ÁöÑKPIÊèêÂèñ...\")\n",
        "        try:\n",
        "            df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
        "\n",
        "            # ‰øùÂ≠òÁªìÊûú\n",
        "            save_results(df_auto, \"fixed_\" + EXPORT_AUTO_XLSX, PDF_PATH)\n",
        "\n",
        "            print(f\"\\nüéâ ‰øÆÂ§çÂÆåÊàê! ÊÄªÂÖ±ÊèêÂèñ‰∫Ü {len(df_auto)} ‰∏™KPI\")\n",
        "            print(\"ÁªìÊûúÂ∑≤‰øùÂ≠òÂà∞ fixed_\" + EXPORT_AUTO_XLSX)\n",
        "\n",
        "            # ÊòæÁ§∫ÂåÖÂê´È•ºÂõæÊï∞ÊçÆÁöÑKPI\n",
        "            pie_chart_kpis = df_auto[df_auto['kpi_text'].str.contains('pie|Pie', case=False, na=False)]\n",
        "            print(f\"\\nüìä È•ºÂõæÁõ∏ÂÖ≥ÁöÑKPI ({len(pie_chart_kpis)} ‰∏™):\")\n",
        "            for idx, row in pie_chart_kpis.iterrows():\n",
        "                print(f\"- {row['kpi_text']}\")\n",
        "\n",
        "            return df_auto\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå ÈáçÊñ∞ËøêË°åÂ§±Ë¥•: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(\"\\n‚ùå Êñ∞ÊèêÁ§∫ËØçÊµãËØïÂ§±Ë¥•ÔºåÈúÄË¶ÅËøõ‰∏ÄÊ≠•Ë∞ÉËØï\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "Yq5MEBJLop0V"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ÈÄöÁî®ÊµãËØïÂáΩÊï∞\n",
        "def test_universal_prompt():\n",
        "    \"\"\"ÊµãËØïÈÄöÁî®ÊèêÁ§∫ËØçÁöÑÊïàÊûú\"\"\"\n",
        "    print(\"=== ÊµãËØïÈÄöÁî®ÂõæÂÉèÂàÜÊûêÊèêÁ§∫ËØç ===\")\n",
        "\n",
        "    try:\n",
        "        # ÊèêÂèñÈ°µÈù¢2ÁöÑÂÖ®È°µÂõæÂÉèËøõË°åÊµãËØï\n",
        "        images = extract_images_from_pdf_fixed(PDF_PATH)\n",
        "        page2_image = None\n",
        "\n",
        "        for img_info in images:\n",
        "            if img_info['page_number'] == 2 and img_info['type'] == 'full_page':\n",
        "                page2_image = img_info['image']\n",
        "                break\n",
        "\n",
        "        if page2_image is None:\n",
        "            print(\"‚ùå Êâæ‰∏çÂà∞È°µÈù¢2ÂõæÂÉè\")\n",
        "            return False\n",
        "\n",
        "        print(f\"‚úÖ ÊâæÂà∞È°µÈù¢2ÂõæÂÉèÔºåÂ∞∫ÂØ∏: {page2_image.width}x{page2_image.height}\")\n",
        "\n",
        "        # ‰ΩøÁî®ÈÄöÁî®ÊèêÁ§∫ËØçÊµãËØï\n",
        "        base64_image = image_to_base64_fixed(page2_image)\n",
        "\n",
        "        print(\"üîÑ ‰ΩøÁî®ÈÄöÁî®ÊèêÁ§∫ËØçË∞ÉÁî®API...\")\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": ENHANCED_IMAGE_KPI_SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": \"\"\"ËØ∑ÂàÜÊûêËøô‰∏™È°µÈù¢‰∏≠ÁöÑÊâÄÊúâÂõæË°®ÂíåË°®Ê†ºÔºåÊèêÂèñÊâÄÊúâÂèØÈáèÂåñÁöÑÊï∞ÊçÆÁÇπ„ÄÇ\n",
        "\n",
        "ÈáçÁÇπÂÖ≥Ê≥®Ôºö\n",
        "- È•ºÂõæ‰∏≠ÊØè‰∏™ÊâáÂΩ¢ÁöÑÂÖ∑‰ΩìÁôæÂàÜÊØî\n",
        "- Ë°®Ê†º‰∏≠ÁöÑÊâÄÊúâÊï∞ÂÄºÊï∞ÊçÆ\n",
        "- Á°Æ‰øùÊØè‰∏™ÊèêÂèñÁöÑKPIÈÉΩÂåÖÂê´ÂÖ∑‰ΩìÁöÑÊï∞Â≠óÔºå‰∏çÂè™ÊòØÊèèËø∞\n",
        "\n",
        "ËØ∑Á°Æ‰øùÊèêÂèñÂÆåÊï¥ÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØ„ÄÇ\"\"\"},\n",
        "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\", \"detail\": \"high\"}}\n",
        "                ]}\n",
        "            ],\n",
        "            temperature=0.0,\n",
        "            max_tokens=4000,\n",
        "            timeout=90\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message.content.strip()\n",
        "        print(f\"\\nüìã APIÂìçÂ∫îÈïøÂ∫¶: {len(content)} Â≠óÁ¨¶\")\n",
        "\n",
        "        # Ëß£ÊûêÂíåÈ™åËØÅÁªìÊûú\n",
        "        try:\n",
        "            if content.startswith('```json'):\n",
        "                content = content[7:]\n",
        "            if content.endswith('```'):\n",
        "                content = content[:-3]\n",
        "            content = content.strip()\n",
        "\n",
        "            if content.startswith('['):\n",
        "                result = json.loads(content)\n",
        "                print(f\"‚úÖ ÊàêÂäüËß£ÊûêJSONÔºåÊâæÂà∞ {len(result)} ‰∏™KPI\")\n",
        "\n",
        "                # ÂàÜÊûêÊèêÂèñË¥®Èáè\n",
        "                complete_kpis = 0\n",
        "                pie_chart_kpis = 0\n",
        "                table_kpis = 0\n",
        "\n",
        "                print(\"\\nüìä ÊèêÂèñÁöÑKPIÂàóË°®:\")\n",
        "                for i, kpi in enumerate(result, 1):\n",
        "                    if isinstance(kpi, dict):\n",
        "                        kpi_text = kpi.get('kpi_text', '')\n",
        "                        quantitative_value = kpi.get('quantitative_value', '')\n",
        "                        chart_type = kpi.get('chart_type', '')\n",
        "\n",
        "                        print(f\"{i:2d}. {kpi_text}\")\n",
        "                        print(f\"    Êï∞ÂÄº: {quantitative_value} {kpi.get('unit', '')}\")\n",
        "                        print(f\"    Á±ªÂûã: {chart_type}\")\n",
        "\n",
        "                        # ÁªüËÆ°ÂàÜÊûê\n",
        "                        if quantitative_value and str(quantitative_value).strip():\n",
        "                            complete_kpis += 1\n",
        "\n",
        "                        if 'pie' in chart_type.lower():\n",
        "                            pie_chart_kpis += 1\n",
        "                        elif 'table' in chart_type.lower():\n",
        "                            table_kpis += 1\n",
        "\n",
        "                        print()\n",
        "\n",
        "                print(f\"üìà Ë¥®ÈáèÂàÜÊûê:\")\n",
        "                print(f\"  - ÂåÖÂê´Êï∞ÂÄºÁöÑKPI: {complete_kpis}/{len(result)} ({complete_kpis/len(result)*100:.1f}%)\")\n",
        "                print(f\"  - È•ºÂõæKPI: {pie_chart_kpis}\")\n",
        "                print(f\"  - Ë°®Ê†ºKPI: {table_kpis}\")\n",
        "\n",
        "                # Ê£ÄÊü•ÊòØÂê¶ÊèêÂèñ‰∫ÜÁõÆÊ†áÊï∞ÊçÆ\n",
        "                success_indicators = [\n",
        "                    any('64' in str(kpi.get('quantitative_value', '')) for kpi in result),\n",
        "                    any('33' in str(kpi.get('quantitative_value', '')) for kpi in result),\n",
        "                    any('68' in str(kpi.get('quantitative_value', '')) for kpi in result),\n",
        "                    any('30' in str(kpi.get('quantitative_value', '')) for kpi in result)\n",
        "                ]\n",
        "\n",
        "                if any(success_indicators):\n",
        "                    print(\"üéâ ÊàêÂäüÊèêÂèñ‰∫ÜÁõÆÊ†áÈ•ºÂõæÊï∞ÊçÆ!\")\n",
        "                    return True\n",
        "                else:\n",
        "                    print(\"‚ö†Ô∏è ÂèØËÉΩÊ≤°ÊúâÊèêÂèñÂà∞ÊúüÊúõÁöÑÈ•ºÂõæÁôæÂàÜÊØî\")\n",
        "                    return False\n",
        "            else:\n",
        "                print(\"‚ùå APIÂìçÂ∫î‰∏çÊòØJSONÊ†ºÂºè\")\n",
        "                print(f\"ÂìçÂ∫îÂÜÖÂÆπ: {content[:300]}...\")\n",
        "                return False\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"‚ùå JSONËß£ÊûêÂ§±Ë¥•: {e}\")\n",
        "            print(f\"ÂìçÂ∫îÂÜÖÂÆπ: {content[:300]}...\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ÊµãËØïÂ§±Ë¥•: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "# Â∫îÁî®ÈÄöÁî®‰øÆÂ§ç\n",
        "def apply_universal_fix():\n",
        "    \"\"\"Â∫îÁî®ÈÄöÁî®ÊèêÁ§∫ËØç‰øÆÂ§çÂπ∂ÈáçÊñ∞ËøêË°å\"\"\"\n",
        "    print(\"üîß Â∫îÁî®ÈÄöÁî®ÊèêÁ§∫ËØç‰øÆÂ§ç...\")\n",
        "\n",
        "    # È¶ñÂÖàÊµãËØïÊñ∞ÊèêÁ§∫ËØç\n",
        "    print(\"Á¨¨‰∏ÄÊ≠•: ÊµãËØïÊñ∞ÁöÑÈÄöÁî®ÊèêÁ§∫ËØç...\")\n",
        "    if test_universal_prompt():\n",
        "        print(\"\\n‚úÖ ÈÄöÁî®ÊèêÁ§∫ËØçÊµãËØïÊàêÂäü!\")\n",
        "\n",
        "        # ËØ¢ÈóÆÊòØÂê¶ÁªßÁª≠ÂÆåÊï¥ÊèêÂèñ\n",
        "        try:\n",
        "            proceed = input(\"\\nÊòØÂê¶ÁªßÁª≠ËøêË°åÂÆåÊï¥ÁöÑKPIÊèêÂèñ? (y/n): \").lower()\n",
        "            if proceed == 'y':\n",
        "                print(\"\\nüîÑ ÈáçÊñ∞ËøêË°åÂÆåÊï¥ÁöÑKPIÊèêÂèñ...\")\n",
        "                df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
        "\n",
        "                # ‰øùÂ≠òÁªìÊûú\n",
        "                output_file = \"universal_fixed_\" + EXPORT_AUTO_XLSX\n",
        "                save_results(df_auto, output_file, PDF_PATH)\n",
        "\n",
        "                print(f\"\\nüéâ ‰øÆÂ§çÂÆåÊàê! ÊÄªÂÖ±ÊèêÂèñ‰∫Ü {len(df_auto)} ‰∏™KPI\")\n",
        "                print(f\"ÁªìÊûúÂ∑≤‰øùÂ≠òÂà∞ {output_file}\")\n",
        "\n",
        "                # ÊòæÁ§∫ÂõæÂÉèÊù•Ê∫êÁöÑKPIÁªüËÆ°\n",
        "                if 'source_type' in df_auto.columns:\n",
        "                    image_kpis = df_auto[df_auto['source_type'] == 'image']\n",
        "                    print(f\"\\nüìä ‰ªéÂõæÂÉè‰∏≠ÊèêÂèñÁöÑKPI: {len(image_kpis)} ‰∏™\")\n",
        "\n",
        "                return df_auto\n",
        "            else:\n",
        "                print(\"Â∑≤ÂèñÊ∂àÂÆåÊï¥ÊèêÂèñ\")\n",
        "                return None\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nÁî®Êà∑ÂèñÊ∂àÊìç‰Ωú\")\n",
        "            return None\n",
        "\n",
        "    else:\n",
        "        print(\"\\n‚ùå ÈÄöÁî®ÊèêÁ§∫ËØçÊµãËØïÂ§±Ë¥•\")\n",
        "        print(\"Âª∫ËÆÆÊ£ÄÊü•APIÂìçÂ∫îÊàñËøõ‰∏ÄÊ≠•Ë∞ÉÊï¥ÊèêÁ§∫ËØç\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "7CYTzMwVqSbv"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ ÊâßË°åÂÖ•Âè£ ============\n",
        "if __name__ == \"__main__\":\n",
        "    # Uncomment to install dependencies first\n",
        "    # install_dependencies()\n",
        "\n",
        "    # Uncomment to see usage examples\n",
        "    # example_usage()\n",
        "\n",
        "    # Run the main extraction\n",
        "    run_kpi_extraction()\n",
        "    #run_debugging_session()\n",
        "    #apply_universal_fix()"
      ],
      "metadata": {
        "id": "_ByxPUdia6io",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b8c0791-d568-422e-f959-d6d667aeb47b"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting KPI extraction process...\n",
            "‚úÖ Environment validation passed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (0, 0, 595.0, 793.0)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (130.458984, 281.02200000000005, 488.56800000000055, 649.988)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (24.01875000000001, 626.215, 565.7600000000002, 758.1642608695653)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (17.619999999999997, 598.225, 567.4499999999999, 762.8299393939394)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (20.459999999999994, 218.20000000000002, 552.6464999999996, 599.0939393939393)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (24.020000000000003, 583.955, 569.6875000000001, 755.6360000000001)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (29.269, 357.42449999999997, 546.6355000000009, 606.349)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (17.619999999999997, 626.215, 562.7475000000002, 750.6956153846154)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (66.62, 403.738, 566.2579999999992, 612.1506153846153)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (101.997, 182.26, 572.2369999999987, 424.5196153846154)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (24.009999999999998, 595.7725, 574.7062499999998, 762.107875)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (127.01600000000005, 183.155, 455.5960000000003, 435.16445454545453)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:218: UserWarning: No tables found on page-31\n",
            "  if self._document_has_no_text():\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:238: UserWarning: No tables found in table area (24.020000000000003, 281.706, 319.82149999999984, 459.1270769230769)\n",
            "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
            "/usr/local/lib/python3.11/dist-packages/camelot/parsers/base.py:218: UserWarning: No tables found on page-31\n",
            "  if self._document_has_no_text():\n",
            "ERROR:root:API call failed: Request timed out.\n",
            "WARNING:root:API response not JSON list: The provided text does not contain any specific numbers, percentages, or measurable quantities that ...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It is s...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It is s...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts, graphs, or quantifiable performance data. It is a vi...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It is a...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is the cover of the \"Canon Environmental Report 1999\" and does not contain any ch...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is the cover of the \"Canon Environmental Report 1999\" and does not contain any ch...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a text-based message from the President of Canon Inc. It does not contain any ...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a text-based document listing operational sites surveyed for Canon's environme...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a page from the Canon Environmental Report. It primarily contains textual info...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is completely gray and does not contain any visible charts or data. Therefore, no...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is completely gray and does not contain any visible charts or data. Therefore, no...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is completely gray and does not contain any visible charts or data. Therefore, no...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any visible charts or graphs with quantifiable performance data....\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is completely gray and does not contain any visible charts or data. Therefore, no...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a text-based document outlining the \"Philosophy and Environment Assurance Guid...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It prim...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains a table outlining voluntary action plans related to environmental assurance activ...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains a table outlining voluntary action plans related to environmental assurance activ...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a text-based document outlining the environmental policy and organizational st...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is primarily a textual document outlining the organizational structure and commit...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains a textual description of an Environmental Management System rather than a chart o...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a text-based document describing the Environmental Management System of Canon....\n",
            "ERROR:root:Error extracting KPIs from image: Request timed out.\n",
            "ERROR:root:Error extracting KPIs from image: Request timed out.\n",
            "WARNING:root:Image analysis response not JSON list: The image contains a table detailing the control of chemical substances, categorized by control leve...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains a table listing different categories of chemical substances and their control lev...\n",
            "WARNING:root:Image analysis response not JSON list: The image is a table titled \"1998 Control Balance Sheet (PRTR)\" showing data related to the use and ...\n",
            "WARNING:root:Image analysis response not JSON list: The image is a table titled \"1998 Control Balance Sheet (PRTR)\" showing data on the use and emission...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains a bar chart titled \"Number of Audited Sites\" and a table titled \"1998 Auditing.\" ...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains tables with quantifiable performance data related to environmental analysis and g...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains tables with quantifiable data related to environmental analysis and green procure...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains tables and text related to social contributions and awards, but it does not inclu...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains tables with information about social contribution activities and awards, but it d...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a table listing issues and responses by Canon over various years. It does not ...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a table, not a chart or graph, and it primarily lists historical events, organ...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a text-based page from a Canon Environmental Report Questionnaire. It does not...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a survey form rather than a chart or graph. It contains questions about the re...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a survey form, not a chart or graph. It contains questions about the readabili...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any visible charts or graphs with quantifiable performance data....\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any visible charts, graphs, or data visualizations with quantifi...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Extraction Summary ===\n",
            "Total KPIs extracted: 566\n",
            "From text/tables: 412\n",
            "From images/charts: 154\n",
            "\n",
            "KPI Distribution by Theme:\n",
            "  Environmental: 508\n",
            "  Social: 27\n",
            "  Economic: 18\n",
            "  Governance: 13\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1IoxrHG3joQz9CaGgenAnsjP3sGaEFA9h",
      "authorship_tag": "ABX9TyPY1HPCNVN5vhrBMRhUHOLE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}