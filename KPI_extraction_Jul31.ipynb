{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esemsc-rw1024/irp-rw1024-public/blob/main/KPI_extraction_Jul31.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "extract_sustainability_kpi.py\n",
        "==================================\n",
        "Automatically extract KPI sentences/table rows from Sustainability Report PDF\n",
        "and compare with manual KPI annotations\n",
        "--------------------------------------------------\n",
        "1. pdfplumber extracts text + tables\n",
        "2. Camelot supplements complex table parsing (optional)\n",
        "3. Chunking to control tokens\n",
        "4. OpenAI ChatCompletion API call (GPT-4o / GPT-4 / GPT-3.5)\n",
        "5. Aggregate, deduplicate, and export to auto_kpi.xlsx\n",
        "6. Compare with manual_kpi.xlsx for differences\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eDB9oRTEZoZw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "645bb6ea-2f68-4bb9-bad5-cae3d1a984e5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nextract_sustainability_kpi.py\\n==================================\\nAutomatically extract KPI sentences/table rows from Sustainability Report PDF\\nand compare with manual KPI annotations\\n--------------------------------------------------\\n1. pdfplumber extracts text + tables\\n2. Camelot supplements complex table parsing (optional)\\n3. Chunking to control tokens\\n4. OpenAI ChatCompletion API call (GPT-4o / GPT-4 / GPT-3.5)\\n5. Aggregate, deduplicate, and export to auto_kpi.xlsx\\n6. Compare with manual_kpi.xlsx for differences\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "id": "LaGnafXLZxfg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "3w2Bya8SZob3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai python-dotenv pdfplumber tiktoken pandas\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install -y ghostscript\n",
        "!pip install \"camelot-py[cv]\"\n",
        "!pip install PyMuPDF Pillow\n",
        "!pip install -q transformers pillow torchvision"
      ],
      "metadata": {
        "id": "JSakj9TyZodt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffc309aa-5954-4ce6-ce4b-a45e60c7b119"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.97.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.7)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,168 kB]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,775 kB]\n",
            "Fetched 11.9 MB in 6s (1,859 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ghostscript is already the newest version (9.55.0~dfsg1-0ubuntu5.12).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n",
            "Requirement already satisfied: camelot-py[cv] in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "\u001b[33mWARNING: camelot-py 1.0.0 does not provide the extra 'cv'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (8.2.1)\n",
            "Requirement already satisfied: chardet>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (5.2.0)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (2.0.2)\n",
            "Requirement already satisfied: openpyxl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (3.1.5)\n",
            "Requirement already satisfied: pdfminer-six>=20240706 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (20250506)\n",
            "Requirement already satisfied: pypdf<4.0,>=3.17 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (3.17.4)\n",
            "Requirement already satisfied: pandas>=2.2.2 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (2.2.2)\n",
            "Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (0.9.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.7.0.68 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (4.12.0.88)\n",
            "Requirement already satisfied: pypdfium2>=4 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (4.30.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl>=3.1.0->camelot-py[cv]) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.2->camelot-py[cv]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.2->camelot-py[cv]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.2->camelot-py[cv]) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer-six>=20240706->camelot-py[cv]) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer-six>=20240706->camelot-py[cv]) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[cv]) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.2->camelot-py[cv]) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[cv]) (2.22)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, time, textwrap, argparse, logging\n",
        "import pdfplumber, pandas as pd, tiktoken\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from typing import List, Dict, Optional, Set, Tuple\n",
        "from pathlib import Path\n",
        "from difflib import SequenceMatcher\n",
        "import base64\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import fitz  # PyMuPDF\n",
        "import numpy as np\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import concurrent.futures\n",
        "import hashlib\n",
        "import pickle\n",
        "# 在现有的导入语句后添加这些新的导入\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "rNQHu6_fZofh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------- Configuration -----------------------------\n",
        "PDF_PATH          = \"/content/Test_Unknown_northwest-sustainability-report-2022_fbqow68f-60-74.pdf\"\n",
        "MANUAL_XLSX       = \"manual_kpi.xlsx\"   # Leave empty if not available\n",
        "EXPORT_AUTO_XLSX  = \"auto_kpi.xlsx\"\n",
        "MODEL_NAME        = \"gpt-4o\"       # Adjust based on account availability\n",
        "MAX_TOKENS_CHUNK  = 1500               # Token limit per chunk\n",
        "SLEEP_SEC         = 0.6                # Rate limiting\n",
        "ENABLE_QUALITY_VALIDATION = True       # Enable additional quality checks\n",
        "# -----------------------------------------------------------------"
      ],
      "metadata": {
        "id": "L9B6ORg1Zohn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Fixed initialization part ============\n",
        "def initialize_environment():\n",
        "    \"\"\"Initialize the environment and API client\"\"\"\n",
        "    # Load environment variables\n",
        "    load_dotenv(\"ruojia_api_key.env\")\n",
        "\n",
        "    # Initialize OpenAI client\n",
        "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"OPENAI_API_KEY not found in environment variables!\")\n",
        "\n",
        "    client = OpenAI(api_key=api_key)\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "    return client, enc\n",
        "\n",
        "# Initialize global variables\n",
        "client, enc = initialize_environment()"
      ],
      "metadata": {
        "id": "mi52QoSbZoja"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Fixed PDF text extraction ============\n",
        "def pdf_to_text_and_tables(path: str) -> str:\n",
        "    \"\"\"Extract text paragraphs and tables using pdfplumber.\"\"\"\n",
        "    all_chunks = []\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"PDF file not found: {path}\")\n",
        "\n",
        "    try:\n",
        "        with pdfplumber.open(path) as pdf:\n",
        "            logging.info(f\"Processing PDF with {len(pdf.pages)} pages...\")\n",
        "\n",
        "            for page_num, page in enumerate(pdf.pages, 1):\n",
        "                try:\n",
        "                    # Extract text\n",
        "                    text = page.extract_text() or \"\"\n",
        "                    if text.strip():\n",
        "                        all_chunks.append(f\"PAGE_{page_num}_TEXT:\\n{text}\")\n",
        "\n",
        "                    # Extract tables\n",
        "                    tables = page.extract_tables()\n",
        "                    for table_num, tb in enumerate(tables):\n",
        "                        if tb and len(tb) > 0:\n",
        "                            try:\n",
        "                                # Handle table headers safely\n",
        "                                if tb[0]:\n",
        "                                    headers = tb[0]\n",
        "                                else:\n",
        "                                    headers = [f\"Col_{i}\" for i in range(len(tb[1]) if len(tb) > 1 else 1)]\n",
        "\n",
        "                                rows = tb[1:] if len(tb) > 1 else []\n",
        "\n",
        "                                if rows:\n",
        "                                    df = pd.DataFrame(rows, columns=headers)\n",
        "                                    # Clean DataFrame\n",
        "                                    df = df.dropna(how='all')  # Remove empty rows\n",
        "                                    if not df.empty:\n",
        "                                        table_txt = f\"TABLE_START_PAGE_{page_num}_{table_num}\\n\" + df.to_csv(index=False) + \"\\nTABLE_END\"\n",
        "                                        all_chunks.append(table_txt)\n",
        "                            except Exception as e:\n",
        "                                logging.warning(f\"Error processing table on page {page_num}: {e}\")\n",
        "                                continue\n",
        "\n",
        "                except Exception as e:\n",
        "                    logging.warning(f\"Error processing page {page_num}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        return \"\\n\\n\".join(all_chunks)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error opening PDF file: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "IkSbr5pqaGsO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Fixed Camelot table extraction ============\n",
        "def generate_table_fingerprint(df: pd.DataFrame) -> str:\n",
        "    \"\"\"Generate table fingerprint for deduplication\"\"\"\n",
        "    try:\n",
        "        fingerprint_parts = []\n",
        "        fingerprint_parts.append(f\"shape_{df.shape[0]}x{df.shape[1]}\")\n",
        "\n",
        "        if not df.columns.empty:\n",
        "            col_names = [str(col).strip().lower().replace(' ', '') for col in df.columns]\n",
        "            col_fingerprint = '_'.join(sorted(col_names))\n",
        "            fingerprint_parts.append(f\"cols_{hash(col_fingerprint)}\")\n",
        "\n",
        "        if df.shape[0] > 0:\n",
        "            numeric_values = []\n",
        "            for col in df.columns:\n",
        "                for val in df[col].head(3):\n",
        "                    if pd.notna(val):\n",
        "                        numbers = re.findall(r'\\d+\\.?\\d*', str(val))\n",
        "                        numeric_values.extend(numbers)\n",
        "\n",
        "            if numeric_values:\n",
        "                numeric_fingerprint = hash('_'.join(sorted(numeric_values[:10])))\n",
        "                fingerprint_parts.append(f\"nums_{numeric_fingerprint}\")\n",
        "\n",
        "        return '_'.join(fingerprint_parts)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error generating table fingerprint: {e}\")\n",
        "        return str(hash(df.to_csv()))\n",
        "\n",
        "def clean_table_data_improved(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Improved table data cleaning\"\"\"\n",
        "    try:\n",
        "        cleaned_df = df.copy()\n",
        "        cleaned_df = cleaned_df.dropna(how='all')\n",
        "        cleaned_df = cleaned_df.dropna(axis=1, how='all')\n",
        "\n",
        "        for col in cleaned_df.columns:\n",
        "            if cleaned_df[col].dtype == 'object':\n",
        "                cleaned_df[col] = cleaned_df[col].astype(str).str.strip()\n",
        "                cleaned_df[col] = cleaned_df[col].replace(['nan', 'NaN', 'None'], '')\n",
        "\n",
        "        if not cleaned_df.empty:\n",
        "            new_columns = []\n",
        "            for i, col in enumerate(cleaned_df.columns):\n",
        "                col_str = str(col).strip()\n",
        "                if col_str in ['nan', 'NaN', 'None', ''] or pd.isna(col):\n",
        "                    new_columns.append(f'Column_{i}')\n",
        "                else:\n",
        "                    new_columns.append(col_str)\n",
        "            cleaned_df.columns = new_columns\n",
        "\n",
        "        cleaned_df = cleaned_df.reset_index(drop=True)\n",
        "        return cleaned_df\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error in table cleaning: {e}\")\n",
        "        return df\n",
        "\n",
        "def is_valid_table_improved(df: pd.DataFrame) -> bool:\n",
        "    \"\"\"Improved table validation\"\"\"\n",
        "    try:\n",
        "        if df.empty or df.shape[0] < 1 or df.shape[1] < 1:\n",
        "            return False\n",
        "\n",
        "        non_null_cells = 0\n",
        "        total_cells = df.shape[0] * df.shape[1]\n",
        "\n",
        "        for col in df.columns:\n",
        "            for val in df[col]:\n",
        "                if pd.notna(val) and str(val).strip() not in ['', 'nan', 'NaN', 'None']:\n",
        "                    non_null_cells += 1\n",
        "\n",
        "        if non_null_cells / total_cells < 0.2:\n",
        "            return False\n",
        "\n",
        "        has_meaningful_content = False\n",
        "        for col in df.columns:\n",
        "            text_content = ' '.join(df[col].dropna().astype(str))\n",
        "            if (any(char.isdigit() for char in text_content) or\n",
        "                '%' in text_content or\n",
        "                any(keyword in text_content.lower() for keyword in [\n",
        "                    'rate', 'percentage', 'total', 'number', 'emission', 'energy',\n",
        "                    'water', 'waste', 'employee', 'year', '2020', '2021', '2022', '2023'\n",
        "                ])):\n",
        "                has_meaningful_content = True\n",
        "                break\n",
        "\n",
        "        return has_meaningful_content\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error validating table: {e}\")\n",
        "        return True\n",
        "\n",
        "def format_table_output_improved(df: pd.DataFrame, table_id: str, parsing_report=None) -> str:\n",
        "    \"\"\"Improved table output formatting\"\"\"\n",
        "    try:\n",
        "        table_info = f\"TABLE_START_{table_id}\\n\"\n",
        "        table_info += f\"DIMENSIONS: {df.shape[0]} rows × {df.shape[1]} columns\\n\"\n",
        "\n",
        "        col_info = \"COLUMNS: \" + \" | \".join([f\"{i}:{col}\" for i, col in enumerate(df.columns)])\n",
        "        table_info += col_info + \"\\n\"\n",
        "\n",
        "        if df.shape[0] > 0:\n",
        "            preview_rows = min(2, df.shape[0])\n",
        "            table_info += f\"PREVIEW_FIRST_{preview_rows}_ROWS:\\n\"\n",
        "            for i in range(preview_rows):\n",
        "                row_preview = \" | \".join([str(df.iloc[i, j])[:20] for j in range(min(5, df.shape[1]))])\n",
        "                table_info += f\"  Row_{i}: {row_preview}\\n\"\n",
        "\n",
        "        if parsing_report:\n",
        "            try:\n",
        "                accuracy = getattr(parsing_report, 'accuracy', 'N/A')\n",
        "                if accuracy != 'N/A':\n",
        "                    table_info += f\"EXTRACTION_ACCURACY: {accuracy:.2f}\\n\"\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        table_info += \"TABLE_DATA_START\\n\"\n",
        "        table_csv = df.to_csv(index=False, na_rep='', quoting=1, escapechar='\\\\')\n",
        "        table_end = f\"TABLE_DATA_END\\nTABLE_END_{table_id}\\n\"\n",
        "\n",
        "        return table_info + table_csv + table_end\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error formatting table output: {e}\")\n",
        "        return f\"TABLE_START_{table_id}\\n{df.to_csv(index=False)}\\nTABLE_END_{table_id}\\n\"\n",
        "\n",
        "def camelot_extra_tables_enhanced(path: str) -> List[str]:\n",
        "    \"\"\"Enhanced table extraction using Camelot with better error handling\"\"\"\n",
        "    try:\n",
        "        import camelot\n",
        "    except ImportError:\n",
        "        logging.warning(\"Camelot not installed, skipping Camelot table parsing.\")\n",
        "        return []\n",
        "\n",
        "    extra_chunks = []\n",
        "    extracted_tables_fingerprints = set()\n",
        "\n",
        "    try:\n",
        "        logging.info(\"Starting Camelot table extraction...\")\n",
        "\n",
        "        # Stream mode extraction\n",
        "        try:\n",
        "            stream_tables = camelot.read_pdf(\n",
        "                path,\n",
        "                pages=\"all\",\n",
        "                flavor=\"stream\",\n",
        "                edge_tol=50,\n",
        "                row_tol=2,\n",
        "                column_tol=0\n",
        "            )\n",
        "\n",
        "            stream_count = 0\n",
        "            for i, table in enumerate(stream_tables):\n",
        "                if not table.df.empty and table.df.shape[0] > 0:\n",
        "                    table_fingerprint = generate_table_fingerprint(table.df)\n",
        "\n",
        "                    if table_fingerprint not in extracted_tables_fingerprints:\n",
        "                        cleaned_df = clean_table_data_improved(table.df)\n",
        "\n",
        "                        if is_valid_table_improved(cleaned_df):\n",
        "                            table_txt = format_table_output_improved(cleaned_df, f\"STREAM_{i}\", table.parsing_report)\n",
        "                            extra_chunks.append(table_txt)\n",
        "                            extracted_tables_fingerprints.add(table_fingerprint)\n",
        "                            stream_count += 1\n",
        "\n",
        "            logging.info(f\"Stream mode extracted {stream_count} valid tables\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Stream mode extraction failed: {e}\")\n",
        "\n",
        "        # Lattice mode extraction\n",
        "        try:\n",
        "            lattice_tables = camelot.read_pdf(\n",
        "                path,\n",
        "                pages=\"all\",\n",
        "                flavor=\"lattice\",\n",
        "                line_scale=15,\n",
        "                line_tol=2,\n",
        "                joint_tol=2\n",
        "            )\n",
        "\n",
        "            lattice_count = 0\n",
        "            for i, table in enumerate(lattice_tables):\n",
        "                if not table.df.empty and table.df.shape[0] > 0:\n",
        "                    table_fingerprint = generate_table_fingerprint(table.df)\n",
        "\n",
        "                    if table_fingerprint not in extracted_tables_fingerprints:\n",
        "                        cleaned_df = clean_table_data_improved(table.df)\n",
        "\n",
        "                        if is_valid_table_improved(cleaned_df):\n",
        "                            table_txt = format_table_output_improved(cleaned_df, f\"LATTICE_{i}\", table.parsing_report)\n",
        "                            extra_chunks.append(table_txt)\n",
        "                            extracted_tables_fingerprints.add(table_fingerprint)\n",
        "                            lattice_count += 1\n",
        "\n",
        "            logging.info(f\"Lattice mode extracted {lattice_count} additional unique tables\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Lattice mode extraction failed: {e}\")\n",
        "\n",
        "        total_extracted = len(extra_chunks)\n",
        "        logging.info(f\"Camelot extraction completed: {total_extracted} total unique tables extracted\")\n",
        "        return extra_chunks\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Camelot table extraction failed: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "amKWHBAHaGuk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Text Chunking ============\n",
        "def split_into_chunks(full_text: str, max_tokens: int) -> List[str]:\n",
        "    \"\"\"Split text into chunks based on token limit\"\"\"\n",
        "    paragraphs = [p for p in full_text.split(\"\\n\") if p.strip()]\n",
        "    chunks, current = [], []\n",
        "    current_tokens = 0\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        para_tokens = len(enc.encode(paragraph))\n",
        "\n",
        "        if current_tokens + para_tokens > max_tokens and current:\n",
        "            chunks.append(\"\\n\".join(current))\n",
        "            current = [paragraph]\n",
        "            current_tokens = para_tokens\n",
        "        else:\n",
        "            current.append(paragraph)\n",
        "            current_tokens += para_tokens\n",
        "\n",
        "    if current:\n",
        "        chunks.append(\"\\n\".join(current))\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "ZSLAWykuaGxs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ System prompt words ============\n",
        "UNIVERSAL_SYSTEM_PROMPT = textwrap.dedent(\"\"\"\n",
        "    You are a professional ESG data analyst specializing in extracting Key Performance Indicators (KPIs) from sustainability reports.\n",
        "\n",
        "    ## CRITICAL: What is a KPI?\n",
        "    A KPI MUST contain SPECIFIC NUMBERS, PERCENTAGES, or MEASURABLE QUANTITIES that demonstrate actual performance or concrete targets.\n",
        "\n",
        "    ## IMPORTANT: Table Data Processing Rules\n",
        "    When processing table data:\n",
        "    1. Pay close attention to column headers to identify the correct time periods\n",
        "    2. Match data values with their corresponding year columns\n",
        "    3. If you see table format like \"Metric, 2021, 2022\" - the first number after metric belongs to 2021, second to 2022\n",
        "    4. Look for table headers that indicate year columns (e.g., \"2020\", \"2021\", \"2022\")\n",
        "    5. Extract each year's data as separate KPIs\n",
        "    6. Avoid extracting the same KPI multiple times - consolidate similar metrics\n",
        "\n",
        "    ## ENHANCED: Advanced Table Processing\n",
        "    7. **EXTRACT ALL DATA POINTS**: For each table cell containing a number, create a separate KPI\n",
        "    8. **REGIONAL/LOCATION DATA**: Pay special attention to location-specific data (countries, regions, cities)\n",
        "    9. **WORKFORCE DATA**: Extract all employee numbers, headcount data, and demographic information\n",
        "    10. **INCOMPLETE DATA**: Extract available data even if some cells are empty or missing\n",
        "    11. **TOTALS AND SUBTOTALS**: Always extract total values and aggregated numbers\n",
        "\n",
        "    ## ✅ VALID KPI EXAMPLES:\n",
        "    - \"Achieved 89.4% reuse and recycle rate for cloud hardware in 2023\"\n",
        "    - \"Diverted over 18,537 metric tons of waste from landfills in 2023\"\n",
        "    - \"Reduced single-use plastics in product packaging to 2.7%\"\n",
        "    - \"Contracted 19 GW of new renewable energy across 16 countries in 2024\"\n",
        "    - \"Provided clean water access to over 1.5 million people in 2023\"\n",
        "    - \"Protected 15,849 acres of land—exceeding target by more than 30%\"\n",
        "    - \"Allocated 761 million toward innovative climate technologies\"\n",
        "    - \"Achieved 80% renewable energy operations by 2024\"\n",
        "    - \"Water replenishment projects estimated to provide over 25 million cubic meters\"\n",
        "    - \"Exceeded annual target to divert 75% of construction waste by reaching 85%\"\n",
        "    - \"Board independence: 78% of directors\"\n",
        "    - \"Women in senior leadership increased to 35% in 2023\"\n",
        "    - \"Employee engagement score: 87% in annual survey\"\n",
        "    - \"Reduced greenhouse gas emissions by 50% compared to 2019 baseline\"\n",
        "    - \"Zero workplace fatalities achieved for third consecutive year\"\n",
        "    - \"Training completion rate: 98% for mandatory compliance courses\"\n",
        "    - \"Supplier ESG assessments completed for 95% of tier-1 suppliers\"\n",
        "    - \"Customer satisfaction rating: 4.6 out of 5.0\"\n",
        "    - \"Data breach incidents: 0 material breaches in 2023\"\n",
        "\n",
        "    ## ❌ NOT KPIs (DO NOT EXTRACT):\n",
        "    - \"Microsoft will require select suppliers to use carbon-free electricity by 2030\"\n",
        "    - \"The company plans to expand Sustainability Manager capabilities\"\n",
        "    - \"We are launching two new Circular Centers in 2023\"\n",
        "    - \"The organization established a new climate innovation fund\"\n",
        "    - \"Microsoft introduced enhanced data governance solutions\"\n",
        "    - \"Updated guidebook to include guidance on corporate responsibility\"\n",
        "    - \"Plans to publish new ESG strategy\"\n",
        "    - \"Implemented a new recycling program\"\n",
        "    - \"Conducted sustainability training sessions\"\n",
        "    - \"Launched employee wellness programs\"\n",
        "    - \"Committed to reducing emissions\"\n",
        "    - \"Focusing on environmental performance\"\n",
        "    - \"Established sustainability committee\"\n",
        "    - \"The company operates facilities in multiple regions\"\n",
        "    - \"Our supply chain includes thousands of vendors globally\"\n",
        "    - Any text without specific numbers, percentages, or quantifiable metrics\n",
        "    - Duplicate or repeated metrics (extract only once per time period)\n",
        "    - Any statement that describes business operations rather than performance outcomes\n",
        "\n",
        "    ## KPI Categories:\n",
        "    ### Environmental:\n",
        "    - **Carbon_Climate**: GHG emissions, carbon footprint, emission reductions, climate targets, scope 1/2/3 emissions, carbon intensity, carbon offsets, TCFD alignment\n",
        "    - **Energy**: Energy consumption, renewable energy percentage, energy efficiency, energy intensity, MWh, GWh, energy savings, fossil fuel usage\n",
        "    - **Water**: Water withdrawal, water consumption, water intensity, water recycling, water reuse, water stress, water discharge quality\n",
        "    - **Waste**: Waste generation, recycling rates, diversion percentages, hazardous waste, non-hazardous waste, zero waste to landfill, e-waste, incineration\n",
        "    - **Biodiversity**: Protected areas, species conservation, habitat restoration, biodiversity impact assessments, land use, ecosystem restoration\n",
        "    - **Circular_Economy**: Recycling rates, material recovery, circular design, raw materials usage, renewable materials, packaging waste\n",
        "    - **Materials**: Raw materials consumption, recycled content, sustainable materials, material intensity, sustainable sourcing\n",
        "\n",
        "    ### Social:\n",
        "    - **Workforce_Diversity**: Employee demographics, gender diversity, age diversity, ethnic diversity, disability inclusion, LGBTQ+ inclusion, workforce composition\n",
        "    - **Gender_Equality**: Women in leadership, gender pay ratio, parental leave return rates, gender representation, female employees percentage\n",
        "    - **Disability_Inclusion**: Employees with disabilities, accessibility compliance, inclusive workplace design, disability support programs\n",
        "    - **Health_Safety**: Lost Time Injury Frequency Rate (LTIFR), Total Recordable Incident Rate (TRIR), fatalities, workplace illness, safety training hours, PPE compliance, emergency drills\n",
        "    - **Employee_Wellbeing**: Employee satisfaction, retention rates, turnover rates, training hours, wellness programs, mental health services, work-life balance\n",
        "    - **Community_Engagement**: Corporate volunteering, social investment, community impact assessments, local hiring, stakeholder engagement activities\n",
        "    - **Human_Rights**: Child labor incidents, forced labor, human rights due diligence, freedom of association, grievance mechanisms, labor audits\n",
        "    - **Labor_Rights**: Collective bargaining coverage, labor complaints resolution, supplier labor audits, working conditions, fair wages\n",
        "    - **Customer_Safety**: Product safety incidents, customer satisfaction, accessibility features, safety recalls, quality metrics\n",
        "    - **Supply_Chain_Social**: Supplier assessments, sustainable sourcing, supplier code compliance, supply chain audits\n",
        "\n",
        "    ### Governance:\n",
        "    - **Board_Governance**: Board independence, board diversity, CEO-chair separation, board ESG expertise, board composition, director tenure\n",
        "    - **Executive_Compensation**: ESG-linked compensation, executive pay ratios, compensation disclosure, incentive structures\n",
        "    - **Ethics_Compliance**: Code of conduct training, corruption incidents, bribery cases, fines and penalties, whistleblower reports, anti-corruption assessments\n",
        "    - **Transparency_Disclosure**: ESG reporting coverage, third-party assurance, political contributions disclosure, GRI/SASB/TCFD compliance\n",
        "    - **Risk_Management**: Risk assessments, mitigation measures, climate risk disclosure, operational risk management\n",
        "    - **Cybersecurity_Data**: Cybersecurity breaches, data privacy policies, cybersecurity training, GDPR compliance, data protection measures\n",
        "    - **Supply_Chain_Governance**: Supplier ESG screening, supplier audits, procurement ESG clauses, vendor compliance rates\n",
        "\n",
        "    ## MANDATORY Requirements:\n",
        "    1. MUST contain specific numbers (e.g., 25%, 15,000, 2.5M, 8.5%, 0.3 per million hours)\n",
        "    2. MUST relate to measurable sustainability outcomes\n",
        "    3. MUST have time reference (year, period, or deadline)\n",
        "    4. MUST be performance-focused (results, not activities or descriptions)\n",
        "    5. MUST NOT be future plans or operational descriptions\n",
        "\n",
        "    ## Output Format:\n",
        "    Return a JSON array. Each KPI must contain:\n",
        "    ```json\n",
        "    {\n",
        "        \"kpi_text\": \"Complete original sentence with the quantifiable metric\",\n",
        "        \"kpi_theme\": \"Environmental/Social/Governance\",\n",
        "        \"kpi_category\": \"Specific category from above list\",\n",
        "        \"quantitative_value\": \"The specific number/percentage extracted\",\n",
        "        \"unit\": \"Unit of measurement (%, tonnes, employees, etc.)\",\n",
        "        \"time_period\": \"Time reference (2023, annual, by 2030, etc.)\",\n",
        "        \"target_or_actual\": \"Target/Actual/Both\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "    ## Additional Instructions:\n",
        "    - If a sentence includes a comparison value, such as a baseline, previous year, or other historical/target data (e.g., \"Compared to 32,395 MWh in 2020\"), extract it as a **separate KPI**.\n",
        "    - Do NOT store the comparison in any other field — just create another valid KPI from it.\n",
        "    - Avoid merging multiple numerical values into one KPI unless they are clearly part of the same metric (e.g., male: X, female: Y).\n",
        "\n",
        "    ## STRICT FILTERING:\n",
        "    - Return empty array [] if no quantifiable KPIs found\n",
        "    - Only extract text that contains specific measurable values\n",
        "    - Ignore all qualitative statements, plans, and descriptions\n",
        "    - Focus only on numerical performance data\n",
        "\n",
        "    Now analyze the following text for sustainability KPIs:\n",
        "\"\"\").strip()\n",
        "\n",
        "# 🔥 新增：增强的图像分析Prompt\n",
        "ENHANCED_IMAGE_KPI_SYSTEM_PROMPT = textwrap.dedent(\"\"\"\n",
        "    You are an expert data analyst specializing in extracting quantifiable KPI data from charts, graphs, and data visualizations in sustainability reports.\n",
        "\n",
        "    ## CRITICAL INSTRUCTION: ALWAYS EXTRACT NUMERICAL VALUES\n",
        "\n",
        "    **Your primary task is to extract the ACTUAL NUMBERS and PERCENTAGES visible in charts, not just descriptions.**\n",
        "\n",
        "    ## MISSION:\n",
        "    Extract ALL quantifiable data points from charts and graphs, including:\n",
        "    - Bar charts (vertical/horizontal)\n",
        "    - Pie charts and donut charts\n",
        "    - Line charts and trend graphs\n",
        "    - Stacked charts and combo charts\n",
        "    - Tables with numerical data\n",
        "    - Infographics with statistics\n",
        "    - Gauge charts and dashboards\n",
        "\n",
        "    ## DETAILED ANALYSIS INSTRUCTIONS:\n",
        "\n",
        "    ### For PIE CHARTS:\n",
        "    1. Read percentage labels on each slice\n",
        "    2. If no labels visible, estimate based on slice size\n",
        "    3. Identify what each slice represents (categories)\n",
        "    4. Extract each slice as separate KPI\n",
        "    5. **MUST read the percentage labels on each slice** - Look for numbers like 64%, 33%, 68%, 30%, etc.\n",
        "    6. **If percentages are visible on the chart, extract them exactly**\n",
        "    7. **If no labels visible, estimate based on slice size using these guidelines:**\n",
        "       - 90° slice = 25%\n",
        "       - 180° slice = 50%\n",
        "       - 270° slice = 75%\n",
        "       - Full circle = 100%\n",
        "    8. **Each slice MUST have a specific percentage value in the final output**\n",
        "\n",
        "    ### For BAR CHARTS:\n",
        "    1. Read Y-axis scale carefully (units, increments)\n",
        "    2. Estimate bar heights using grid lines and scale\n",
        "    3. Read X-axis labels (years, categories, regions)\n",
        "    4. Extract each bar as separate KPI\n",
        "    5. Pay attention to grouped/stacked bars\n",
        "\n",
        "    ### For LINE CHARTS:\n",
        "    1. Read data points at intersection of grid lines\n",
        "    2. Follow trend lines to extract values for each time period\n",
        "    3. Use Y-axis scale for value estimation\n",
        "    4. Extract each data point as separate KPI\n",
        "\n",
        "    ### For TABLES:\n",
        "    1. Read all numerical values in cells\n",
        "    2. Match values with row and column headers\n",
        "    3. Extract each cell with numerical data as KPI\n",
        "\n",
        "    ## MANDATORY VALUE EXTRACTION RULES:\n",
        "\n",
        "    **RULE 1**: Every KPI MUST contain a specific numerical value (percentage, amount, count, etc.)\n",
        "    **RULE 2**: For charts with categories, you MUST find and extract the quantitative values for each category\n",
        "    **RULE 3**: Never create KPIs without specific numbers - descriptions alone are incomplete\n",
        "    **RULE 4**: Include complete context: what + how much + when/where if available\n",
        "\n",
        "\n",
        "    ## VALUE ESTIMATION GUIDELINES:\n",
        "    - Use proportional analysis: if a bar reaches 80% of scale maximum, calculate 80% of max value\n",
        "    - For pie charts: estimate slice angles (90° = 25%, 180° = 50%, etc.)\n",
        "    - Cross-reference with any visible data labels or legends\n",
        "    - Be conservative but reasonably accurate in estimates\n",
        "\n",
        "    ## CHART IDENTIFICATION:\n",
        "    First identify the chart type, then apply appropriate extraction method.\n",
        "    Look for:\n",
        "    - Axes and scales\n",
        "    - Data labels and legends\n",
        "    - Grid lines for reference\n",
        "    - Color coding and patterns\n",
        "    - Title and subtitle information\n",
        "\n",
        "    ## OUTPUT FORMAT:\n",
        "    Return a JSON array. For each data point found:\n",
        "    ```json\n",
        "    {\n",
        "        \"kpi_text\": \"Complete description with the ACTUAL NUMERICAL VALUE included\",\n",
        "        \"kpi_theme\": \"Environmental/Social/Governance\",\n",
        "        \"kpi_category\": \"Specific category based on content\",\n",
        "        \"quantitative_value\": \"The exact number/percentage (e.g., '64', '33.5', '68')\",\n",
        "        \"unit\": \"% / tonnes / employees / MWh / USD / etc.\",\n",
        "        \"time_period\": \"2021/2020/2022/Year/period/etc if identifiable\",\n",
        "        \"target_or_actual\": \"Actual\",\n",
        "        \"chart_type\": \"pie_chart/bar_chart/line_chart/table/etc\",\n",
        "        \"estimation_confidence\": \"High/Medium/Low\",\n",
        "        \"chart_title\": \"Chart title if visible\",\n",
        "        \"data_source\": \"Legend or source if visible\"\n",
        "    }\n",
        "\n",
        "    ```\n",
        "    ## EXAMPLES of CORRECT vs INCORRECT extraction:\n",
        "\n",
        "    ### ❌ INCORRECT (incomplete - missing numerical values):\n",
        "    ```json\n",
        "    {\n",
        "        \"kpi_text\": \"Energy consumption by facility type\",\n",
        "        \"quantitative_value\": \"\",\n",
        "        \"unit\": \"%\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "    ### ✅ CORRECT (complete with specific values):\n",
        "    ```json\n",
        "    {\n",
        "        \"kpi_text\": \"Office buildings account for 45% of total energy consumption\",\n",
        "        \"quantitative_value\": \"45\",\n",
        "        \"unit\": \"%\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "    ### ❌ INCORRECT (category without value):\n",
        "    ```json\n",
        "    {\n",
        "        \"kpi_text\": \"Renewable energy percentage by region\",\n",
        "        \"quantitative_value\": \"\",\n",
        "        \"unit\": \"%\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "    ### ✅ CORRECT (specific regional data):\n",
        "    ```json\n",
        "    {\n",
        "        \"kpi_text\": \"North America achieved 78% renewable energy usage\",\n",
        "        \"quantitative_value\": \"78\",\n",
        "        \"unit\": \"%\"\n",
        "    }\n",
        "    ```\n",
        "    ## QUALITY ASSURANCE CHECKLIST:\n",
        "    Before returning results, verify:\n",
        "    - ✅ Every KPI contains a specific numerical value\n",
        "    - ✅ Chart categories are paired with their quantitative data\n",
        "    - ✅ KPI descriptions are complete and self-explanatory\n",
        "    - ✅ Units are correctly identified and specified\n",
        "    - ✅ Context (time, location, category) is preserved when available\n",
        "    - Each KPI must have a specific numerical value\n",
        "    - Context must be clear and self-contained\n",
        "    - Avoid extracting the same data point multiple times\n",
        "    - Focus on sustainability/ESG metrics when possible\n",
        "\n",
        "    ## VALUE ESTIMATION GUIDELINES:\n",
        "    - **High confidence**: Numbers clearly visible in image\n",
        "    - **Medium confidence**: Numbers estimated using chart scales/grid lines\n",
        "    - **Low confidence**: Values approximated from proportional analysis\n",
        "    - **If no numerical data is visible, return empty array []**\n",
        "\n",
        "    ## IMPORTANT NOTES:\n",
        "    - Extract ALL visible data points, not just main highlights\n",
        "    - Include context in descriptions (e.g., \"According to pie chart showing emission sources\")\n",
        "    - If values are not clearly visible, make reasonable estimates and mark confidence as \"Low\"\n",
        "    - Return empty array [] ONLY if image contains no charts/graphs with quantifiable data\n",
        "    - For multi-year data, create separate KPIs for each year\n",
        "    - Pay special attention to small text and numbers\n",
        "    - Focus on extracting actual performance data, not just identifying chart elements\n",
        "    - If you can see numbers in the image, you MUST extract them\n",
        "    - Pie chart percentages are usually the most important data points\n",
        "    - Return empty array [] ONLY if no numerical data is visible\n",
        "\n",
        "    Now analyze the provided image and extract ALL quantifiable KPI data points:\n",
        "\"\"\").strip()"
      ],
      "metadata": {
        "id": "IonQ4PK2aGzy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ KPI Extraction Function ============\n",
        "def extract_page_from_chunk(chunk: str) -> str:\n",
        "    \"\"\"Extract page information from chunk\"\"\"\n",
        "    # Look for PAGE_X_TEXT: format\n",
        "    page_matches = re.findall(r'PAGE_(\\d+)_TEXT:', chunk)\n",
        "    if page_matches:\n",
        "        pages = [int(p) for p in page_matches]\n",
        "        if len(pages) == 1:\n",
        "            return str(pages[0])\n",
        "        else:\n",
        "            return f\"{min(pages)}-{max(pages)}\"\n",
        "\n",
        "    # Look for TABLE_START_PAGE_X_\n",
        "    table_matches = re.findall(r'TABLE_START_PAGE_(\\d+)_', chunk)\n",
        "    if table_matches:\n",
        "        pages = [int(p) for p in table_matches]\n",
        "        if len(pages) == 1:\n",
        "            return str(pages[0])\n",
        "        else:\n",
        "            return f\"{min(pages)}-{max(pages)}\"\n",
        "\n",
        "    return \"Unknown\"\n",
        "\n",
        "def contains_procedural_language(text: str) -> bool:\n",
        "    \"\"\"Check if text contains procedural language\"\"\"\n",
        "    procedural_words = [\n",
        "        'introduced', 'established', 'set up', 'implemented', 'created',\n",
        "        'launched', 'formed', 'built', 'installed', 'deployed',\n",
        "        'additionally introduced', 'procedure for', 'standardization management'\n",
        "    ]\n",
        "    text_lower = text.lower()\n",
        "    return any(word in text_lower for word in procedural_words)\n",
        "\n",
        "def is_data_fragment(kpi_text: str) -> bool:\n",
        "    \"\"\"Check if text is a meaningless data fragment\"\"\"\n",
        "    text = kpi_text.strip()\n",
        "\n",
        "    # Filter pure numbers or simple percentages without context\n",
        "    if re.match(r'^\\d+\\.?\\d*%?$', text):\n",
        "        return True\n",
        "\n",
        "    # Filter very short text (less than 4 meaningful words)\n",
        "    meaningful_words = [word for word in text.split() if len(word) > 2 and not word.isdigit()]\n",
        "    if len(meaningful_words) < 3:\n",
        "        return True\n",
        "\n",
        "    # Filter text with only numbers and common connecting words\n",
        "    words = text.lower().split()\n",
        "    non_functional_words = [word for word in words if word not in ['in', 'of', 'the', 'and', 'or', 'to', 'for', 'with', 'by']]\n",
        "    if len(non_functional_words) < 3:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def standardize_kpi_universal(kpi_item: Dict) -> Dict:\n",
        "    \"\"\"Universal KPI data standardization\"\"\"\n",
        "    standardized = kpi_item.copy()\n",
        "\n",
        "    # Standardize numerical formats\n",
        "    quantitative_value = str(standardized.get('quantitative_value', '')).strip()\n",
        "    kpi_text = standardized.get('kpi_text', '').lower()\n",
        "\n",
        "    # Smart handling of percentage formats\n",
        "    if quantitative_value and quantitative_value.replace('.', '').replace('-', '').replace(',', '').isdigit():\n",
        "        # Check if original text suggests this is a percentage\n",
        "        percentage_indicators = ['percent', 'percentage', '%', 'rate', 'ratio', 'proportion', 'share']\n",
        "        if any(indicator in kpi_text for indicator in percentage_indicators):\n",
        "            if not quantitative_value.endswith('%'):\n",
        "                standardized['quantitative_value'] = quantitative_value + '%'\n",
        "                if not standardized.get('unit'):\n",
        "                    standardized['unit'] = '%'\n",
        "\n",
        "    # Ensure unit field consistency\n",
        "    if '%' in str(standardized.get('quantitative_value', '')):\n",
        "        standardized['unit'] = '%'\n",
        "\n",
        "    # Clean and normalize KPI text\n",
        "    kpi_text_original = standardized.get('kpi_text', '').strip()\n",
        "    # Remove extra spaces and newlines\n",
        "    kpi_text_cleaned = ' '.join(kpi_text_original.split())\n",
        "    standardized['kpi_text'] = kpi_text_cleaned\n",
        "\n",
        "    return standardized\n",
        "\n",
        "def generate_universal_metric_key(kpi_item: Dict) -> str:\n",
        "    \"\"\"Generate universal metric key for deduplication\"\"\"\n",
        "    try:\n",
        "        # Extract core elements\n",
        "        category = kpi_item.get('kpi_category', '').lower().strip()\n",
        "        value = str(kpi_item.get('quantitative_value', '')).replace('%', '').replace(',', '').strip()\n",
        "        time_period = kpi_item.get('time_period', '').lower().strip()\n",
        "        unit = kpi_item.get('unit', '').lower().strip()\n",
        "\n",
        "        # Extract key semantic information from KPI text\n",
        "        kpi_text = kpi_item.get('kpi_text', '').lower()\n",
        "\n",
        "        # Extract primary number (for more precise matching)\n",
        "        numbers_in_text = re.findall(r'\\d+\\.?\\d*', kpi_text)\n",
        "        primary_number = numbers_in_text[0] if numbers_in_text else value\n",
        "\n",
        "        # Generate semantic signature: extract keywords from text\n",
        "        # Remove common stop words\n",
        "        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'}\n",
        "\n",
        "        # Extract keywords (length>2 and not stop words)\n",
        "        words = re.findall(r'\\b\\w+\\b', kpi_text)\n",
        "        key_words = [word for word in words if len(word) > 2 and word not in stop_words and not word.isdigit()]\n",
        "\n",
        "        # Sort keywords to ensure consistency\n",
        "        key_words = sorted(set(key_words))[:5]  # Take at most 5 keywords\n",
        "        semantic_signature = '_'.join(key_words)\n",
        "\n",
        "        # Build universal metric key\n",
        "        key_components = []\n",
        "\n",
        "        if category:\n",
        "            key_components.append(f\"cat:{category}\")\n",
        "        if primary_number:\n",
        "            key_components.append(f\"val:{primary_number}\")\n",
        "        if time_period:\n",
        "            key_components.append(f\"time:{time_period}\")\n",
        "        if unit:\n",
        "            key_components.append(f\"unit:{unit}\")\n",
        "        if semantic_signature:\n",
        "            key_components.append(f\"sem:{semantic_signature}\")\n",
        "\n",
        "        # Generate final key\n",
        "        metric_key = \"|\".join(key_components)\n",
        "\n",
        "        # If all components are empty, use text hash\n",
        "        if not metric_key:\n",
        "            metric_key = f\"hash:{hash(kpi_text)}\"\n",
        "\n",
        "        return metric_key\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error generating universal metric key: {e}\")\n",
        "        # Fallback to text hash\n",
        "        return f\"fallback:{hash(kpi_item.get('kpi_text', ''))}\"\n",
        "\n",
        "def extract_kpi_from_chunk_universal(chunk: str) -> List[Dict]:\n",
        "    \"\"\"Universal KPI extraction function for various sustainability reports\"\"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=MODEL_NAME,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": UNIVERSAL_SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": f\"\"\"Extract ALL KPIs from this text. Requirements:\n",
        "\n",
        "1. Create COMPLETE, MEANINGFUL KPI descriptions with full context\n",
        "2. DO NOT extract standalone numbers without explanatory text\n",
        "3. Include all relevant context (time, location, metric type, etc.)\n",
        "4. Use consistent formatting for similar metrics\n",
        "5. Ensure each KPI is self-explanatory\n",
        "\n",
        "Text to analyze:\n",
        "{chunk}\"\"\"}\n",
        "            ],\n",
        "            temperature=0.0,\n",
        "            max_tokens=4000,\n",
        "            timeout=60\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Clean potential markdown formatting\n",
        "        if content.startswith('```json'):\n",
        "            content = content[7:]\n",
        "        if content.endswith('```'):\n",
        "            content = content[:-3]\n",
        "\n",
        "        if not content.strip().startswith(\"[\"):\n",
        "            logging.warning(f\"API response not JSON list: {content[:100]}...\")\n",
        "            return []\n",
        "\n",
        "        result = json.loads(content)\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            logging.warning(\"API response is not a list format\")\n",
        "            return []\n",
        "\n",
        "        # Extract page information\n",
        "        page_number = extract_page_from_chunk(chunk)\n",
        "\n",
        "        # Universal validation and deduplication logic\n",
        "        validated_result = []\n",
        "        seen_metrics = set()\n",
        "\n",
        "        for item in result:\n",
        "            if isinstance(item, dict) and 'kpi_text' in item and 'kpi_theme' in item:\n",
        "                if item['kpi_text'].strip() and item['kpi_theme'].strip():\n",
        "\n",
        "                    # Check procedural language\n",
        "                    if contains_procedural_language(item['kpi_text']):\n",
        "                        logging.debug(f\"Procedural statement filtered: {item['kpi_text'][:50]}...\")\n",
        "                        continue\n",
        "\n",
        "                    # Filter meaningless data fragments\n",
        "                    if is_data_fragment(item['kpi_text']):\n",
        "                        logging.debug(f\"Data fragment filtered: {item['kpi_text']}\")\n",
        "                        continue\n",
        "\n",
        "                    # Standardize KPI data\n",
        "                    standardized_item = standardize_kpi_universal(item)\n",
        "\n",
        "                    # Add page information\n",
        "                    standardized_item['source_page'] = page_number\n",
        "                    standardized_item['source_type'] = 'text'\n",
        "\n",
        "                    # Universal deduplication mechanism\n",
        "                    metric_key = generate_universal_metric_key(standardized_item)\n",
        "\n",
        "                    if metric_key not in seen_metrics:\n",
        "                        validated_result.append(standardized_item)\n",
        "                        seen_metrics.add(metric_key)\n",
        "                        logging.debug(f\"KPI extracted: {standardized_item['kpi_text'][:80]}...\")\n",
        "                    else:\n",
        "                        logging.debug(f\"Duplicate metric filtered: {standardized_item['kpi_text'][:50]}...\")\n",
        "\n",
        "        logging.info(f\"Chunk processed: {len(validated_result)} unique KPIs extracted\")\n",
        "        return validated_result\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        logging.warning(f\"JSON parsing failed: {e}\\nContent: {content[:300]}...\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logging.error(f\"API call failed: {e}\")\n",
        "        return []\n",
        "\n",
        "def post_process_kpis_universal(kpis: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"Universal KPI post-processing for various report types\"\"\"\n",
        "    if not kpis:\n",
        "        return kpis\n",
        "\n",
        "    # Step 1: Deduplication based on metric keys\n",
        "    unique_kpis_dict = {}\n",
        "\n",
        "    for kpi in kpis:\n",
        "        metric_key = generate_universal_metric_key(kpi)\n",
        "\n",
        "        if metric_key not in unique_kpis_dict:\n",
        "            unique_kpis_dict[metric_key] = kpi\n",
        "        else:\n",
        "            # If duplicate, keep the more complete KPI description\n",
        "            existing_kpi = unique_kpis_dict[metric_key]\n",
        "            current_kpi = kpi\n",
        "\n",
        "            # Compare KPI text completeness\n",
        "            if len(current_kpi.get('kpi_text', '')) > len(existing_kpi.get('kpi_text', '')):\n",
        "                unique_kpis_dict[metric_key] = current_kpi\n",
        "                logging.debug(f\"Replaced with more complete KPI: {current_kpi.get('kpi_text', '')[:50]}...\")\n",
        "            else:\n",
        "                logging.debug(f\"Kept existing KPI: {existing_kpi.get('kpi_text', '')[:50]}...\")\n",
        "\n",
        "    # Step 2: Text similarity-based secondary deduplication\n",
        "    final_kpis = list(unique_kpis_dict.values())\n",
        "\n",
        "    # Use text similarity to check remaining potential duplicates\n",
        "    final_unique_kpis = []\n",
        "\n",
        "    for current_kpi in final_kpis:\n",
        "        is_duplicate = False\n",
        "        current_text = current_kpi.get('kpi_text', '')\n",
        "\n",
        "        for existing_kpi in final_unique_kpis:\n",
        "            existing_text = existing_kpi.get('kpi_text', '')\n",
        "\n",
        "            # Calculate text similarity\n",
        "            similarity = calculate_text_similarity(current_text, existing_text)\n",
        "\n",
        "            # If similarity is very high, consider it duplicate\n",
        "            if similarity > 0.8:\n",
        "                is_duplicate = True\n",
        "                logging.debug(f\"Text similarity duplicate filtered: {current_text[:50]}...\")\n",
        "                break\n",
        "\n",
        "        if not is_duplicate:\n",
        "            final_unique_kpis.append(current_kpi)\n",
        "\n",
        "    logging.info(f\"Universal post-processing: {len(final_unique_kpis)}/{len(kpis)} KPIs retained\")\n",
        "    return final_unique_kpis\n",
        "\n",
        "def calculate_text_similarity(text1: str, text2: str) -> float:\n",
        "    \"\"\"Calculate similarity between two texts\"\"\"\n",
        "    # Normalize texts\n",
        "    norm1 = ' '.join(text1.lower().split())\n",
        "    norm2 = ' '.join(text2.lower().split())\n",
        "\n",
        "    # Word sets\n",
        "    words1 = set(norm1.split())\n",
        "    words2 = set(norm2.split())\n",
        "\n",
        "    if len(words1) == 0 or len(words2) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate intersection and union\n",
        "    intersection = len(words1.intersection(words2))\n",
        "    union = len(words1.union(words2))\n",
        "\n",
        "    return intersection / union if union > 0 else 0.0\n",
        "\n",
        "def validate_kpi_quality(kpis: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"Additional quality validation for extracted KPIs with relaxed filtering\"\"\"\n",
        "    if not ENABLE_QUALITY_VALIDATION:\n",
        "        return kpis\n",
        "\n",
        "    quality_kpis = []\n",
        "\n",
        "    for kpi in kpis:\n",
        "        kpi_text = kpi.get('kpi_text', '').lower()\n",
        "\n",
        "        # Exclude \"planned tone\" KPIs (not actual performance)\n",
        "        is_future_statement = any(word in kpi_text for word in [\n",
        "            'will', 'aim to', 'plan to', 'planning to', 'intend to',\n",
        "            'is expected to', 'is scheduled to', 'expects to', 'expected to',\n",
        "            'targeting', 'propose to', 'going to', 'shall', 'to be installed'\n",
        "        ])\n",
        "        if is_future_statement:\n",
        "            logging.debug(f\"KPI rejected (future plan): {kpi_text[:100]}...\")\n",
        "            continue\n",
        "\n",
        "        # Filter procedural language\n",
        "        if contains_procedural_language(kpi_text):\n",
        "            logging.debug(f\"KPI rejected (procedural language): {kpi_text[:100]}...\")\n",
        "            continue\n",
        "\n",
        "        # Filter for phrases like \"place name + percentage\" (not ESG KPIs, but distribution descriptions)\n",
        "        geo_percent_pattern = re.compile(r\"^[a-z\\s,:%-]+(?:\\s)?\\d{1,3}%$\")\n",
        "        if geo_percent_pattern.match(kpi_text.strip()) and len(kpi_text.strip().split()) <= 6:\n",
        "            logging.debug(f\"KPI rejected (geo+percent short form): {kpi_text}\")\n",
        "            continue\n",
        "\n",
        "        # Verb whitelist: must include action verbs\n",
        "        allowed_kpi_verbs = [\n",
        "            'reduce', 'reduced', 'achieve', 'achieved', 'improve', 'improved',\n",
        "            'diverted', 'trained', 'invested', 'decreased', 'increased',\n",
        "            'consumed', 'emitted', 'saved', 'reached', 'attained', 'completed',\n",
        "            'recorded', 'cut', 'lowered', 'targeted', 'complied', 'avoided',\n",
        "            'used', 'recycled', 'sourced', 'returned', 'measured', 'maintained',\n",
        "            'reported', 'accounted', 'utilized', 'were', 'was'  # Add state verbs\n",
        "        ]\n",
        "        if not any(verb in kpi_text for verb in allowed_kpi_verbs):\n",
        "            logging.debug(f\"KPI rejected (no action verb): {kpi_text[:100]}...\")\n",
        "            continue\n",
        "\n",
        "        # Greylist verbs (action words but not necessarily performance words) - remove problematic words\n",
        "        graylist_verbs = [\n",
        "            'launched',  # Keep some potentially useful words, but remove obvious procedural words\n",
        "            'formed', 'opened', 'started'\n",
        "        ]\n",
        "\n",
        "        contains_graylist = any(verb in kpi_text for verb in graylist_verbs)\n",
        "\n",
        "        # Check for quantitative indicators\n",
        "        has_numbers = any(char.isdigit() for char in kpi_text)\n",
        "        has_percentage = '%' in kpi_text\n",
        "\n",
        "        # Extended units and measurement indicators\n",
        "        has_units = any(unit in kpi_text for unit in [\n",
        "            'tonnes', 'tons', 'kg', 'mwh', 'kwh', 'gwh', 'litres', 'liters', 'gallons',\n",
        "            'employees', 'hours', 'million', 'billion', 'thousand', 'm³', 'co2e', 'tco2e',\n",
        "            'dollars', 'usd', 'eur', 'gbp', 'incidents', 'rate', 'ratio', 'intensity',\n",
        "            'frequency', 'recordable', 'fatalities', 'injuries', 'directors', 'board',\n",
        "            'workforce', 'leadership', 'diversity', 'inclusion', 'satisfaction', 'retention',\n",
        "            'turnover', 'training', 'safety', 'ltifr', 'trir', 'compliance', 'audit',\n",
        "            'assessment', 'screening', 'supplier', 'breach', 'violation', 'disclosure',\n",
        "            'assurance', 'coverage', 'participation', 'completion', 'investment',\n",
        "            'volunteering', 'engagement', 'grievance', 'whistleblower', 'compensation',\n",
        "            'people', 'staff', 'workers', 'positions', 'roles', 'headcount', 'fte',\n",
        "            'performance', 'score', 'index', 'metric', 'level', 'amount', 'value',\n",
        "            'average', 'median', 'total', 'sum', 'count', 'number', 'quantity'\n",
        "        ])\n",
        "\n",
        "        # More flexible time reference detection\n",
        "        has_time_ref = any(time_word in kpi_text for time_word in [\n",
        "            '2019', '2020', '2021', '2022', '2023', '2024', '2025', '2026', '2027', '2028', '2029', '2030',\n",
        "            '2031', '2032', '2033', '2034', '2035', '2040', '2045', '2050',\n",
        "            'annual', 'yearly', 'year', 'quarter', 'month', 'by', 'target', 'baseline', 'fy',\n",
        "            'per year', 'per annum', 'quarterly', 'monthly', 'daily', 'future', 'deadline',\n",
        "            'period', 'reporting', 'current', 'previous', 'next', 'last', 'this'\n",
        "        ])\n",
        "\n",
        "        # Enhanced sustainability context detection\n",
        "        has_sustainability_context = any(sus_word in kpi_text for sus_word in [\n",
        "            # Environmental keywords\n",
        "            'emission', 'carbon', 'energy', 'renewable', 'waste', 'water', 'recycl',\n",
        "            'environmental', 'ghg', 'scope', 'climate', 'biodiversity', 'circular',\n",
        "            'materials', 'intensity', 'consumption', 'efficiency', 'footprint',\n",
        "            'sustainable', 'sustainability', 'green', 'clean', 'eco', 'offset',\n",
        "            'tcfd', 'nature', 'habitat', 'ecosystem', 'pollution', 'discharge',\n",
        "            'electricity', 'gas', 'fuel', 'solar', 'wind', 'hydro', 'nuclear',\n",
        "\n",
        "            # Social keywords\n",
        "            'safety', 'training', 'employee', 'diversity', 'community', 'social',\n",
        "            'workforce', 'gender', 'women', 'female', 'male', 'disability', 'disabled',\n",
        "            'inclusion', 'equity', 'equality', 'lgbtq', 'minorities', 'ethnic',\n",
        "            'health', 'wellbeing', 'wellness', 'satisfaction', 'retention', 'turnover',\n",
        "            'injury', 'incident', 'fatality', 'ltifr', 'trir', 'recordable',\n",
        "            'human rights', 'labor', 'child labor', 'forced labor', 'slavery',\n",
        "            'freedom', 'association', 'collective bargaining', 'grievance',\n",
        "            'volunteering', 'investment', 'hiring', 'local', 'stakeholder',\n",
        "            'customer', 'supplier', 'supply chain', 'accessibility', 'parental',\n",
        "            'mental health', 'ppe', 'emergency', 'drill', 'compliance',\n",
        "            'people', 'staff', 'workers', 'employment', 'job', 'career',\n",
        "            'leadership', 'management', 'senior', 'executive', 'promotion',\n",
        "\n",
        "            # Governance keywords\n",
        "            'governance', 'board', 'director', 'independent', 'chair', 'ceo',\n",
        "            'executive', 'compensation', 'pay', 'ethics', 'compliance', 'corruption',\n",
        "            'bribery', 'code of conduct', 'whistleblower', 'transparency',\n",
        "            'disclosure', 'reporting', 'assurance', 'audit', 'risk', 'management',\n",
        "            'cybersecurity', 'data', 'privacy', 'gdpr', 'breach', 'policy',\n",
        "            'screening', 'assessment', 'due diligence', 'political', 'contribution',\n",
        "            'gri', 'sasb', 'oversight', 'expertise', 'separation', 'incentive',\n",
        "            'fine', 'penalty', 'violation', 'resolution', 'anti-corruption',\n",
        "\n",
        "            # General business performance that could be sustainability-related\n",
        "            'performance', 'quality', 'delivery', 'customer', 'service', 'product',\n",
        "            'operation', 'facility', 'site', 'location', 'region', 'business'\n",
        "        ])\n",
        "\n",
        "        # If it is a greylist verb sentence, but there is no performance content such as numbers, units, time, etc. → delete\n",
        "        if contains_graylist and not (has_numbers or has_units or has_percentage or has_time_ref or has_sustainability_context):\n",
        "            logging.debug(f\"KPI rejected (graylist verb, no quantitative data): {kpi_text[:100]}...\")\n",
        "            continue\n",
        "\n",
        "        # More lenient quality scoring - only require numbers and either units/percentage OR time reference OR sustainability context\n",
        "        basic_requirements = has_numbers and (has_percentage or has_units or has_time_ref or has_sustainability_context)\n",
        "\n",
        "        # Additional check for obvious ESG relevance\n",
        "        is_esg_relevant = any(esg_word in kpi_text for esg_word in [\n",
        "            'emission', 'carbon', 'energy', 'waste', 'water', 'renewable', 'employee',\n",
        "            'safety', 'training', 'diversity', 'governance', 'board', 'compliance',\n",
        "            'sustainability', 'environmental', 'social', 'ghg', 'co2', 'workforce',\n",
        "            'gender', 'health', 'injury', 'incident', 'ethics', 'transparency'\n",
        "        ])\n",
        "\n",
        "        if basic_requirements or is_esg_relevant:\n",
        "            quality_kpis.append(kpi)\n",
        "            logging.debug(f\"KPI accepted: {kpi_text[:100]}...\")\n",
        "        else:\n",
        "            logging.debug(f\"KPI filtered out for quality: {kpi_text[:100]}...\")\n",
        "\n",
        "    logging.info(f\"Quality validation: {len(quality_kpis)}/{len(kpis)} KPIs passed\")\n",
        "    return quality_kpis"
      ],
      "metadata": {
        "id": "sB_kDyLZaG1o"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Image processing functions ============\n",
        "def extract_numeric_spans(page):\n",
        "    text_dict = page.get_text(\"dict\")\n",
        "    nums = []\n",
        "    for block in text_dict[\"blocks\"]:\n",
        "        for line in block.get(\"lines\", []):\n",
        "            for span in line.get(\"spans\", []):\n",
        "                s = span[\"text\"].strip()\n",
        "                if re.match(r\"[\\d,.]+%?$\", s):          # Pure number or number + %\n",
        "                    nums.append({\n",
        "                        \"text\": s,\n",
        "                        \"bbox\": span[\"bbox\"],           # (x0,y0,x1,y1)\n",
        "                        \"font\": span[\"size\"]\n",
        "                    })\n",
        "    return nums\n",
        "\n",
        "def extract_images_from_pdf_fixed(pdf_path: str) -> List[Dict]:\n",
        "    \"\"\"Extract images from PDF using PyMuPDF\"\"\"\n",
        "    images = []\n",
        "\n",
        "    try:\n",
        "        pdf_document = fitz.open(pdf_path)\n",
        "\n",
        "        for page_num in range(len(pdf_document)):\n",
        "            page = pdf_document[page_num]\n",
        "            image_list = page.get_images()\n",
        "\n",
        "            # 🔥 New: Extract page screenshots as an alternative\n",
        "            page_pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # high resolution\n",
        "            page_img = Image.frombytes(\"RGB\", [page_pix.width, page_pix.height], page_pix.samples)\n",
        "\n",
        "            # Add full page screenshot\n",
        "            images.append({\n",
        "                'image': page_img,\n",
        "                'page_number': page_num + 1,\n",
        "                'width': page_img.width,\n",
        "                'height': page_img.height,\n",
        "                'image_index': 'full_page',\n",
        "                'type': 'full_page'\n",
        "            })\n",
        "\n",
        "\n",
        "            for img_index, img in enumerate(image_list):\n",
        "                try:\n",
        "                    xref = img[0]\n",
        "                    base_image = pdf_document.extract_image(xref)\n",
        "                    image_bytes = base_image[\"image\"]\n",
        "\n",
        "                    image = Image.open(BytesIO(image_bytes))\n",
        "\n",
        "                    # Convert to RGB if needed\n",
        "                    if image.mode in ['RGBA', 'LA']:\n",
        "                        background = Image.new('RGB', image.size, (255, 255, 255))\n",
        "                        if image.mode == 'RGBA':\n",
        "                            background.paste(image, mask=image.split()[-1])\n",
        "                        else:\n",
        "                            background.paste(image)\n",
        "                        image = background\n",
        "                    elif image.mode != 'RGB':\n",
        "                        image = image.convert('RGB')\n",
        "\n",
        "                    # Filter small images\n",
        "                    if image.width >= 50 and image.height >= 50:\n",
        "                        images.append({\n",
        "                            'image': image,\n",
        "                            'page_number': page_num + 1,\n",
        "                            'width': image.width,\n",
        "                            'height': image.height,\n",
        "                            'image_index': img_index,\n",
        "                            'type': 'extracted'  # 🔥 Added type identifier\n",
        "                        })\n",
        "\n",
        "                except Exception as e:\n",
        "                    logging.warning(f\"Error extracting image {img_index} from page {page_num + 1}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        pdf_document.close()\n",
        "        logging.info(f\"Extracted {len(images)} images from PDF\")\n",
        "        return images\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error extracting images from PDF: {e}\")\n",
        "        return []\n",
        "\n",
        "def image_to_base64_fixed(image: Image.Image) -> str:\n",
        "    \"\"\"Convert image to base64 with error handling\"\"\"\n",
        "    try:\n",
        "        if image.mode not in ['RGB', 'L']:\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        # Resize large images\n",
        "        max_size = (1536, 1536)\n",
        "        if image.width > max_size[0] or image.height > max_size[1]:\n",
        "            # Calculate scaling to maintain aspect ratio\n",
        "            ratio = min(max_size[0]/image.width, max_size[1]/image.height)\n",
        "            new_size = (int(image.width * ratio), int(image.height * ratio))\n",
        "            image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "        buffered = BytesIO()\n",
        "        image.save(buffered, format=\"JPEG\", quality=95)\n",
        "        img_str = base64.b64encode(buffered.getvalue()).decode()\n",
        "\n",
        "        return img_str\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error converting image to base64: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "Vfegv4osaG3b"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# Multi-crop / multi-resolution generator (supports crop parameter 0)\n",
        "# ------------------------------------------------------------\n",
        "from itertools import product\n",
        "\n",
        "def generate_image_variants(img: Image.Image,\n",
        "                            max_side_full: int = 1200,\n",
        "                            crop_size: int = 768,\n",
        "                            stride: int = 512) -> List[Tuple[Image.Image, str]]:\n",
        "    \"\"\"\n",
        "    Returns [(variant_image, variant_tag), ...]\n",
        "    variant_tag value: original / resized / crop_{row}_{col}\n",
        "    \"\"\"\n",
        "    variants = []\n",
        "\n",
        "    # 0) Original image\n",
        "    variants.append((img, \"original\"))\n",
        "\n",
        "    # 1) Zoom (if the original image is too large)\n",
        "    w, h = img.size\n",
        "    if max(w, h) > max_side_full:\n",
        "        scale = max_side_full / float(max(w, h))\n",
        "        resized = img.resize((int(w * scale), int(h * scale)), Image.Resampling.LANCZOS)\n",
        "        variants.append((resized, \"resized\"))\n",
        "    else:\n",
        "        resized = img  # Keep the original image without scaling\n",
        "        variants.append((resized, \"resized\"))  # Unified plus resized version\n",
        "\n",
        "    # 2) Sliding window cropping (skipped when cropping size or step size is 0)\n",
        "    if crop_size > 0 and stride > 0:\n",
        "        base_img = variants[-1][0]\n",
        "        bw, bh = base_img.size\n",
        "        if bw > crop_size or bh > crop_size:\n",
        "            xs = list(range(0, max(bw - crop_size, 1), stride)) + [bw - crop_size]\n",
        "            ys = list(range(0, max(bh - crop_size, 1), stride)) + [bh - crop_size]\n",
        "            for r, c in product(range(len(ys)), range(len(xs))):\n",
        "                x, y = xs[c], ys[r]\n",
        "                crop = base_img.crop((x, y, x + crop_size, y + crop_size))\n",
        "                # Filter solid color areas\n",
        "                if np.array(crop.convert('L')).std() < 5:\n",
        "                    continue\n",
        "                variants.append((crop, f\"crop_{r}_{c}\"))\n",
        "\n",
        "    return variants"
      ],
      "metadata": {
        "id": "ma9D4crJaG5z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------\n",
        "# 📊 A chart recognition function that replaces plotclassifier (Hugging Face model)\n",
        "# ---------------------------------------------\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
        "import torch\n",
        "# 🔧 Fix: Chart recognition with CLIP model\n",
        "def setup_chart_classifier():\n",
        "    \"\"\"Setting up the chart classifier\"\"\"\n",
        "    try:\n",
        "        from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "        # Loading CLIP Model\n",
        "        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "        def is_chart_image_clip(image: Image.Image) -> bool:\n",
        "            \"\"\"Use CLIP to determine whether it is a chart\"\"\"\n",
        "            try:\n",
        "                # Defines text description related to the chart\n",
        "                chart_labels = [\n",
        "                    \"a chart\", \"a graph\", \"a bar chart\", \"a pie chart\",\n",
        "                    \"a line graph\", \"a table\", \"data visualization\",\n",
        "                    \"statistics\", \"a diagram\", \"an infographic\"\n",
        "                ]\n",
        "\n",
        "                # Processing Input\n",
        "                inputs = processor(\n",
        "                    text=chart_labels,\n",
        "                    images=image,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True\n",
        "                )\n",
        "\n",
        "                # Get prediction results\n",
        "                outputs = model(**inputs)\n",
        "                logits_per_image = outputs.logits_per_image\n",
        "                probs = logits_per_image.softmax(dim=1)\n",
        "\n",
        "                # If the probability of any chart label is greater than 0.25, it is considered to be a chart\n",
        "                max_prob = probs.max().item()\n",
        "                is_chart = max_prob > 0.25\n",
        "\n",
        "                logging.debug(f\"CLIP chart recognition: maximum probability = {max_prob:.3f}, result = {is_chart}\")\n",
        "                return is_chart\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"CLIP chart recognition failed: {e}\")\n",
        "                # Downgrade to statistical methods\n",
        "                gray = image.convert('L')\n",
        "                return np.array(gray).std() > 15\n",
        "\n",
        "        logging.info(\"✅ Graph recognition using CLIP model\")\n",
        "        return is_chart_image_clip\n",
        "\n",
        "    except ImportError:\n",
        "        logging.warning(\"CLIP model is not available, use statistical methods\")\n",
        "        def is_chart_image_stats(image: Image.Image) -> bool:\n",
        "            \"\"\"Statistical method to determine whether it is a chart\"\"\"\n",
        "            try:\n",
        "                gray = image.convert('L')\n",
        "                std_dev = np.array(gray).std()\n",
        "                return std_dev > 15\n",
        "            except:\n",
        "                return True\n",
        "\n",
        "        return is_chart_image_stats\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to set chart classifier: {e}\")\n",
        "        def is_chart_image_fallback(image: Image.Image) -> bool:\n",
        "            return True  # Conservative Strategy: When in Doubt, Analyze\n",
        "        return is_chart_image_fallback\n",
        "\n",
        "# Initialize the graph classifier\n",
        "is_chart_image = setup_chart_classifier()\n",
        "\n",
        "\n",
        "def extract_kpi_from_image_fixed(image: Image.Image, page_number: int, image_type: str = 'extracted') -> List[Dict]:\n",
        "    \"\"\"Extract KPIs from image with improved error handling\"\"\"\n",
        "    try:\n",
        "        # 🔥 New: Pre-filter: Check if it might be a chart\n",
        "        if not is_chart_image(image):\n",
        "            logging.debug(f\"Image on page {page_number} filtered out (not likely a chart)\")\n",
        "            return []\n",
        "\n",
        "        base64_image = image_to_base64_fixed(image)\n",
        "        if not base64_image:\n",
        "            return []\n",
        "\n",
        "        # 🔥 Change: Use enhanced prompt\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": ENHANCED_IMAGE_KPI_SYSTEM_PROMPT  # 🔥 Using the new prompt\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\n",
        "                            \"type\": \"text\",\n",
        "                            # 🔥 New: Detailed user instructions\n",
        "                            \"text\": \"\"\"Analyze this image carefully for quantifiable performance data.\n",
        "\n",
        "IMPORTANT ANALYSIS PRINCIPLES:\n",
        "\n",
        "1. **Chart Type Recognition**:\n",
        "   - Stacked charts: Multiple colors/patterns layered in same position\n",
        "   - Grouped charts: Multiple elements side by side at same position\n",
        "   - Simple charts: One data point per position\n",
        "\n",
        "2. **Value Extraction Rules**:\n",
        "   - For STACKED charts: Read each layer separately, NOT the total height\n",
        "   - For GROUPED charts: Read each element individually\n",
        "   - For SIMPLE charts: Read data point values directly\n",
        "\n",
        "3. **Data Relevance Filter**:\n",
        "   ✅ EXTRACT: Performance outcomes, efficiency metrics, reduction rates, satisfaction scores, compliance rates\n",
        "   ❌ SKIP: Certification counts, project timelines, implementation schedules, organizational charts, process flows\n",
        "\n",
        "4. **Quality Standards**:\n",
        "   - Only extract clear, quantifiable performance indicators\n",
        "   - Each data point must have complete context\n",
        "   - If uncertain about values, don't estimate\n",
        "   - If chart shows mainly operational/administrative data, return empty array\n",
        "\n",
        "Please analyze this chart step by step:\n",
        "- First identify the chart type\n",
        "- Then determine if it contains performance KPIs\n",
        "- Finally extract all relevant performance data points\n",
        "\n",
        "Focus on measurable outcomes and achievements, not counts or processes.\"\"\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"type\": \"image_url\",\n",
        "                            \"image_url\": {\n",
        "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
        "                                \"detail\": \"high\"\n",
        "                            }\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.1,\n",
        "            max_tokens=4000,\n",
        "            timeout=60\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message.content.strip()\n",
        "\n",
        "        if not content:\n",
        "            return []\n",
        "\n",
        "        # Clean formatting\n",
        "        if content.startswith('```json'):\n",
        "            content = content[7:]\n",
        "        if content.endswith('```'):\n",
        "            content = content[:-3]\n",
        "\n",
        "        content = content.strip()\n",
        "\n",
        "        if not content.startswith(\"[\"):\n",
        "            logging.warning(f\"Image analysis response not JSON list: {content[:100]}...\")\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            result = json.loads(content)\n",
        "        except json.JSONDecodeError as e:\n",
        "            logging.warning(f\"JSON parsing failed for image analysis: {e}\")\n",
        "            return []\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            return []\n",
        "\n",
        "        # Process results\n",
        "        processed_result = []\n",
        "        for item in result:\n",
        "            if isinstance(item, dict) and 'kpi_text' in item:\n",
        "                if not item.get('kpi_text', '').strip():\n",
        "                    continue\n",
        "\n",
        "                item['source_page'] = page_number\n",
        "                item['source_type'] = 'image'\n",
        "                item['image_type'] = image_type  # 🔥 新增字段\n",
        "\n",
        "                # 🔥 更改：确保有chart标识\n",
        "                kpi_text = item['kpi_text']\n",
        "                if not any(marker in kpi_text.lower() for marker in ['chart', 'graph', 'table', 'figure']):\n",
        "                    chart_type = item.get('chart_type', 'chart')\n",
        "                    item['kpi_text'] = f\"[{chart_type.title()}] {kpi_text}\"\n",
        "\n",
        "                processed_result.append(item)\n",
        "\n",
        "        if processed_result:\n",
        "            logging.info(f\"✅ Extracted {len(processed_result)} KPIs from {image_type} on page {page_number}\")\n",
        "        else:\n",
        "            logging.debug(f\"❌ No KPIs found in {image_type} on page {page_number}\")\n",
        "\n",
        "        return processed_result\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error extracting KPIs from image: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def process_pdf_images_for_kpis_fixed(pdf_path: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Traverse each page of the PDF:\n",
        "        • Perform multiple cropping + Vision on all ‘extracted’ images on the page\n",
        "        • If the page has not captured the KPI, perform Vision on the entire page screenshot\n",
        "    \"\"\"\n",
        "    logging.info(\"Starting page-by-page image KPI extraction …\")\n",
        "\n",
        "    images = extract_images_from_pdf_fixed(pdf_path)\n",
        "    if not images:\n",
        "        return []\n",
        "\n",
        "    # Aggregate images by page\n",
        "    page_dict = {}\n",
        "    for info in images:\n",
        "        pg = info[\"page_number\"]\n",
        "        page_dict.setdefault(pg, {\"extracted\": [], \"full\": None})\n",
        "        if info[\"type\"] == \"extracted\":\n",
        "            page_dict[pg][\"extracted\"].append(info[\"image\"])\n",
        "        else:                    # full_page\n",
        "            page_dict[pg][\"full\"] = info[\"image\"]\n",
        "\n",
        "    all_image_kpis: List[Dict] = []\n",
        "\n",
        "    # —— Page by page processing ——\n",
        "    for pg in sorted(page_dict.keys()):\n",
        "        logging.info(f\"\\n=== Page {pg} ===\")\n",
        "        page_kpis: List[Dict] = []\n",
        "\n",
        "        # ① Individually extracted images\n",
        "        for idx, img in enumerate(page_dict[pg][\"extracted\"]):\n",
        "            for var_img, var_tag in generate_image_variants(img, 1200, 768, 512):\n",
        "                kpis = extract_kpi_from_image_fixed(\n",
        "                    var_img, pg, f\"extracted_{var_tag}\"\n",
        "                )\n",
        "                for k in kpis:\n",
        "                    key = generate_universal_metric_key(k)\n",
        "                    if key not in {generate_universal_metric_key(x) for x in page_kpis}:\n",
        "                        page_kpis.append(k)\n",
        "                time.sleep(0.8)\n",
        "\n",
        "        # ② If it is still empty, analyze the entire page again\n",
        "        if not page_kpis and page_dict[pg][\"full\"] is not None:\n",
        "            for var_img, var_tag in generate_image_variants(\n",
        "                    page_dict[pg][\"full\"], 1200, 0, 0):   # 只做 original/resized\n",
        "                kpis = extract_kpi_from_image_fixed(\n",
        "                    var_img, pg, f\"full_{var_tag}\"\n",
        "                )\n",
        "                for k in kpis:\n",
        "                    key = generate_universal_metric_key(k)\n",
        "                    if key not in {generate_universal_metric_key(x) for x in page_kpis}:\n",
        "                        page_kpis.append(k)\n",
        "                time.sleep(1.0)\n",
        "\n",
        "        logging.info(f\"  → Page {pg} KPI count: {len(page_kpis)}\")\n",
        "        all_image_kpis.extend(page_kpis)\n",
        "\n",
        "    logging.info(f\"Image KPI extraction finished: {len(all_image_kpis)} KPIs from {len(page_dict)} pages\")\n",
        "    return all_image_kpis"
      ],
      "metadata": {
        "id": "F5ZLpwL8Zok_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8db2fda0-b5b5-4bfc-94c6-530aa98387d3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Conservative image filter\n",
        "def conservative_image_filter(image: Image.Image) -> Tuple[bool, str]:\n",
        "    \"\"\"Conservative image filtering - only filters obviously useless images\"\"\"\n",
        "    try:\n",
        "        # Only filter very small images (maybe logos, icons)\n",
        "        if image.width < 40 or image.height < 40:\n",
        "            return False, \"too_small_icon\"\n",
        "\n",
        "        # Filter only images of almost pure colors (decorative elements)\n",
        "        gray = np.array(image.convert('L'))\n",
        "        std_dev = gray.std()\n",
        "\n",
        "        # Very conservative threshold - only images with completely pure colors are filtered\n",
        "        if std_dev < 3:\n",
        "            return False, \"pure_color\"\n",
        "\n",
        "        # Check if it is a pure white background (blank area)\n",
        "        mean_val = gray.mean()\n",
        "        if mean_val > 250 and std_dev < 8:\n",
        "            return False, \"blank_white\"\n",
        "\n",
        "        # Default: Process all other images to ensure integrity\n",
        "        return True, \"keep_for_analysis\"\n",
        "    except Exception:\n",
        "        return True, \"filter_error_keep\"\n",
        "\n",
        "# 2. Cache mechanism\n",
        "class FastKPICache:\n",
        "    def __init__(self, cache_dir: str = \"fast_kpi_cache\"):\n",
        "        self.cache_dir = cache_dir\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "        self.hit_count = 0\n",
        "        self.miss_count = 0\n",
        "\n",
        "    def get_image_hash(self, image: Image.Image) -> str:\n",
        "        \"\"\"Fast image fingerprint generation\"\"\"\n",
        "        width, height = image.size\n",
        "        if width > 100 and height > 100:\n",
        "            center_crop = image.crop((\n",
        "                width//4, height//4,\n",
        "                3*width//4, 3*height//4\n",
        "            )).resize((32, 32))\n",
        "            img_bytes = BytesIO()\n",
        "            center_crop.save(img_bytes, format='JPEG', quality=50)\n",
        "            sample_hash = hashlib.md5(img_bytes.getvalue()).hexdigest()[:16]\n",
        "        else:\n",
        "            sample_hash = hashlib.md5(str(width * height).encode()).hexdigest()[:16]\n",
        "\n",
        "        return f\"{width}x{height}_{sample_hash}\"\n",
        "\n",
        "    def get_cached_kpis(self, image_hash: str) -> Optional[List[Dict]]:\n",
        "        cache_file = os.path.join(self.cache_dir, f\"{image_hash}.pkl\")\n",
        "        if os.path.exists(cache_file):\n",
        "            try:\n",
        "                with open(cache_file, 'rb') as f:\n",
        "                    self.hit_count += 1\n",
        "                    return pickle.load(f)\n",
        "            except:\n",
        "                pass\n",
        "        self.miss_count += 1\n",
        "        return None\n",
        "\n",
        "    def cache_kpis(self, image_hash: str, kpis: List[Dict]):\n",
        "        cache_file = os.path.join(self.cache_dir, f\"{image_hash}.pkl\")\n",
        "        try:\n",
        "            with open(cache_file, 'wb') as f:\n",
        "                pickle.dump(kpis, f)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def get_stats(self):\n",
        "        total = self.hit_count + self.miss_count\n",
        "        hit_rate = self.hit_count / total if total > 0 else 0\n",
        "        return f\"Cache: {self.hit_count} hits, {self.miss_count} misses (hit rate: {hit_rate:.1%})\"\n",
        "\n",
        "# Initialize the cache\n",
        "fast_cache = FastKPICache()\n",
        "\n",
        "# 3. Optimized API calls\n",
        "COMPREHENSIVE_EXTRACTION_PROMPT = \"\"\"\n",
        "You are an expert data analyst. Extract ALL quantifiable performance indicators from this image.\n",
        "\n",
        "CRITICAL REQUIREMENTS:\n",
        "1. Extract EVERY visible number, percentage, and metric\n",
        "2. Include ALL data points from charts, graphs, and tables\n",
        "3. Do not skip any quantifiable information\n",
        "\n",
        "Return complete JSON array:\n",
        "[\n",
        "  {\n",
        "    \"kpi_text\": \"Complete contextual description with the specific number\",\n",
        "    \"quantitative_value\": \"exact number only\",\n",
        "    \"unit\": \"unit of measurement\",\n",
        "    \"kpi_theme\": \"Environmental/Social/Governance\",\n",
        "    \"kpi_category\": \"specific category\",\n",
        "    \"time_period\": \"year/period if visible\"\n",
        "  }\n",
        "]\n",
        "\n",
        "COMPLETENESS IS CRITICAL - Extract everything quantifiable.\n",
        "\"\"\"\n",
        "\n",
        "def extract_kpi_optimized(image: Image.Image, page_number: int) -> List[Dict]:\n",
        "    \"\"\"Optimized KPI extraction\"\"\"\n",
        "    try:\n",
        "        # Check the cache\n",
        "        image_hash = fast_cache.get_image_hash(image)\n",
        "        cached_kpis = fast_cache.get_cached_kpis(image_hash)\n",
        "        if cached_kpis is not None:\n",
        "            for kpi in cached_kpis:\n",
        "                kpi['source_page'] = page_number\n",
        "            return cached_kpis\n",
        "\n",
        "        # Optimizing image encoding\n",
        "        base64_image = image_to_base64_optimized(image)\n",
        "        if not base64_image:\n",
        "            return []\n",
        "\n",
        "        # API Calls\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": COMPREHENSIVE_EXTRACTION_PROMPT},\n",
        "                    {\"type\": \"image_url\", \"image_url\": {\n",
        "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
        "                        \"detail\": \"high\"\n",
        "                    }}\n",
        "                ]\n",
        "            }],\n",
        "            temperature=0.0,\n",
        "            max_tokens=2000,\n",
        "            timeout=60\n",
        "        )\n",
        "\n",
        "        # Parsing results\n",
        "        kpis = parse_optimized_response(response, page_number)\n",
        "\n",
        "        # Caching results\n",
        "        fast_cache.cache_kpis(image_hash, kpis)\n",
        "\n",
        "        return kpis\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Optimized KPI extraction failed for page {page_number}: {e}\")\n",
        "        return []\n",
        "\n",
        "def image_to_base64_optimized(image: Image.Image) -> str:\n",
        "    \"\"\"Optimized image encoding\"\"\"\n",
        "    try:\n",
        "        max_dimension = 1400  # Maintain high quality\n",
        "        width, height = image.size\n",
        "\n",
        "        if max(width, height) > max_dimension:\n",
        "            scale = max_dimension / max(width, height)\n",
        "            new_size = (int(width * scale), int(height * scale))\n",
        "            image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "        if image.mode != 'RGB':\n",
        "            if image.mode in ['RGBA', 'LA']:\n",
        "                background = Image.new('RGB', image.size, (255, 255, 255))\n",
        "                if image.mode == 'RGBA':\n",
        "                    background.paste(image, mask=image.split()[-1])\n",
        "                else:\n",
        "                    background.paste(image)\n",
        "                image = background\n",
        "            else:\n",
        "                image = image.convert('RGB')\n",
        "\n",
        "        buffered = BytesIO()\n",
        "        image.save(buffered, format=\"JPEG\", quality=92, optimize=True)\n",
        "        return base64.b64encode(buffered.getvalue()).decode()\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Optimized image encoding failed: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def parse_optimized_response(response, page_number: int) -> List[Dict]:\n",
        "    \"\"\"Optimized response parsing\"\"\"\n",
        "    try:\n",
        "        content = response.choices[0].message.content.strip()\n",
        "\n",
        "        if content.startswith('```json'):\n",
        "            content = content[7:]\n",
        "        if content.endswith('```'):\n",
        "            content = content[:-3]\n",
        "        content = content.strip()\n",
        "\n",
        "        if not content.startswith('['):\n",
        "            return []\n",
        "\n",
        "        result = json.loads(content)\n",
        "        if not isinstance(result, list):\n",
        "            return []\n",
        "\n",
        "        validated_kpis = []\n",
        "        for item in result:\n",
        "            if (isinstance(item, dict) and\n",
        "                item.get('kpi_text', '').strip() and\n",
        "                item.get('quantitative_value', '').strip()):\n",
        "\n",
        "                kpi = {\n",
        "                    'kpi_text': item.get('kpi_text', '').strip(),\n",
        "                    'quantitative_value': str(item.get('quantitative_value', '')).strip(),\n",
        "                    'unit': item.get('unit', '').strip(),\n",
        "                    'kpi_theme': item.get('kpi_theme', 'Environmental').strip(),\n",
        "                    'kpi_category': item.get('kpi_category', '').strip(),\n",
        "                    'time_period': item.get('time_period', '').strip(),\n",
        "                    'source_page': page_number,\n",
        "                    'source_type': 'image'\n",
        "                }\n",
        "                validated_kpis.append(kpi)\n",
        "\n",
        "        return validated_kpis\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Optimized response parsing failed: {e}\")\n",
        "        return []\n",
        "\n",
        "# 4. Parallel image processing\n",
        "def process_images_in_parallel(image_data: List[Dict], max_workers: int = 3) -> List[Dict]:\n",
        "    \"\"\"Parallel image processing\"\"\"\n",
        "    if not image_data:\n",
        "        return []\n",
        "\n",
        "    print(f\"🔄 Processing {len(image_data)} images in parallel...\")\n",
        "\n",
        "    all_kpis = []\n",
        "    completed_count = 0\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        future_to_info = {}\n",
        "        for img_info in image_data:\n",
        "            future = executor.submit(\n",
        "                extract_kpi_optimized,\n",
        "                img_info['image'],\n",
        "                img_info['page_number']\n",
        "            )\n",
        "            future_to_info[future] = img_info\n",
        "\n",
        "        for future in concurrent.futures.as_completed(future_to_info):\n",
        "            img_info = future_to_info[future]\n",
        "            try:\n",
        "                kpis = future.result(timeout=90)\n",
        "                all_kpis.extend(kpis)\n",
        "                completed_count += 1\n",
        "\n",
        "                if completed_count % 5 == 0:\n",
        "                    progress = completed_count / len(image_data) * 100\n",
        "                    print(f\"   📈 Progress: {completed_count}/{len(image_data)} ({progress:.1f}%)\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Image processing failed for page {img_info['page_number']}: {e}\")\n",
        "                continue\n",
        "\n",
        "    print(f\"📊 Parallel processing completed: {len(all_kpis)} KPIs extracted\")\n",
        "    print(f\"📋 {fast_cache.get_stats()}\")\n",
        "\n",
        "    return all_kpis"
      ],
      "metadata": {
        "id": "7j4_GQ6Llw8e"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "N7yh-bSwZonF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0950998b-3490-4281-bb82-c947dfa7448a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Main processing function ============\n",
        "def process_sustainability_report_with_enhanced_images(pdf_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Main processing function with image analysis\"\"\"\n",
        "    logging.info(\"Starting enhanced PDF processing with image analysis...\")\n",
        "\n",
        "    # Step 1: Text and table extraction\n",
        "    logging.info(\"Step 1/5: Reading PDF text and tables...\")\n",
        "    full_text = pdf_to_text_and_tables(pdf_path)\n",
        "\n",
        "    camelot_tables = camelot_extra_tables_enhanced(pdf_path)\n",
        "    if camelot_tables:\n",
        "        full_text += \"\\n\\n\" + \"\\n\\n\".join(camelot_tables)\n",
        "\n",
        "    logging.info(\"Step 2/5: Chunking text...\")\n",
        "    chunks = split_into_chunks(full_text, MAX_TOKENS_CHUNK)\n",
        "\n",
        "    logging.info(\"Step 3/5: Extracting KPIs from text...\")\n",
        "    text_kpis = []\n",
        "    for idx, chunk in enumerate(chunks, 1):\n",
        "        logging.info(f\"Processing text chunk {idx}/{len(chunks)}\")\n",
        "        if chunk.strip():\n",
        "            chunk_kpis = extract_kpi_from_chunk_universal(chunk)\n",
        "            text_kpis.extend(chunk_kpis)\n",
        "            if idx < len(chunks):\n",
        "                time.sleep(SLEEP_SEC)\n",
        "\n",
        "    # Step 4: Image KPI extraction\n",
        "    logging.info(\"Step 4/5: Extracting KPIs from images...\")\n",
        "    image_kpis = process_pdf_images_for_kpis_fixed(pdf_path)\n",
        "\n",
        "    # Step 5: Combine and process\n",
        "    logging.info(\"Step 5/5: Combining and processing all KPIs...\")\n",
        "\n",
        "    for kpi in text_kpis:\n",
        "        if 'source_type' not in kpi:\n",
        "            kpi['source_type'] = 'text'\n",
        "\n",
        "    all_kpis = text_kpis + image_kpis\n",
        "    all_kpis = post_process_kpis_universal(all_kpis)\n",
        "\n",
        "    df_auto = pd.DataFrame(all_kpis)\n",
        "\n",
        "    if not df_auto.empty:\n",
        "        if 'source_type' not in df_auto.columns:\n",
        "            df_auto['source_type'] = 'text'\n",
        "\n",
        "        initial_count = len(df_auto)\n",
        "        df_auto = df_auto.drop_duplicates(subset=['kpi_text'], keep='first')\n",
        "        final_count = len(df_auto)\n",
        "\n",
        "        logging.info(f\"Removed {initial_count - final_count} duplicate KPIs\")\n",
        "\n",
        "        try:\n",
        "            df_auto = df_auto.sort_values(['source_type', 'kpi_theme', 'kpi_category'], na_position='last')\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "        text_kpi_count = len([kpi for kpi in all_kpis if kpi.get('source_type', 'text') != 'image'])\n",
        "        image_kpi_count = len([kpi for kpi in all_kpis if kpi.get('source_type') == 'image'])\n",
        "\n",
        "        logging.info(f\"KPI Summary: {text_kpi_count} from text/tables, {image_kpi_count} from images\")\n",
        "\n",
        "    return df_auto\n"
      ],
      "metadata": {
        "id": "Mr1OIB2BZosj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Optimize moderator processing function ============\n",
        "def process_sustainability_report_OPTIMIZED(pdf_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Optimize moderator processing functions - improve performance while ensuring integrity\"\"\"\n",
        "\n",
        "    start_time = time.time()\n",
        "    print(\"⚡ Starting OPTIMIZED processing with completeness guarantee...\")\n",
        "\n",
        "    try:\n",
        "        # Parallel text and image preprocessing\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
        "            print(\"🔄 Starting parallel text and image preprocessing...\")\n",
        "\n",
        "            # Text processing (using your existing logic)\n",
        "            def extract_text_kpis():\n",
        "                full_text = pdf_to_text_and_tables(pdf_path)\n",
        "                camelot_tables = camelot_extra_tables_enhanced(pdf_path)\n",
        "                if camelot_tables:\n",
        "                    full_text += \"\\n\\n\" + \"\\n\\n\".join(camelot_tables)\n",
        "\n",
        "                chunks = split_into_chunks(full_text, MAX_TOKENS_CHUNK)\n",
        "                text_kpis = []\n",
        "                for idx, chunk in enumerate(chunks, 1):\n",
        "                    if chunk.strip():\n",
        "                        chunk_kpis = extract_kpi_from_chunk_universal(chunk)\n",
        "                        text_kpis.extend(chunk_kpis)\n",
        "                        if idx < len(chunks):\n",
        "                            time.sleep(SLEEP_SEC)\n",
        "                return text_kpis\n",
        "\n",
        "            #Image preprocessing (using optimized filtering)\n",
        "            def extract_and_filter_images():\n",
        "                all_images = extract_images_from_pdf_fixed(pdf_path)\n",
        "                filtered_images = []\n",
        "\n",
        "                for img_info in all_images:\n",
        "                    should_process, reason = conservative_image_filter(img_info['image'])\n",
        "                    if should_process:\n",
        "                        filtered_images.append(img_info)\n",
        "\n",
        "                print(f\"📊 Conservative filtering: Kept {len(filtered_images)}/{len(all_images)} images\")\n",
        "                return filtered_images\n",
        "\n",
        "            text_future = executor.submit(extract_text_kpis)\n",
        "            image_future = executor.submit(extract_and_filter_images)\n",
        "\n",
        "            text_kpis = text_future.result()\n",
        "            image_data = image_future.result()\n",
        "\n",
        "        preprocessing_time = time.time() - start_time\n",
        "        print(f\"⏱️  Preprocessing completed in {preprocessing_time:.1f}s\")\n",
        "\n",
        "        # Parallel Image KPI Extraction\n",
        "        image_start = time.time()\n",
        "        image_kpis = process_images_in_parallel(image_data, max_workers=3)\n",
        "        image_time = time.time() - image_start\n",
        "        print(f\"⏱️  Image processing completed in {image_time:.1f}s\")\n",
        "\n",
        "        # Post-processing\n",
        "        all_kpis = text_kpis + image_kpis\n",
        "        all_kpis = post_process_kpis_universal(all_kpis)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        df_auto = pd.DataFrame(all_kpis)\n",
        "\n",
        "        if not df_auto.empty:\n",
        "            initial_count = len(df_auto)\n",
        "            df_auto = df_auto.drop_duplicates(subset=['kpi_text'], keep='first')\n",
        "            final_count = len(df_auto)\n",
        "\n",
        "            if 'source_type' not in df_auto.columns:\n",
        "                df_auto['source_type'] = 'text'\n",
        "\n",
        "            print(f\"🔄 Removed {initial_count - final_count} exact duplicates\")\n",
        "\n",
        "        # Performance Statistics\n",
        "        total_time = time.time() - start_time\n",
        "        text_count = len([k for k in all_kpis if k.get('source_type') != 'image'])\n",
        "        image_count = len([k for k in all_kpis if k.get('source_type') == 'image'])\n",
        "\n",
        "        print(f\"\\n⚡ OPTIMIZED processing completed!\")\n",
        "        print(f\"⏱️  Total time: {total_time:.1f}s ({total_time/60:.1f}min)\")\n",
        "        print(f\"📊 Results:\")\n",
        "        print(f\"   - Text/Tables: {text_count} KPIs\")\n",
        "        print(f\"   - Images/Charts: {image_count} KPIs\")\n",
        "        print(f\"   - Total unique: {len(df_auto)} KPIs\")\n",
        "        print(f\"⚡ Performance: {len(df_auto)/total_time:.1f} KPIs/second\")\n",
        "\n",
        "        return df_auto\n",
        "\n",
        "    except Exception as e:\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"❌ Optimized processing failed after {total_time:.1f}s: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return pd.DataFrame()"
      ],
      "metadata": {
        "id": "YWPKhJAPmHg7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Result saving and comparison functions ============\n",
        "def infer_stakeholder(row) -> str:\n",
        "    \"\"\"Infer affected stakeholders based on KPI theme and category\"\"\"\n",
        "    theme = row.get('kpi_theme', '').lower()\n",
        "    category = row.get('kpi_category', '').lower()\n",
        "    kpi_text = row.get('kpi_text', '').lower()\n",
        "\n",
        "    if theme == 'environmental':\n",
        "        return \"Environment, Community, Future Generations\"\n",
        "    elif theme == 'social':\n",
        "        if 'employee' in category or 'workforce' in category or 'gender' in category:\n",
        "            return \"Employees\"\n",
        "        elif 'customer' in category or 'safety' in category:\n",
        "            return \"Customers, Community\"\n",
        "        elif 'community' in category:\n",
        "            return \"Local Communities\"\n",
        "        elif 'supply' in category or 'supplier' in kpi_text:\n",
        "            return \"Suppliers, Business Partners\"\n",
        "        else:\n",
        "            return \"Employees, Community\"\n",
        "    elif theme == 'governance':\n",
        "        if 'board' in category:\n",
        "            return \"Shareholders, Investors\"\n",
        "        elif 'cyber' in category or 'data' in category:\n",
        "            return \"Customers, Employees, Business Partners\"\n",
        "        else:\n",
        "            return \"Shareholders, Investors, Stakeholders\"\n",
        "    else:\n",
        "        return \"All Stakeholders\"\n",
        "\n",
        "def save_results(df_auto: pd.DataFrame, output_path: str, pdf_path: str = \"\") -> None:\n",
        "    \"\"\"Save results to Excel file with proper formatting\"\"\"\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True)\n",
        "\n",
        "        if not df_auto.empty:\n",
        "            # Add metadata columns\n",
        "            pdf_filename = os.path.basename(pdf_path) if pdf_path else \"Unknown\"\n",
        "            df_auto['PDF file name'] = pdf_filename\n",
        "            df_auto['Title of the report'] = \"\"\n",
        "\n",
        "            if 'source_page' in df_auto.columns:\n",
        "                df_auto['Absolute Page Number'] = df_auto['source_page']\n",
        "                df_auto = df_auto.drop('source_page', axis=1)\n",
        "            else:\n",
        "                df_auto['Absolute Page Number'] = \"Unknown\"\n",
        "\n",
        "            df_auto['Impacted Stakeholder'] = df_auto.apply(infer_stakeholder, axis=1)\n",
        "\n",
        "            # Reorder columns\n",
        "            original_columns = [col for col in df_auto.columns if col not in\n",
        "                              ['PDF file name', 'Title of the report', 'Absolute Page Number', 'Impacted Stakeholder']]\n",
        "            new_column_order = ['PDF file name', 'Title of the report', 'Absolute Page Number', 'Impacted Stakeholder'] + original_columns\n",
        "            df_auto = df_auto[new_column_order]\n",
        "\n",
        "        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
        "            df_auto.to_excel(writer, sheet_name='Auto_KPIs', index=False)\n",
        "\n",
        "            if not df_auto.empty:\n",
        "                # Theme summary\n",
        "                theme_summary = df_auto.groupby('kpi_theme').size().reset_index(name='count')\n",
        "                theme_summary.to_excel(writer, sheet_name='Theme_Summary', index=False)\n",
        "\n",
        "                # Category summary\n",
        "                category_summary = df_auto.groupby(['kpi_theme', 'kpi_category']).size().reset_index(name='count')\n",
        "                category_summary.to_excel(writer, sheet_name='Category_Summary', index=False)\n",
        "\n",
        "        logging.info(f\"Results saved to {output_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving results: {e}\")"
      ],
      "metadata": {
        "id": "kucPzNTdZouV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Main execution function ============\n",
        "def main():\n",
        "    \"\"\"Enhanced main execution function with validation\"\"\"\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format=\"%(asctime)s - %(levelname)s: %(message)s\",\n",
        "        datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(PDF_PATH):\n",
        "            logging.error(f\"PDF file not found: {PDF_PATH}\")\n",
        "            return\n",
        "\n",
        "        # Process the PDF\n",
        "        df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
        "\n",
        "        # Save results\n",
        "        save_results(df_auto, EXPORT_AUTO_XLSX, PDF_PATH)\n",
        "        logging.info(f\"KPI extraction completed: {len(df_auto)} KPIs extracted\")\n",
        "\n",
        "        # Enhanced validation with comprehensive analysis\n",
        "        if MANUAL_XLSX and Path(MANUAL_XLSX).exists():\n",
        "            print(\"\\n🔍 Running comprehensive validation...\")\n",
        "            validation_results = enhanced_compare_with_manual_kpis(\n",
        "                df_auto, MANUAL_XLSX, \"comprehensive_validation\"\n",
        "            )\n",
        "\n",
        "            if validation_results:\n",
        "                print(\"✅ Validation completed with detailed analysis!\")\n",
        "                print(f\"📁 Detailed results saved to: comprehensive_validation/\")\n",
        "            else:\n",
        "                print(\"⚠️ Validation encountered issues\")\n",
        "        else:\n",
        "            logging.info(\"Manual KPI file not found, skipping validation.\")\n",
        "\n",
        "        # Display summary\n",
        "        if not df_auto.empty:\n",
        "            print(f\"\\n=== Extraction Summary ===\")\n",
        "            print(f\"Total KPIs extracted: {len(df_auto)}\")\n",
        "\n",
        "            # Source statistics\n",
        "            if 'source_type' in df_auto.columns:\n",
        "                source_counts = df_auto['source_type'].value_counts()\n",
        "                print(f\"From text/tables: {source_counts.get('text', 0)}\")\n",
        "                print(f\"From images/charts: {source_counts.get('image', 0)}\")\n",
        "\n",
        "            # Theme statistics\n",
        "            if 'kpi_theme' in df_auto.columns:\n",
        "                theme_counts = df_auto['kpi_theme'].value_counts()\n",
        "                print(f\"\\nKPI Distribution by Theme:\")\n",
        "                for theme, count in theme_counts.items():\n",
        "                    print(f\"  {theme}: {count}\")\n",
        "        else:\n",
        "            print(\"\\nNo KPIs were extracted from the document.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in main execution: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "uSueutkDZowK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Auxiliary functions ============\n",
        "def install_dependencies():\n",
        "    \"\"\"Install required dependencies\"\"\"\n",
        "    try:\n",
        "        import subprocess\n",
        "        import sys\n",
        "\n",
        "        dependencies = [\n",
        "            \"openai\",\n",
        "            \"python-dotenv\",\n",
        "            \"pdfplumber\",\n",
        "            \"tiktoken\",\n",
        "            \"pandas\",\n",
        "            \"PyMuPDF\",\n",
        "            \"Pillow\",\n",
        "            \"openpyxl\"\n",
        "        ]\n",
        "\n",
        "        for dep in dependencies:\n",
        "            try:\n",
        "                __import__(dep.replace('-', '_'))\n",
        "                print(f\"✅ {dep} is already installed\")\n",
        "            except ImportError:\n",
        "                print(f\"Installing {dep}...\")\n",
        "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", dep])\n",
        "                print(f\"✅ Installed {dep}\")\n",
        "\n",
        "        # Optional Camelot installation\n",
        "        try:\n",
        "            import camelot\n",
        "            print(\"✅ Camelot is already installed\")\n",
        "        except ImportError:\n",
        "            print(\"Installing Camelot (optional)...\")\n",
        "            try:\n",
        "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"camelot-py[cv]\"])\n",
        "                print(\"✅ Installed Camelot\")\n",
        "            except:\n",
        "                print(\"⚠️ Camelot installation failed (optional dependency)\")\n",
        "\n",
        "        print(\"🎉 All dependencies checked/installed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error with dependencies: {e}\")\n",
        "\n",
        "def validate_environment():\n",
        "    \"\"\"Validate environment setup\"\"\"\n",
        "    issues = []\n",
        "\n",
        "    # Check API key\n",
        "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "        issues.append(\"OPENAI_API_KEY not found in environment variables\")\n",
        "\n",
        "    # Check PDF file\n",
        "    if not os.path.exists(PDF_PATH):\n",
        "        issues.append(f\"PDF file not found: {PDF_PATH}\")\n",
        "\n",
        "    # Check required imports\n",
        "    required_modules = ['openai', 'pdfplumber', 'pandas', 'tiktoken', 'PIL', 'fitz']\n",
        "    for module in required_modules:\n",
        "        try:\n",
        "            __import__(module)\n",
        "        except ImportError:\n",
        "            issues.append(f\"Required module '{module}' not installed\")\n",
        "\n",
        "    if issues:\n",
        "        print(\"❌ Environment validation failed:\")\n",
        "        for issue in issues:\n",
        "            print(f\"  - {issue}\")\n",
        "        return False\n",
        "    else:\n",
        "        print(\"✅ Environment validation passed\")\n",
        "        return True\n"
      ],
      "metadata": {
        "id": "qcCShhlea6ZK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Simplified execution interface ============\n",
        "def run_kpi_extraction():\n",
        "    \"\"\"Simplified interface to run KPI extraction\"\"\"\n",
        "    print(\"🚀 Starting KPI extraction process...\")\n",
        "\n",
        "    # Validate environment\n",
        "    if not validate_environment():\n",
        "        print(\"Please fix the environment issues before running.\")\n",
        "        return\n",
        "\n",
        "    # Run main function\n",
        "    main()"
      ],
      "metadata": {
        "id": "Qw6B5H8ga6bl"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Optimized execution interface ============\n",
        "def run_optimized_kpi_extraction():\n",
        "    \"\"\"Run optimized KPI extraction\"\"\"\n",
        "    print(\"⚡ Starting OPTIMIZED KPI extraction...\")\n",
        "    print(\"🎯 Goal: Extract ALL KPIs with 60-70% better performance\")\n",
        "\n",
        "    # Verify the environment\n",
        "    if not validate_environment():\n",
        "        print(\"Please fix the environment issues before running.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Run optimization process\n",
        "        df_results = process_sustainability_report_OPTIMIZED(PDF_PATH)\n",
        "\n",
        "        # Save the results\n",
        "        output_file = \"OPTIMIZED_\" + EXPORT_AUTO_XLSX\n",
        "        save_results(df_results, output_file, PDF_PATH)\n",
        "        print(f\"💾 Results saved to: {output_file}\")\n",
        "\n",
        "        # Show Statistics\n",
        "        if not df_results.empty and 'source_type' in df_results.columns:\n",
        "            source_counts = df_results['source_type'].value_counts()\n",
        "            print(f\"\\n📈 Final Statistics:\")\n",
        "            for source, count in source_counts.items():\n",
        "                print(f\"   - {source}: {count} KPIs\")\n",
        "\n",
        "        return df_results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Optimized extraction failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def compare_original_vs_optimized():\n",
        "    \"\"\"Compare the performance of the original version and the optimized version\"\"\"\n",
        "    print(\"🔬 Performance Comparison Test\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Test the original version\n",
        "    print(\"\\n📊 Testing Original Version...\")\n",
        "    original_start = time.time()\n",
        "    try:\n",
        "        original_df = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
        "        original_time = time.time() - original_start\n",
        "        print(f\"⏱️  Original version: {original_time:.1f}s, {len(original_df)} KPIs\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Original version failed: {e}\")\n",
        "        original_time = 999\n",
        "        original_df = pd.DataFrame()\n",
        "\n",
        "    # Test optimized version\n",
        "    print(\"\\n⚡ Testing Optimized Version...\")\n",
        "    optimized_start = time.time()\n",
        "    try:\n",
        "        optimized_df = process_sustainability_report_OPTIMIZED(PDF_PATH)\n",
        "        optimized_time = time.time() - optimized_start\n",
        "        print(f\"⏱️  Optimized version: {optimized_time:.1f}s, {len(optimized_df)} KPIs\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Optimized version failed: {e}\")\n",
        "        optimized_time = 999\n",
        "        optimized_df = pd.DataFrame()\n",
        "\n",
        "    # Performance comparison\n",
        "    if original_time < 999 and optimized_time < 999:\n",
        "        speedup = original_time / optimized_time\n",
        "        time_saved = original_time - optimized_time\n",
        "        kpi_diff = abs(len(optimized_df) - len(original_df))\n",
        "\n",
        "        print(f\"\\n🚀 Performance Results:\")\n",
        "        print(f\"   - Speed improvement: {speedup:.1f}x faster\")\n",
        "        print(f\"   - Time saved: {time_saved:.1f}s ({time_saved/60:.1f}min)\")\n",
        "        print(f\"   - KPI difference: {kpi_diff} KPIs\")\n",
        "        print(f\"   - Completeness: {len(optimized_df)/len(original_df)*100:.1f}% of original\" if len(original_df) > 0 else \"\")\n",
        "\n",
        "        return {\"original\": original_df, \"optimized\": optimized_df, \"speedup\": speedup}\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "aJrVHLTgmPJw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KPIValidationPipeline:\n",
        "    \"\"\"Comprehensive KPI validation and evaluation system\"\"\"\n",
        "\n",
        "    def __init__(self, manual_excel_path: str, auto_excel_path: str,\n",
        "                 output_dir: str = \"validation_results\"):\n",
        "        \"\"\"\n",
        "        Initialize validation pipeline\n",
        "\n",
        "        Args:\n",
        "            manual_excel_path: Path to manual KPI annotations\n",
        "            auto_excel_path: Path to automatically extracted KPIs\n",
        "            output_dir: Directory to save validation results\n",
        "        \"\"\"\n",
        "        self.manual_path = manual_excel_path\n",
        "        self.auto_path = auto_excel_path\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Load data\n",
        "        self.manual_df = self._load_excel_safe(manual_excel_path, \"manual\")\n",
        "        self.auto_df = self._load_excel_safe(auto_excel_path, \"auto\")\n",
        "\n",
        "        # Validation results\n",
        "        self.validation_results = {}\n",
        "        self.detailed_analysis = {}\n",
        "\n",
        "        # Similarity thresholds\n",
        "        self.similarity_thresholds = {\n",
        "            'exact': 1.0,\n",
        "            'high': 0.9,\n",
        "            'medium': 0.7,\n",
        "            'low': 0.5\n",
        "        }\n",
        "\n",
        "        logging.info(f\"Validation pipeline initialized:\")\n",
        "        logging.info(f\"  Manual KPIs: {len(self.manual_df)}\")\n",
        "        logging.info(f\"  Auto KPIs: {len(self.auto_df)}\")\n",
        "\n",
        "    def _load_excel_safe(self, filepath: str, source_type: str) -> pd.DataFrame:\n",
        "        \"\"\"Safely load Excel file with error handling\"\"\"\n",
        "        try:\n",
        "            if not Path(filepath).exists():\n",
        "                logging.warning(f\"{source_type.title()} file not found: {filepath}\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            df = pd.read_excel(filepath)\n",
        "            logging.info(f\"Loaded {source_type} file: {len(df)} rows\")\n",
        "\n",
        "            # Standardize column names\n",
        "            df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "            # Ensure required columns exist\n",
        "            required_cols = ['kpi_text']\n",
        "            for col in required_cols:\n",
        "                if col not in df.columns:\n",
        "                    # Try to find similar column names\n",
        "                    similar_cols = [c for c in df.columns if 'kpi' in c.lower() or 'text' in c.lower()]\n",
        "                    if similar_cols:\n",
        "                        df['kpi_text'] = df[similar_cols[0]]\n",
        "                        logging.info(f\"Using column '{similar_cols[0]}' as kpi_text\")\n",
        "                    else:\n",
        "                        logging.warning(f\"Required column '{col}' not found in {source_type} file\")\n",
        "                        df['kpi_text'] = \"\"\n",
        "\n",
        "            # Clean text data\n",
        "            df['kpi_text'] = df['kpi_text'].astype(str).str.strip()\n",
        "            df = df[df['kpi_text'] != ''].reset_index(drop=True)\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading {source_type} file: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def normalize_text(self, text: str) -> str:\n",
        "        \"\"\"Normalize text for comparison\"\"\"\n",
        "        if pd.isna(text) or text == '':\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to string and lowercase\n",
        "        text = str(text).lower().strip()\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        # Remove common punctuation but keep percentages and numbers\n",
        "        text = re.sub(r'[^\\w\\s\\%\\.\\,\\-]', ' ', text)\n",
        "\n",
        "        # Normalize number formats\n",
        "        text = re.sub(r'\\b(\\d+),(\\d+)\\b', r'\\1\\2', text)  # Remove commas in numbers\n",
        "        text = re.sub(r'\\s+', ' ', text)  # Normalize spaces\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def calculate_text_similarity(self, text1: str, text2: str) -> Dict[str, float]:\n",
        "        \"\"\"Calculate multiple similarity metrics between two texts\"\"\"\n",
        "        norm1 = self.normalize_text(text1)\n",
        "        norm2 = self.normalize_text(text2)\n",
        "\n",
        "        if not norm1 or not norm2:\n",
        "            return {'sequence': 0.0, 'cosine': 0.0, 'jaccard': 0.0, 'combined': 0.0}\n",
        "\n",
        "        # 1. Sequence similarity (exact match)\n",
        "        sequence_sim = SequenceMatcher(None, norm1, norm2).ratio()\n",
        "\n",
        "        # 2. Cosine similarity (semantic)\n",
        "        try:\n",
        "            vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=1)\n",
        "            tfidf_matrix = vectorizer.fit_transform([norm1, norm2])\n",
        "            cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "        except:\n",
        "            cosine_sim = 0.0\n",
        "\n",
        "        # 3. Jaccard similarity (token overlap)\n",
        "        tokens1 = set(norm1.split())\n",
        "        tokens2 = set(norm2.split())\n",
        "        if tokens1 or tokens2:\n",
        "            jaccard_sim = len(tokens1.intersection(tokens2)) / len(tokens1.union(tokens2))\n",
        "        else:\n",
        "            jaccard_sim = 0.0\n",
        "\n",
        "        # 4. Combined similarity\n",
        "        combined_sim = (sequence_sim * 0.4 + cosine_sim * 0.4 + jaccard_sim * 0.2)\n",
        "\n",
        "        return {\n",
        "            'sequence': sequence_sim,\n",
        "            'cosine': cosine_sim,\n",
        "            'jaccard': jaccard_sim,\n",
        "            'combined': combined_sim\n",
        "        }\n",
        "\n",
        "    def find_matches(self, threshold: float = 0.7, similarity_type: str = 'combined') -> pd.DataFrame:\n",
        "        \"\"\"Find matches between manual and auto KPIs\"\"\"\n",
        "        matches = []\n",
        "        auto_matched = set()\n",
        "\n",
        "        for manual_idx, manual_row in self.manual_df.iterrows():\n",
        "            manual_text = manual_row['kpi_text']\n",
        "            best_match = None\n",
        "            best_similarity = 0.0\n",
        "\n",
        "            for auto_idx, auto_row in self.auto_df.iterrows():\n",
        "                if auto_idx in auto_matched:\n",
        "                    continue\n",
        "\n",
        "                auto_text = auto_row['kpi_text']\n",
        "                similarities = self.calculate_text_similarity(manual_text, auto_text)\n",
        "                similarity = similarities[similarity_type]\n",
        "\n",
        "                if similarity > best_similarity and similarity >= threshold:\n",
        "                    best_similarity = similarity\n",
        "                    best_match = {\n",
        "                        'manual_idx': manual_idx,\n",
        "                        'auto_idx': auto_idx,\n",
        "                        'manual_text': manual_text,\n",
        "                        'auto_text': auto_text,\n",
        "                        'similarity': similarity,\n",
        "                        'all_similarities': similarities\n",
        "                    }\n",
        "\n",
        "            if best_match:\n",
        "                matches.append(best_match)\n",
        "                auto_matched.add(best_match['auto_idx'])\n",
        "\n",
        "        return pd.DataFrame(matches)\n",
        "\n",
        "    def calculate_metrics_at_threshold(self, threshold: float = 0.7,\n",
        "                                     similarity_type: str = 'combined') -> Dict[str, float]:\n",
        "        \"\"\"Calculate precision, recall, F1 at specific threshold\"\"\"\n",
        "        matches_df = self.find_matches(threshold, similarity_type)\n",
        "\n",
        "        true_positives = len(matches_df)\n",
        "        false_positives = len(self.auto_df) - true_positives\n",
        "        false_negatives = len(self.manual_df) - true_positives\n",
        "\n",
        "        # Calculate metrics\n",
        "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0\n",
        "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "        return {\n",
        "            'threshold': threshold,\n",
        "            'similarity_type': similarity_type,\n",
        "            'true_positives': true_positives,\n",
        "            'false_positives': false_positives,\n",
        "            'false_negatives': false_negatives,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1_score,\n",
        "            'total_manual': len(self.manual_df),\n",
        "            'total_auto': len(self.auto_df),\n",
        "            'match_rate': true_positives / len(self.manual_df) if len(self.manual_df) > 0 else 0.0\n",
        "        }\n",
        "\n",
        "    def run_comprehensive_evaluation(self) -> Dict[str, any]:\n",
        "        \"\"\"Run comprehensive evaluation across multiple thresholds and similarity types\"\"\"\n",
        "        logging.info(\"Running comprehensive evaluation...\")\n",
        "\n",
        "        results = {\n",
        "            'threshold_analysis': [],\n",
        "            'similarity_type_analysis': [],\n",
        "            'category_analysis': {},\n",
        "            'detailed_matches': {},\n",
        "            'false_positives': [],\n",
        "            'false_negatives': []\n",
        "        }\n",
        "\n",
        "        # 1. Threshold analysis\n",
        "        thresholds = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "        similarity_types = ['combined', 'sequence', 'cosine', 'jaccard']\n",
        "\n",
        "        for threshold in thresholds:\n",
        "            for sim_type in similarity_types:\n",
        "                metrics = self.calculate_metrics_at_threshold(threshold, sim_type)\n",
        "                results['threshold_analysis'].append(metrics)\n",
        "\n",
        "        # 2. Find optimal threshold\n",
        "        best_f1 = 0.0\n",
        "        best_config = None\n",
        "        for metrics in results['threshold_analysis']:\n",
        "            if metrics['f1_score'] > best_f1:\n",
        "                best_f1 = metrics['f1_score']\n",
        "                best_config = (metrics['threshold'], metrics['similarity_type'])\n",
        "\n",
        "        # 3. Detailed analysis at optimal threshold\n",
        "        if best_config:\n",
        "            optimal_threshold, optimal_sim_type = best_config\n",
        "            logging.info(f\"Optimal configuration: threshold={optimal_threshold}, similarity={optimal_sim_type}\")\n",
        "\n",
        "            matches_df = self.find_matches(optimal_threshold, optimal_sim_type)\n",
        "            results['detailed_matches'] = matches_df.to_dict('records')\n",
        "\n",
        "            # Find false positives and false negatives\n",
        "            matched_auto_indices = set(matches_df['auto_idx'].tolist()) if not matches_df.empty else set()\n",
        "            matched_manual_indices = set(matches_df['manual_idx'].tolist()) if not matches_df.empty else set()\n",
        "\n",
        "            # False positives (auto KPIs not matched to manual)\n",
        "            fp_indices = set(range(len(self.auto_df))) - matched_auto_indices\n",
        "            results['false_positives'] = [\n",
        "                {\n",
        "                    'index': idx,\n",
        "                    'kpi_text': self.auto_df.iloc[idx]['kpi_text'],\n",
        "                    'category': self.auto_df.iloc[idx].get('kpi_category', 'Unknown'),\n",
        "                    'theme': self.auto_df.iloc[idx].get('kpi_theme', 'Unknown'),\n",
        "                    'source': self.auto_df.iloc[idx].get('source_type', 'Unknown')\n",
        "                }\n",
        "                for idx in fp_indices\n",
        "            ]\n",
        "\n",
        "            # False negatives (manual KPIs not matched by auto)\n",
        "            fn_indices = set(range(len(self.manual_df))) - matched_manual_indices\n",
        "            results['false_negatives'] = [\n",
        "                {\n",
        "                    'index': idx,\n",
        "                    'kpi_text': self.manual_df.iloc[idx]['kpi_text'],\n",
        "                    'category': self.manual_df.iloc[idx].get('kpi_category', 'Unknown'),\n",
        "                    'theme': self.manual_df.iloc[idx].get('kpi_theme', 'Unknown')\n",
        "                }\n",
        "                for idx in fn_indices\n",
        "            ]\n",
        "\n",
        "        # 4. Category-level analysis\n",
        "        if 'kpi_category' in self.manual_df.columns and 'kpi_category' in self.auto_df.columns:\n",
        "            results['category_analysis'] = self._analyze_by_category()\n",
        "\n",
        "        # 5. Theme-level analysis\n",
        "        if 'kpi_theme' in self.manual_df.columns and 'kpi_theme' in self.auto_df.columns:\n",
        "            results['theme_analysis'] = self._analyze_by_theme()\n",
        "\n",
        "        self.validation_results = results\n",
        "        return results\n",
        "\n",
        "    def _analyze_by_category(self) -> Dict[str, Dict]:\n",
        "        \"\"\"Analyze performance by KPI category\"\"\"\n",
        "        category_results = {}\n",
        "\n",
        "        manual_categories = self.manual_df['kpi_category'].value_counts()\n",
        "        auto_categories = self.auto_df['kpi_category'].value_counts()\n",
        "\n",
        "        all_categories = set(manual_categories.index) | set(auto_categories.index)\n",
        "\n",
        "        for category in all_categories:\n",
        "            manual_count = manual_categories.get(category, 0)\n",
        "            auto_count = auto_categories.get(category, 0)\n",
        "\n",
        "            # Find matches within this category\n",
        "            manual_cat_df = self.manual_df[self.manual_df['kpi_category'] == category]\n",
        "            auto_cat_df = self.auto_df[self.auto_df['kpi_category'] == category]\n",
        "\n",
        "            category_matches = 0\n",
        "            if not manual_cat_df.empty and not auto_cat_df.empty:\n",
        "                for _, manual_row in manual_cat_df.iterrows():\n",
        "                    best_sim = 0.0\n",
        "                    for _, auto_row in auto_cat_df.iterrows():\n",
        "                        sim = self.calculate_text_similarity(\n",
        "                            manual_row['kpi_text'],\n",
        "                            auto_row['kpi_text']\n",
        "                        )['combined']\n",
        "                        best_sim = max(best_sim, sim)\n",
        "                    if best_sim >= 0.7:\n",
        "                        category_matches += 1\n",
        "\n",
        "            category_precision = category_matches / auto_count if auto_count > 0 else 0.0\n",
        "            category_recall = category_matches / manual_count if manual_count > 0 else 0.0\n",
        "            category_f1 = 2 * (category_precision * category_recall) / (category_precision + category_recall) if (category_precision + category_recall) > 0 else 0.0\n",
        "\n",
        "            category_results[category] = {\n",
        "                'manual_count': manual_count,\n",
        "                'auto_count': auto_count,\n",
        "                'matches': category_matches,\n",
        "                'precision': category_precision,\n",
        "                'recall': category_recall,\n",
        "                'f1_score': category_f1\n",
        "            }\n",
        "\n",
        "        return category_results\n",
        "\n",
        "    def _analyze_by_theme(self) -> Dict[str, Dict]:\n",
        "        \"\"\"Analyze performance by KPI theme\"\"\"\n",
        "        theme_results = {}\n",
        "\n",
        "        manual_themes = self.manual_df['kpi_theme'].value_counts()\n",
        "        auto_themes = self.auto_df['kpi_theme'].value_counts()\n",
        "\n",
        "        all_themes = set(manual_themes.index) | set(auto_themes.index)\n",
        "\n",
        "        for theme in all_themes:\n",
        "            manual_count = manual_themes.get(theme, 0)\n",
        "            auto_count = auto_themes.get(theme, 0)\n",
        "\n",
        "            theme_results[theme] = {\n",
        "                'manual_count': manual_count,\n",
        "                'auto_count': auto_count,\n",
        "                'coverage': auto_count / manual_count if manual_count > 0 else 0.0\n",
        "            }\n",
        "\n",
        "        return theme_results\n",
        "\n",
        "    def generate_visualizations(self):\n",
        "        \"\"\"Generate comprehensive visualizations\"\"\"\n",
        "        if not self.validation_results:\n",
        "            logging.warning(\"No validation results found. Run evaluation first.\")\n",
        "            return\n",
        "\n",
        "        # Set style\n",
        "        try:\n",
        "            plt.style.use('seaborn-v0_8')\n",
        "        except:\n",
        "            plt.style.use('seaborn')  # 备用样式\n",
        "        fig = plt.figure(figsize=(20, 16))\n",
        "\n",
        "        # 1. Threshold analysis\n",
        "        threshold_df = pd.DataFrame(self.validation_results['threshold_analysis'])\n",
        "\n",
        "        plt.subplot(3, 3, 1)\n",
        "        for sim_type in threshold_df['similarity_type'].unique():\n",
        "            data = threshold_df[threshold_df['similarity_type'] == sim_type]\n",
        "            plt.plot(data['threshold'], data['f1_score'], marker='o', label=sim_type)\n",
        "        plt.xlabel('Similarity Threshold')\n",
        "        plt.ylabel('F1 Score')\n",
        "        plt.title('F1 Score vs Threshold by Similarity Type')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 2. Precision-Recall curve\n",
        "        plt.subplot(3, 3, 2)\n",
        "        for sim_type in threshold_df['similarity_type'].unique():\n",
        "            data = threshold_df[threshold_df['similarity_type'] == sim_type]\n",
        "            plt.plot(data['recall'], data['precision'], marker='o', label=sim_type)\n",
        "        plt.xlabel('Recall')\n",
        "        plt.ylabel('Precision')\n",
        "        plt.title('Precision-Recall Curves')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 3. Category analysis\n",
        "        if 'category_analysis' in self.validation_results:\n",
        "            plt.subplot(3, 3, 3)\n",
        "            cat_analysis = self.validation_results['category_analysis']\n",
        "            categories = list(cat_analysis.keys())[:10]  # Top 10 categories\n",
        "            f1_scores = [cat_analysis[cat]['f1_score'] for cat in categories]\n",
        "\n",
        "            plt.barh(categories, f1_scores)\n",
        "            plt.xlabel('F1 Score')\n",
        "            plt.title('F1 Score by Category (Top 10)')\n",
        "            plt.tight_layout()\n",
        "\n",
        "        # 4. Theme distribution comparison\n",
        "        plt.subplot(3, 3, 4)\n",
        "        if 'kpi_theme' in self.manual_df.columns:\n",
        "            manual_themes = self.manual_df['kpi_theme'].value_counts()\n",
        "            auto_themes = self.auto_df['kpi_theme'].value_counts()\n",
        "\n",
        "            x = np.arange(len(manual_themes))\n",
        "            width = 0.35\n",
        "\n",
        "            plt.bar(x - width/2, manual_themes.values, width, label='Manual', alpha=0.8)\n",
        "            plt.bar(x + width/2, auto_themes.reindex(manual_themes.index, fill_value=0).values,\n",
        "                   width, label='Auto', alpha=0.8)\n",
        "\n",
        "            plt.xlabel('Theme')\n",
        "            plt.ylabel('Count')\n",
        "            plt.title('KPI Count by Theme')\n",
        "            plt.xticks(x, manual_themes.index, rotation=45)\n",
        "            plt.legend()\n",
        "\n",
        "        # 5. Similarity distribution\n",
        "        plt.subplot(3, 3, 5)\n",
        "        if self.validation_results['detailed_matches']:\n",
        "            similarities = [match['similarity'] for match in self.validation_results['detailed_matches']]\n",
        "            plt.hist(similarities, bins=20, edgecolor='black', alpha=0.7)\n",
        "            plt.xlabel('Similarity Score')\n",
        "            plt.ylabel('Frequency')\n",
        "            plt.title('Distribution of Similarity Scores (Matches)')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 6. Error analysis\n",
        "        plt.subplot(3, 3, 6)\n",
        "        fp_count = len(self.validation_results['false_positives'])\n",
        "        fn_count = len(self.validation_results['false_negatives'])\n",
        "        tp_count = len(self.validation_results['detailed_matches'])\n",
        "\n",
        "        labels = ['True Positives', 'False Positives', 'False Negatives']\n",
        "        counts = [tp_count, fp_count, fn_count]\n",
        "        colors = ['green', 'red', 'orange']\n",
        "\n",
        "        plt.pie(counts, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "        plt.title('Classification Results')\n",
        "\n",
        "        # 7. Coverage by source type\n",
        "        plt.subplot(3, 3, 7)\n",
        "        if 'source_type' in self.auto_df.columns:\n",
        "            source_counts = self.auto_df['source_type'].value_counts()\n",
        "            plt.pie(source_counts.values, labels=source_counts.index, autopct='%1.1f%%')\n",
        "            plt.title('Auto KPIs by Source Type')\n",
        "\n",
        "        # 8. Performance metrics summary\n",
        "        plt.subplot(3, 3, 8)\n",
        "        best_metrics = max(self.validation_results['threshold_analysis'],\n",
        "                          key=lambda x: x['f1_score'])\n",
        "\n",
        "        metrics = ['Precision', 'Recall', 'F1 Score']\n",
        "        values = [best_metrics['precision'], best_metrics['recall'], best_metrics['f1_score']]\n",
        "\n",
        "        bars = plt.bar(metrics, values, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
        "        plt.ylabel('Score')\n",
        "        plt.title(f'Best Performance Metrics\\n(Threshold: {best_metrics[\"threshold\"]})')\n",
        "        plt.ylim(0, 1)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar, value in zip(bars, values):\n",
        "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                    f'{value:.3f}', ha='center', va='bottom')\n",
        "\n",
        "        # 9. Match quality distribution\n",
        "        plt.subplot(3, 3, 9)\n",
        "        if self.validation_results['detailed_matches']:\n",
        "            match_similarities = [match['similarity'] for match in self.validation_results['detailed_matches']]\n",
        "            quality_bins = [0.5, 0.7, 0.8, 0.9, 1.0]\n",
        "            quality_labels = ['Medium', 'Good', 'Very Good', 'Excellent']\n",
        "\n",
        "            quality_counts = []\n",
        "            for i in range(len(quality_bins)-1):\n",
        "                count = sum(1 for sim in match_similarities\n",
        "                          if quality_bins[i] <= sim < quality_bins[i+1])\n",
        "                quality_counts.append(count)\n",
        "\n",
        "            plt.bar(quality_labels, quality_counts, color='lightblue', edgecolor='black')\n",
        "            plt.ylabel('Number of Matches')\n",
        "            plt.title('Match Quality Distribution')\n",
        "            plt.xticks(rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save visualization\n",
        "        viz_path = self.output_dir / \"validation_visualizations.png\"\n",
        "        plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        logging.info(f\"Visualizations saved to {viz_path}\")\n",
        "\n",
        "    def generate_detailed_report(self) -> str:\n",
        "        \"\"\"Generate comprehensive validation report\"\"\"\n",
        "        if not self.validation_results:\n",
        "            logging.warning(\"No validation results found. Run evaluation first.\")\n",
        "            return \"\"\n",
        "\n",
        "        # Find best configuration\n",
        "        best_metrics = max(self.validation_results['threshold_analysis'],\n",
        "                          key=lambda x: x['f1_score'])\n",
        "\n",
        "        report = f\"\"\"\n",
        "# KPI Extraction Validation Report\n",
        "Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "## Dataset Overview\n",
        "- **Manual KPIs**: {len(self.manual_df)} annotations\n",
        "- **Auto KPIs**: {len(self.auto_df)} extractions\n",
        "- **Manual file**: {self.manual_path}\n",
        "- **Auto file**: {self.auto_path}\n",
        "\n",
        "## Best Performance Configuration\n",
        "- **Similarity Type**: {best_metrics['similarity_type']}\n",
        "- **Threshold**: {best_metrics['threshold']}\n",
        "- **Precision**: {best_metrics['precision']:.3f}\n",
        "- **Recall**: {best_metrics['recall']:.3f}\n",
        "- **F1 Score**: {best_metrics['f1_score']:.3f}\n",
        "\n",
        "## Detailed Metrics\n",
        "- **True Positives**: {best_metrics['true_positives']}\n",
        "- **False Positives**: {best_metrics['false_positives']}\n",
        "- **False Negatives**: {best_metrics['false_negatives']}\n",
        "- **Match Rate**: {best_metrics['match_rate']:.3f}\n",
        "\n",
        "## Error Analysis\n",
        "\n",
        "### False Positives ({len(self.validation_results['false_positives'])})\n",
        "KPIs extracted automatically but not in manual annotations:\n",
        "\"\"\"\n",
        "\n",
        "        # Add false positives\n",
        "        for i, fp in enumerate(self.validation_results['false_positives'][:10], 1):\n",
        "            report += f\"\\n{i}. **{fp['category']}** | {fp['kpi_text']}\\n\"\n",
        "\n",
        "        if len(self.validation_results['false_positives']) > 10:\n",
        "            report += f\"\\n... and {len(self.validation_results['false_positives']) - 10} more\\n\"\n",
        "\n",
        "        report += f\"\"\"\n",
        "### False Negatives ({len(self.validation_results['false_negatives'])})\n",
        "KPIs in manual annotations but missed by extraction:\n",
        "\"\"\"\n",
        "\n",
        "        # Add false negatives\n",
        "        for i, fn in enumerate(self.validation_results['false_negatives'][:10], 1):\n",
        "            report += f\"\\n{i}. **{fn['category']}** | {fn['kpi_text']}\\n\"\n",
        "\n",
        "        if len(self.validation_results['false_negatives']) > 10:\n",
        "            report += f\"\\n... and {len(self.validation_results['false_negatives']) - 10} more\\n\"\n",
        "\n",
        "        # Category analysis\n",
        "        if 'category_analysis' in self.validation_results:\n",
        "            report += \"\\n## Category-wise Performance\\n\\n\"\n",
        "            report += \"| Category | Manual | Auto | Matches | Precision | Recall | F1 |\\n\"\n",
        "            report += \"|----------|--------|------|---------|-----------|--------|----|\\\\n\"\n",
        "\n",
        "            for category, metrics in self.validation_results['category_analysis'].items():\n",
        "                report += f\"| {category[:20]} | {metrics['manual_count']} | {metrics['auto_count']} | {metrics['matches']} | {metrics['precision']:.3f} | {metrics['recall']:.3f} | {metrics['f1_score']:.3f} |\\n\"\n",
        "\n",
        "        # Theme analysis\n",
        "        if 'theme_analysis' in self.validation_results:\n",
        "            report += \"\\n## Theme-wise Coverage\\n\\n\"\n",
        "            report += \"| Theme | Manual Count | Auto Count | Coverage |\\n\"\n",
        "            report += \"|-------|--------------|------------|----------|\\n\"\n",
        "\n",
        "            for theme, metrics in self.validation_results['theme_analysis'].items():\n",
        "                report += f\"| {theme} | {metrics['manual_count']} | {metrics['auto_count']} | {metrics['coverage']:.3f} |\\n\"\n",
        "\n",
        "        # Recommendations\n",
        "        report += f\"\"\"\n",
        "## Recommendations\n",
        "\n",
        "### Strengths\n",
        "- Overall F1 Score: {best_metrics['f1_score']:.3f}\n",
        "- Precision: {best_metrics['precision']:.3f} (low false positive rate)\n",
        "- Recall: {best_metrics['recall']:.3f} (good coverage)\n",
        "\n",
        "### Areas for Improvement\n",
        "\"\"\"\n",
        "\n",
        "        if best_metrics['precision'] < 0.8:\n",
        "            report += \"- **Precision**: Consider stricter filtering to reduce false positives\\n\"\n",
        "\n",
        "        if best_metrics['recall'] < 0.8:\n",
        "            report += \"- **Recall**: Improve extraction to catch more manual KPIs\\n\"\n",
        "\n",
        "        if best_metrics['f1_score'] < 0.7:\n",
        "            report += \"- **Overall Performance**: Significant room for improvement in both precision and recall\\n\"\n",
        "\n",
        "        # Source-specific recommendations\n",
        "        if 'source_type' in self.auto_df.columns:\n",
        "            text_kpis = len(self.auto_df[self.auto_df['source_type'] == 'text'])\n",
        "            image_kpis = len(self.auto_df[self.auto_df['source_type'] == 'image'])\n",
        "\n",
        "            report += f\"\"\"\n",
        "### Source Type Analysis\n",
        "- **Text/Table KPIs**: {text_kpis}\n",
        "- **Image/Chart KPIs**: {image_kpis}\n",
        "- **Image Coverage**: {image_kpis / (text_kpis + image_kpis) * 100:.1f}%\n",
        "\"\"\"\n",
        "\n",
        "        return report\n",
        "\n",
        "    def save_results(self):\n",
        "        \"\"\"Save all validation results to files\"\"\"\n",
        "        # Save detailed results as JSON\n",
        "        results_path = self.output_dir / \"validation_results.json\"\n",
        "        with open(results_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.validation_results, f, indent=2, ensure_ascii=False, default=str)\n",
        "\n",
        "        # Save matches as Excel\n",
        "        if self.validation_results['detailed_matches']:\n",
        "            matches_df = pd.DataFrame(self.validation_results['detailed_matches'])\n",
        "            matches_path = self.output_dir / \"detailed_matches.xlsx\"\n",
        "            matches_df.to_excel(matches_path, index=False)\n",
        "\n",
        "        # Save false positives and negatives\n",
        "        fp_df = pd.DataFrame(self.validation_results['false_positives'])\n",
        "        fn_df = pd.DataFrame(self.validation_results['false_negatives'])\n",
        "\n",
        "        with pd.ExcelWriter(self.output_dir / \"error_analysis.xlsx\") as writer:\n",
        "            fp_df.to_excel(writer, sheet_name='False_Positives', index=False)\n",
        "            fn_df.to_excel(writer, sheet_name='False_Negatives', index=False)\n",
        "\n",
        "        # Save report\n",
        "        report = self.generate_detailed_report()\n",
        "        report_path = self.output_dir / \"validation_report.md\"\n",
        "        with open(report_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(report)\n",
        "\n",
        "        # Save metrics summary\n",
        "        threshold_df = pd.DataFrame(self.validation_results['threshold_analysis'])\n",
        "        threshold_df.to_excel(self.output_dir / \"threshold_analysis.xlsx\", index=False)\n",
        "\n",
        "        logging.info(f\"All validation results saved to {self.output_dir}\")\n",
        "\n",
        "        return {\n",
        "            'results_json': results_path,\n",
        "            'matches_excel': self.output_dir / \"detailed_matches.xlsx\",\n",
        "            'error_analysis': self.output_dir / \"error_analysis.xlsx\",\n",
        "            'report_markdown': report_path,\n",
        "            'threshold_analysis': self.output_dir / \"threshold_analysis.xlsx\",\n",
        "            'visualizations': self.output_dir / \"validation_visualizations.png\"\n",
        "        }\n",
        "\n",
        "    def run_full_validation(self) -> Dict[str, any]:\n",
        "        \"\"\"Run complete validation pipeline\"\"\"\n",
        "        logging.info(\"Starting full validation pipeline...\")\n",
        "\n",
        "        # Step 1: Run comprehensive evaluation\n",
        "        self.run_comprehensive_evaluation()\n",
        "\n",
        "        # Step 2: Generate visualizations\n",
        "        self.generate_visualizations()\n",
        "\n",
        "        # Step 3: Save all results\n",
        "        saved_files = self.save_results()\n",
        "\n",
        "        # Step 4: Print summary\n",
        "        best_metrics = max(self.validation_results['threshold_analysis'],\n",
        "                          key=lambda x: x['f1_score'])\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"KPI EXTRACTION VALIDATION SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"📊 Dataset: {len(self.manual_df)} manual vs {len(self.auto_df)} auto KPIs\")\n",
        "        print(f\"🎯 Best F1 Score: {best_metrics['f1_score']:.3f}\")\n",
        "        print(f\"📈 Precision: {best_metrics['precision']:.3f}\")\n",
        "        print(f\"📉 Recall: {best_metrics['recall']:.3f}\")\n",
        "        print(f\"✅ True Positives: {best_metrics['true_positives']}\")\n",
        "        print(f\"❌ False Positives: {best_metrics['false_positives']}\")\n",
        "        print(f\"⚠️  False Negatives: {best_metrics['false_negatives']}\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"📁 Results saved to: {self.output_dir}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        return {\n",
        "            'validation_results': self.validation_results,\n",
        "            'saved_files': saved_files,\n",
        "            'best_metrics': best_metrics\n",
        "        }\n"
      ],
      "metadata": {
        "id": "UfLJWwsd4nDX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ 兼容性函数 ============\n",
        "def extract_kpi_from_chunk(chunk: str) -> List[Dict]:\n",
        "    \"\"\"Backward compatibility function\"\"\"\n",
        "    return extract_kpi_from_chunk_universal(chunk)\n",
        "\n",
        "def process_sustainability_report(pdf_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Backward compatibility function for text-only processing\"\"\"\n",
        "    return process_text_only()\n",
        "\n",
        "def process_sustainability_report_with_images(pdf_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Backward compatibility function for full processing\"\"\"\n",
        "    return process_sustainability_report_with_enhanced_images(pdf_path)\n"
      ],
      "metadata": {
        "id": "uhyRnexga6fA"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ 使用示例 ============\n",
        "def example_usage():\n",
        "    \"\"\"Usage examples\"\"\"\n",
        "    print(\"=== KPI Extraction Tool Usage Examples ===\\n\")\n",
        "\n",
        "    print(\"1. Full extraction (text + images):\")\n",
        "    print(\"   df_results = process_sustainability_report_with_enhanced_images(PDF_PATH)\")\n",
        "    print(\"   save_results(df_results, EXPORT_AUTO_XLSX, PDF_PATH)\\n\")\n",
        "\n",
        "    print(\"2. Text-only extraction:\")\n",
        "    print(\"   df_results = process_text_only()\")\n",
        "    print(\"   # Results automatically saved\\n\")\n",
        "\n",
        "    print(\"3. Simple run:\")\n",
        "    print(\"   run_kpi_extraction()  # Complete pipeline with validation\\n\")\n",
        "\n",
        "    print(\"4. Debug single component:\")\n",
        "    print(\"   test_text_extraction_only()  # Test first 3 chunks\")\n",
        "    print(\"   debug_single_image_analysis('path/to/image.jpg')\\n\")\n",
        "\n",
        "    print(\"5. Install dependencies:\")\n",
        "    print(\"   install_dependencies()  # Install all required packages\\n\")\n"
      ],
      "metadata": {
        "id": "fldjMEkaa6g1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MetadataValidationExtension:\n",
        "    \"\"\"元数据验证扩展 - 验证KPI文本以外的字段准确性\"\"\"\n",
        "\n",
        "    def __init__(self, validation_pipeline: KPIValidationPipeline):\n",
        "        \"\"\"\n",
        "        基于现有的KPIValidationPipeline扩展元数据验证功能\n",
        "\n",
        "        Args:\n",
        "            validation_pipeline: 现有的验证管道实例\n",
        "        \"\"\"\n",
        "        self.main_validator = validation_pipeline\n",
        "        self.metadata_results = {}\n",
        "\n",
        "        # 需要验证的元数据字段配置\n",
        "        self.metadata_fields = {\n",
        "            # 文档相关字段\n",
        "            'document_fields': {\n",
        "                'PDF file name': {'weight': 0.05, 'type': 'exact'},\n",
        "                'Title of the report': {'weight': 0.05, 'type': 'exact'},\n",
        "                'Absolute Page Number': {'weight': 0.15, 'type': 'exact'},  # 您提到的问题字段\n",
        "                'Impacted Stakeholder': {'weight': 0.10, 'type': 'similarity'}\n",
        "            },\n",
        "\n",
        "            # KPI分类字段\n",
        "            'classification_fields': {\n",
        "                'kpi_theme': {'weight': 0.15, 'type': 'exact'},  # 您提到的问题字段\n",
        "                'kpi_category': {'weight': 0.15, 'type': 'similarity'}\n",
        "            },\n",
        "\n",
        "            # 数值相关字段\n",
        "            'quantitative_fields': {\n",
        "                'quantitative_value': {'weight': 0.20, 'type': 'numerical'},  # 您提到的问题字段\n",
        "                'unit': {'weight': 0.10, 'type': 'similarity'},\n",
        "                'time_period': {'weight': 0.05, 'type': 'similarity'}\n",
        "            },\n",
        "\n",
        "            # 其他分析字段\n",
        "            'analysis_fields': {\n",
        "                'target_or_actual': {'weight': 0.05, 'type': 'exact'},\n",
        "                'source_type': {'weight': 0.05, 'type': 'exact'},\n",
        "                'chart_type': {'weight': 0.03, 'type': 'similarity'},\n",
        "                'estimation_confidence': {'weight': 0.02, 'type': 'exact'},\n",
        "                'chart_title': {'weight': 0.03, 'type': 'similarity'},\n",
        "                'data_source': {'weight': 0.02, 'type': 'similarity'},\n",
        "                'image_type': {'weight': 0.02, 'type': 'exact'}\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # 问题字段标记（您特别关注的字段）\n",
        "        self.problematic_fields = {\n",
        "            'Absolute Page Number': 'page_number_issues',\n",
        "            'kpi_theme': 'theme_classification_issues',\n",
        "            'quantitative_value': 'value_extraction_issues'\n",
        "        }\n",
        "\n",
        "        logging.info(\"元数据验证扩展初始化完成\")\n",
        "\n",
        "    def validate_single_field(self, manual_value, auto_value, field_name: str, validation_type: str) -> Dict[str, any]:\n",
        "        \"\"\"\n",
        "        验证单个字段的准确性\n",
        "\n",
        "        Args:\n",
        "            manual_value: 手动标注值\n",
        "            auto_value: 自动提取值\n",
        "            field_name: 字段名称\n",
        "            validation_type: 验证类型 ('exact', 'similarity', 'numerical')\n",
        "\n",
        "        Returns:\n",
        "            验证结果字典\n",
        "        \"\"\"\n",
        "        result = {\n",
        "            'field_name': field_name,\n",
        "            'manual_value': manual_value,\n",
        "            'auto_value': auto_value,\n",
        "            'validation_type': validation_type,\n",
        "            'is_correct': False,\n",
        "            'score': 0.0,\n",
        "            'error_type': None,\n",
        "            'notes': \"\"\n",
        "        }\n",
        "\n",
        "        # 处理空值情况\n",
        "        manual_clean = str(manual_value).strip() if pd.notna(manual_value) else \"\"\n",
        "        auto_clean = str(auto_value).strip() if pd.notna(auto_value) else \"\"\n",
        "\n",
        "        if not manual_clean and not auto_clean:\n",
        "            result.update({'is_correct': True, 'score': 1.0, 'notes': 'Both values empty'})\n",
        "            return result\n",
        "        elif not manual_clean or not auto_clean:\n",
        "            result.update({'error_type': 'missing_value', 'notes': 'One value is missing'})\n",
        "            return result\n",
        "\n",
        "        # 根据验证类型进行比较\n",
        "        if validation_type == 'exact':\n",
        "            is_match = manual_clean.lower() == auto_clean.lower()\n",
        "            result.update({\n",
        "                'is_correct': is_match,\n",
        "                'score': 1.0 if is_match else 0.0,\n",
        "                'error_type': None if is_match else 'exact_mismatch'\n",
        "            })\n",
        "\n",
        "        elif validation_type == 'similarity':\n",
        "            # 使用主验证器的文本相似度算法\n",
        "            similarity_scores = self.main_validator.calculate_text_similarity(manual_clean, auto_clean)\n",
        "            similarity = similarity_scores['combined']\n",
        "\n",
        "            # 对于元数据，使用更高的相似度阈值\n",
        "            threshold = 0.8\n",
        "            is_match = similarity >= threshold\n",
        "\n",
        "            result.update({\n",
        "                'is_correct': is_match,\n",
        "                'score': similarity,\n",
        "                'similarity_details': similarity_scores,\n",
        "                'error_type': None if is_match else 'similarity_mismatch',\n",
        "                'notes': f'Similarity: {similarity:.3f}'\n",
        "            })\n",
        "\n",
        "        elif validation_type == 'numerical':\n",
        "            try:\n",
        "                # 标准化数值格式\n",
        "                manual_num = self._normalize_numerical_value(manual_clean)\n",
        "                auto_num = self._normalize_numerical_value(auto_clean)\n",
        "\n",
        "                if manual_num is not None and auto_num is not None:\n",
        "                    # 允许小幅度差异（适应提取中的舍入误差）\n",
        "                    tolerance = 0.01 if abs(manual_num) < 10 else abs(manual_num) * 0.001\n",
        "                    is_match = abs(manual_num - auto_num) <= tolerance\n",
        "\n",
        "                    result.update({\n",
        "                        'is_correct': is_match,\n",
        "                        'score': 1.0 if is_match else max(0.0, 1.0 - abs(manual_num - auto_num) / max(abs(manual_num), 1)),\n",
        "                        'error_type': None if is_match else 'numerical_mismatch',\n",
        "                        'manual_parsed': manual_num,\n",
        "                        'auto_parsed': auto_num,\n",
        "                        'notes': f'Manual: {manual_num}, Auto: {auto_num}'\n",
        "                    })\n",
        "                else:\n",
        "                    result.update({\n",
        "                        'error_type': 'numerical_parse_error',\n",
        "                        'notes': f'Failed to parse numerical values: \"{manual_clean}\" vs \"{auto_clean}\"'\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                result.update({\n",
        "                    'error_type': 'numerical_validation_error',\n",
        "                    'notes': f'Numerical validation failed: {str(e)}'\n",
        "                })\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _normalize_numerical_value(self, value_str: str) -> float:\n",
        "        \"\"\"标准化数值字符串为float\"\"\"\n",
        "        if not value_str:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # 移除常见的非数字字符，保留数字、小数点、负号\n",
        "            cleaned = re.sub(r'[^\\d\\.\\-\\+]', '', str(value_str))\n",
        "            if cleaned:\n",
        "                return float(cleaned)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # 尝试提取第一个数字\n",
        "        numbers = re.findall(r'-?\\d+\\.?\\d*', str(value_str))\n",
        "        if numbers:\n",
        "            try:\n",
        "                return float(numbers[0])\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return None\n",
        "\n",
        "    def validate_metadata_for_matched_pairs(self) -> Dict[str, any]:\n",
        "        \"\"\"\n",
        "        对已匹配的KPI对进行元数据验证\n",
        "\n",
        "        Returns:\n",
        "            元数据验证结果\n",
        "        \"\"\"\n",
        "        logging.info(\"开始元数据验证...\")\n",
        "\n",
        "        # 获取主验证器的匹配结果\n",
        "        if not hasattr(self.main_validator, 'validation_results') or not self.main_validator.validation_results:\n",
        "            logging.error(\"主验证器尚未运行，请先运行文本验证\")\n",
        "            return {}\n",
        "\n",
        "        matched_pairs = self.main_validator.validation_results.get('detailed_matches', [])\n",
        "        if not matched_pairs:\n",
        "            logging.warning(\"没有找到匹配的KPI对，无法进行元数据验证\")\n",
        "            return {}\n",
        "\n",
        "        metadata_results = {\n",
        "            'total_pairs': len(matched_pairs),\n",
        "            'field_results': {},\n",
        "            'overall_scores': {},\n",
        "            'problematic_field_analysis': {},\n",
        "            'detailed_results': []\n",
        "        }\n",
        "\n",
        "        # 初始化字段结果统计\n",
        "        all_fields = {}\n",
        "        for category, fields in self.metadata_fields.items():\n",
        "            all_fields.update(fields)\n",
        "\n",
        "        for field_name in all_fields.keys():\n",
        "            metadata_results['field_results'][field_name] = {\n",
        "                'total_comparisons': 0,\n",
        "                'correct_count': 0,\n",
        "                'total_score': 0.0,\n",
        "                'error_types': {},\n",
        "                'examples': {'correct': [], 'incorrect': []}\n",
        "            }\n",
        "\n",
        "        # 对每个匹配对进行元数据验证\n",
        "        for pair_idx, match_pair in enumerate(matched_pairs):\n",
        "            manual_idx = match_pair['manual_idx']\n",
        "            auto_idx = match_pair['auto_idx']\n",
        "            text_similarity = match_pair['similarity']\n",
        "\n",
        "            # 获取完整的KPI记录\n",
        "            manual_kpi = self.main_validator.manual_df.iloc[manual_idx]\n",
        "            auto_kpi = self.main_validator.auto_df.iloc[auto_idx]\n",
        "\n",
        "            pair_results = {\n",
        "                'pair_index': pair_idx,\n",
        "                'manual_idx': manual_idx,\n",
        "                'auto_idx': auto_idx,\n",
        "                'text_similarity': text_similarity,\n",
        "                'manual_text': match_pair['manual_text'],\n",
        "                'auto_text': match_pair['auto_text'],\n",
        "                'field_validations': {},\n",
        "                'pair_metadata_score': 0.0\n",
        "            }\n",
        "\n",
        "            total_weight = 0.0\n",
        "            weighted_score = 0.0\n",
        "\n",
        "            # 验证每个元数据字段\n",
        "            for field_name, field_config in all_fields.items():\n",
        "                manual_value = manual_kpi.get(field_name, '')\n",
        "                auto_value = auto_kpi.get(field_name, '')\n",
        "\n",
        "                field_result = self.validate_single_field(\n",
        "                    manual_value, auto_value, field_name, field_config['type']\n",
        "                )\n",
        "\n",
        "                pair_results['field_validations'][field_name] = field_result\n",
        "\n",
        "                # 更新字段统计\n",
        "                field_stats = metadata_results['field_results'][field_name]\n",
        "                field_stats['total_comparisons'] += 1\n",
        "                field_stats['total_score'] += field_result['score']\n",
        "\n",
        "                if field_result['is_correct']:\n",
        "                    field_stats['correct_count'] += 1\n",
        "                    if len(field_stats['examples']['correct']) < 3:\n",
        "                        field_stats['examples']['correct'].append({\n",
        "                            'manual': manual_value,\n",
        "                            'auto': auto_value,\n",
        "                            'pair_idx': pair_idx\n",
        "                        })\n",
        "                else:\n",
        "                    error_type = field_result.get('error_type', 'unknown')\n",
        "                    field_stats['error_types'][error_type] = field_stats['error_types'].get(error_type, 0) + 1\n",
        "\n",
        "                    if len(field_stats['examples']['incorrect']) < 3:\n",
        "                        field_stats['examples']['incorrect'].append({\n",
        "                            'manual': manual_value,\n",
        "                            'auto': auto_value,\n",
        "                            'error_type': error_type,\n",
        "                            'pair_idx': pair_idx,\n",
        "                            'notes': field_result.get('notes', '')\n",
        "                        })\n",
        "\n",
        "                # 计算加权分数\n",
        "                weight = field_config['weight']\n",
        "                weighted_score += field_result['score'] * weight\n",
        "                total_weight += weight\n",
        "\n",
        "            # 计算该对KPI的元数据总分\n",
        "            if total_weight > 0:\n",
        "                pair_results['pair_metadata_score'] = weighted_score / total_weight\n",
        "\n",
        "            metadata_results['detailed_results'].append(pair_results)\n",
        "\n",
        "        # 计算整体分数\n",
        "        self._calculate_overall_metadata_scores(metadata_results, all_fields)\n",
        "\n",
        "        # 分析问题字段\n",
        "        self._analyze_problematic_fields(metadata_results)\n",
        "\n",
        "        self.metadata_results = metadata_results\n",
        "        logging.info(f\"元数据验证完成，共验证 {len(matched_pairs)} 对KPI\")\n",
        "\n",
        "        return metadata_results\n",
        "\n",
        "    def _calculate_overall_metadata_scores(self, metadata_results: Dict, all_fields: Dict):\n",
        "        \"\"\"计算整体元数据分数\"\"\"\n",
        "        overall_scores = metadata_results['overall_scores']\n",
        "\n",
        "        # 按类别计算分数\n",
        "        for category, fields in self.metadata_fields.items():\n",
        "            category_score = 0.0\n",
        "            category_weight = 0.0\n",
        "\n",
        "            for field_name, field_config in fields.items():\n",
        "                field_stats = metadata_results['field_results'][field_name]\n",
        "                if field_stats['total_comparisons'] > 0:\n",
        "                    field_accuracy = field_stats['correct_count'] / field_stats['total_comparisons']\n",
        "                    field_avg_score = field_stats['total_score'] / field_stats['total_comparisons']\n",
        "\n",
        "                    # 使用平均分数而不是简单的正确率\n",
        "                    category_score += field_avg_score * field_config['weight']\n",
        "                    category_weight += field_config['weight']\n",
        "\n",
        "            if category_weight > 0:\n",
        "                overall_scores[category] = category_score / category_weight\n",
        "            else:\n",
        "                overall_scores[category] = 0.0\n",
        "\n",
        "        # 计算总体元数据分数\n",
        "        total_score = 0.0\n",
        "        total_weight = 0.0\n",
        "\n",
        "        for field_name, field_config in all_fields.items():\n",
        "            field_stats = metadata_results['field_results'][field_name]\n",
        "            if field_stats['total_comparisons'] > 0:\n",
        "                field_avg_score = field_stats['total_score'] / field_stats['total_comparisons']\n",
        "                total_score += field_avg_score * field_config['weight']\n",
        "                total_weight += field_config['weight']\n",
        "\n",
        "        if total_weight > 0:\n",
        "            overall_scores['overall_metadata_score'] = total_score / total_weight\n",
        "        else:\n",
        "            overall_scores['overall_metadata_score'] = 0.0\n",
        "\n",
        "    def _analyze_problematic_fields(self, metadata_results: Dict):\n",
        "        \"\"\"分析问题字段的详细情况\"\"\"\n",
        "        problematic_analysis = metadata_results['problematic_field_analysis']\n",
        "\n",
        "        for field_name, issue_type in self.problematic_fields.items():\n",
        "            if field_name in metadata_results['field_results']:\n",
        "                field_stats = metadata_results['field_results'][field_name]\n",
        "\n",
        "                if field_stats['total_comparisons'] > 0:\n",
        "                    accuracy = field_stats['correct_count'] / field_stats['total_comparisons']\n",
        "                    avg_score = field_stats['total_score'] / field_stats['total_comparisons']\n",
        "\n",
        "                    problematic_analysis[field_name] = {\n",
        "                        'issue_type': issue_type,\n",
        "                        'accuracy': accuracy,\n",
        "                        'average_score': avg_score,\n",
        "                        'total_errors': field_stats['total_comparisons'] - field_stats['correct_count'],\n",
        "                        'error_breakdown': field_stats['error_types'],\n",
        "                        'severity': self._assess_field_severity(accuracy, field_name),\n",
        "                        'improvement_priority': self._get_improvement_priority(field_name, accuracy)\n",
        "                    }\n",
        "\n",
        "    def _assess_field_severity(self, accuracy: float, field_name: str) -> str:\n",
        "        \"\"\"评估字段问题的严重程度\"\"\"\n",
        "        if accuracy >= 0.9:\n",
        "            return \"low\"\n",
        "        elif accuracy >= 0.7:\n",
        "            return \"medium\"\n",
        "        elif accuracy >= 0.5:\n",
        "            return \"high\"\n",
        "        else:\n",
        "            return \"critical\"\n",
        "\n",
        "    def _get_improvement_priority(self, field_name: str, accuracy: float) -> str:\n",
        "        \"\"\"获取改进优先级\"\"\"\n",
        "        field_importance = {\n",
        "            'Absolute Page Number': 'medium',  # 您说会升级\n",
        "            'kpi_theme': 'high',  # 重要分类字段\n",
        "            'quantitative_value': 'critical'  # 最重要的数值字段\n",
        "        }\n",
        "\n",
        "        importance = field_importance.get(field_name, 'low')\n",
        "\n",
        "        if accuracy < 0.5 and importance in ['high', 'critical']:\n",
        "            return 'urgent'\n",
        "        elif accuracy < 0.7 and importance == 'critical':\n",
        "            return 'high'\n",
        "        elif accuracy < 0.8 and importance in ['high', 'critical']:\n",
        "            return 'medium'\n",
        "        else:\n",
        "            return 'low'\n",
        "\n",
        "    def generate_metadata_report(self) -> str:\n",
        "        \"\"\"生成元数据验证报告\"\"\"\n",
        "        if not self.metadata_results:\n",
        "            return \"No metadata validation results available. Please run validate_metadata_for_matched_pairs() first.\"\n",
        "\n",
        "        results = self.metadata_results\n",
        "        overall_scores = results['overall_scores']\n",
        "\n",
        "        report = f\"\"\"\n",
        "# 元数据验证报告 (KPI文本验证独立报告)\n",
        "\n",
        "## 验证概览\n",
        "- **验证KPI对数**: {results['total_pairs']}\n",
        "- **总体元数据分数**: {overall_scores.get('overall_metadata_score', 0):.3f}\n",
        "- **验证时间**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "## 分类得分\n",
        "\n",
        "### 文档相关字段 ({overall_scores.get('document_fields', 0):.3f})\n",
        "\"\"\"\n",
        "\n",
        "        # 添加各类别详细分数\n",
        "        for category, category_score in overall_scores.items():\n",
        "            if category.endswith('_fields'):\n",
        "                category_name = category.replace('_', ' ').title()\n",
        "                report += f\"- **{category_name}**: {category_score:.3f}\\n\"\n",
        "\n",
        "        report += \"\\n## 字段详细分析\\n\\n\"\n",
        "        report += \"| 字段名 | 准确率 | 平均分数 | 错误数 | 主要错误类型 |\\n\"\n",
        "        report += \"|--------|--------|----------|--------|-------------|\\n\"\n",
        "\n",
        "        # 按准确率排序显示字段\n",
        "        field_items = list(results['field_results'].items())\n",
        "        field_items.sort(key=lambda x: x[1]['correct_count'] / max(x[1]['total_comparisons'], 1))\n",
        "\n",
        "        for field_name, field_stats in field_items:\n",
        "            if field_stats['total_comparisons'] > 0:\n",
        "                accuracy = field_stats['correct_count'] / field_stats['total_comparisons']\n",
        "                avg_score = field_stats['total_score'] / field_stats['total_comparisons']\n",
        "                error_count = field_stats['total_comparisons'] - field_stats['correct_count']\n",
        "\n",
        "                main_error = max(field_stats['error_types'].items(),\n",
        "                               key=lambda x: x[1], default=('none', 0))[0]\n",
        "\n",
        "                report += f\"| {field_name} | {accuracy:.3f} | {avg_score:.3f} | {error_count} | {main_error} |\\n\"\n",
        "\n",
        "        # 问题字段重点分析\n",
        "        if results['problematic_field_analysis']:\n",
        "            report += \"\\n## 重点关注字段分析\\n\\n\"\n",
        "\n",
        "            for field_name, analysis in results['problematic_field_analysis'].items():\n",
        "                report += f\"### {field_name} ({analysis['issue_type']})\\n\"\n",
        "                report += f\"- **准确率**: {analysis['accuracy']:.3f}\\n\"\n",
        "                report += f\"- **平均分数**: {analysis['average_score']:.3f}\\n\"\n",
        "                report += f\"- **错误数**: {analysis['total_errors']}\\n\"\n",
        "                report += f\"- **严重程度**: {analysis['severity']}\\n\"\n",
        "                report += f\"- **改进优先级**: {analysis['improvement_priority']}\\n\"\n",
        "\n",
        "                if analysis['error_breakdown']:\n",
        "                    report += \"- **错误类型分布**:\\n\"\n",
        "                    for error_type, count in analysis['error_breakdown'].items():\n",
        "                        report += f\"  - {error_type}: {count}\\n\"\n",
        "                report += \"\\n\"\n",
        "\n",
        "        # 改进建议\n",
        "        report += \"## 改进建议\\n\\n\"\n",
        "\n",
        "        critical_fields = [name for name, analysis in results['problematic_field_analysis'].items()\n",
        "                          if analysis['improvement_priority'] in ['urgent', 'high']]\n",
        "\n",
        "        if critical_fields:\n",
        "            report += \"### 优先改进字段:\\n\"\n",
        "            for field in critical_fields:\n",
        "                analysis = results['problematic_field_analysis'][field]\n",
        "                report += f\"- **{field}**: {analysis['issue_type']} (优先级: {analysis['improvement_priority']})\\n\"\n",
        "\n",
        "        report += \"\\n### 具体建议:\\n\"\n",
        "        if 'Absolute Page Number' in results['problematic_field_analysis']:\n",
        "            report += \"- **页码提取**: 考虑改进PDF页码识别算法\\n\"\n",
        "        if 'kpi_theme' in results['problematic_field_analysis']:\n",
        "            report += \"- **主题分类**: 优化KPI主题分类逻辑\\n\"\n",
        "        if 'quantitative_value' in results['problematic_field_analysis']:\n",
        "            report += \"- **数值提取**: 加强数值识别和标准化处理\\n\"\n",
        "\n",
        "        return report\n",
        "\n",
        "    def save_metadata_validation_results(self, output_dir: str = None):\n",
        "        \"\"\"保存元数据验证结果\"\"\"\n",
        "        if not self.metadata_results:\n",
        "            logging.warning(\"No metadata results to save\")\n",
        "            return {}\n",
        "\n",
        "        if output_dir is None:\n",
        "            output_dir = self.main_validator.output_dir / \"metadata_validation\"\n",
        "\n",
        "        output_path = Path(output_dir)\n",
        "        output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        saved_files = {}\n",
        "\n",
        "        # 1. 保存详细结果JSON\n",
        "        results_file = output_path / \"metadata_validation_results.json\"\n",
        "        with open(results_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.metadata_results, f, indent=2, ensure_ascii=False, default=str)\n",
        "        saved_files['detailed_json'] = results_file\n",
        "\n",
        "        # 2. 保存字段分析Excel\n",
        "        field_analysis_data = []\n",
        "        for field_name, field_stats in self.metadata_results['field_results'].items():\n",
        "            if field_stats['total_comparisons'] > 0:\n",
        "                accuracy = field_stats['correct_count'] / field_stats['total_comparisons']\n",
        "                avg_score = field_stats['total_score'] / field_stats['total_comparisons']\n",
        "\n",
        "                field_analysis_data.append({\n",
        "                    'Field Name': field_name,\n",
        "                    'Total Comparisons': field_stats['total_comparisons'],\n",
        "                    'Correct Count': field_stats['correct_count'],\n",
        "                    'Accuracy': accuracy,\n",
        "                    'Average Score': avg_score,\n",
        "                    'Error Count': field_stats['total_comparisons'] - field_stats['correct_count'],\n",
        "                    'Main Error Type': max(field_stats['error_types'].items(),\n",
        "                                         key=lambda x: x[1], default=('none', 0))[0]\n",
        "                })\n",
        "\n",
        "        field_df = pd.DataFrame(field_analysis_data)\n",
        "        field_df = field_df.sort_values('Accuracy')\n",
        "\n",
        "        excel_file = output_path / \"metadata_field_analysis.xlsx\"\n",
        "        with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n",
        "            field_df.to_excel(writer, sheet_name='Field Analysis', index=False)\n",
        "\n",
        "            # 问题字段详细分析\n",
        "            if self.metadata_results['problematic_field_analysis']:\n",
        "                prob_data = []\n",
        "                for field_name, analysis in self.metadata_results['problematic_field_analysis'].items():\n",
        "                    prob_data.append({\n",
        "                        'Field Name': field_name,\n",
        "                        'Issue Type': analysis['issue_type'],\n",
        "                        'Accuracy': analysis['accuracy'],\n",
        "                        'Average Score': analysis['average_score'],\n",
        "                        'Total Errors': analysis['total_errors'],\n",
        "                        'Severity': analysis['severity'],\n",
        "                        'Improvement Priority': analysis['improvement_priority']\n",
        "                    })\n",
        "\n",
        "                prob_df = pd.DataFrame(prob_data)\n",
        "                prob_df.to_excel(writer, sheet_name='Problematic Fields', index=False)\n",
        "\n",
        "        saved_files['field_analysis_excel'] = excel_file\n",
        "\n",
        "        # 3. 保存元数据报告\n",
        "        report = self.generate_metadata_report()\n",
        "        report_file = output_path / \"metadata_validation_report.md\"\n",
        "        with open(report_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(report)\n",
        "        saved_files['report_markdown'] = report_file\n",
        "\n",
        "        logging.info(f\"元数据验证结果已保存到: {output_path}\")\n",
        "        return saved_files\n",
        "\n",
        "# 集成函数：在主验证流程中添加元数据验证\n",
        "def run_complete_validation_with_metadata(df_auto: pd.DataFrame, manual_xlsx_path: str,\n",
        "                                         output_dir: str = \"complete_validation\") -> Dict[str, any]:\n",
        "    \"\"\"\n",
        "    运行完整验证（文本验证 + 元数据验证）\n",
        "\n",
        "    Args:\n",
        "        df_auto: 自动提取的KPI DataFrame\n",
        "        manual_xlsx_path: 手动标注文件路径\n",
        "        output_dir: 输出目录\n",
        "\n",
        "    Returns:\n",
        "        完整验证结果\n",
        "    \"\"\"\n",
        "    logging.info(\"开始运行完整验证（文本 + 元数据）...\")\n",
        "\n",
        "    # Step 1: 运行文本验证\n",
        "    print(\"🔍 Step 1: 运行文本验证...\")\n",
        "    text_validation_results = enhanced_compare_with_manual_kpis(\n",
        "        df_auto, manual_xlsx_path, f\"{output_dir}/text_validation\"\n",
        "    )\n",
        "\n",
        "    if not text_validation_results:\n",
        "        logging.error(\"文本验证失败，无法继续元数据验证\")\n",
        "        return {}\n",
        "\n",
        "    # Step 2: 运行元数据验证\n",
        "    print(\"📊 Step 2: 运行元数据验证...\")\n",
        "\n",
        "    # 获取文本验证器实例\n",
        "    temp_auto_path = Path(f\"{output_dir}/text_validation\") / \"temp_auto_kpis.xlsx\"\n",
        "    if not temp_auto_path.exists():\n",
        "        temp_auto_path = Path(output_dir) / \"temp_auto_kpis.xlsx\"\n",
        "        df_auto.to_excel(temp_auto_path, index=False)\n",
        "\n",
        "    main_validator = KPIValidationPipeline(\n",
        "        manual_excel_path=manual_xlsx_path,\n",
        "        auto_excel_path=str(temp_auto_path),\n",
        "        output_dir=f\"{output_dir}/text_validation\"\n",
        "    )\n",
        "\n",
        "    # 确保文本验证已运行\n",
        "    if not hasattr(main_validator, 'validation_results') or not main_validator.validation_results:\n",
        "        main_validator.run_comprehensive_evaluation()\n",
        "\n",
        "    # 创建元数据验证扩展\n",
        "    metadata_validator = MetadataValidationExtension(main_validator)\n",
        "    metadata_results = metadata_validator.validate_metadata_for_matched_pairs()\n",
        "\n",
        "    # 保存元数据验证结果\n",
        "    metadata_saved_files = metadata_validator.save_metadata_validation_results(f\"{output_dir}/metadata_validation\")\n",
        "\n",
        "    # Step 3: 生成综合报告\n",
        "    print(\"📋 Step 3: 生成综合报告...\")\n",
        "    complete_results = {\n",
        "        'text_validation': text_validation_results,\n",
        "        'metadata_validation': metadata_results,\n",
        "        'saved_files': {\n",
        "            'text_validation_files': text_validation_results.get('saved_files', {}),\n",
        "            'metadata_validation_files': metadata_saved_files\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # 打印摘要\n",
        "    text_best = text_validation_results.get('best_metrics', {})\n",
        "    metadata_overall = metadata_results.get('overall_scores', {})\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"完整验证结果摘要\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"📝 文本验证 F1 分数: {text_best.get('f1_score', 0):.3f}\")\n",
        "    print(f\"📊 元数据总体分数: {metadata_overall.get('overall_metadata_score', 0):.3f}\")\n",
        "    print(f\"🎯 验证KPI对数: {metadata_results.get('total_pairs', 0)}\")\n",
        "\n",
        "    print(f\"\\n📂 元数据分类得分:\")\n",
        "    for category, score in metadata_overall.items():\n",
        "        if category.endswith('_fields'):\n",
        "            category_name = category.replace('_', ' ').title()\n",
        "            print(f\"   {category_name}: {score:.3f}\")\n",
        "\n",
        "    # 显示问题字段\n",
        "    if metadata_results.get('problematic_field_analysis'):\n",
        "        print(f\"\\n⚠️  需要关注的字段:\")\n",
        "        for field_name, analysis in metadata_results['problematic_field_analysis'].items():\n",
        "            print(f\"   {field_name}: {analysis['accuracy']:.3f} (优先级: {analysis['improvement_priority']})\")\n",
        "\n",
        "    print(f\"\\n📁 完整结果保存在: {output_dir}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # 清理临时文件\n",
        "    if temp_auto_path.exists():\n",
        "        temp_auto_path.unlink()\n",
        "\n",
        "    return complete_results\n",
        "\n",
        "# 便捷使用函数\n",
        "def add_metadata_validation_to_existing_pipeline():\n",
        "    \"\"\"为现有验证流程添加元数据验证的示例函数\"\"\"\n",
        "\n",
        "    def enhanced_main_with_metadata():\n",
        "        \"\"\"增强版main函数，包含元数据验证\"\"\"\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format=\"%(asctime)s - %(levelname)s: %(message)s\",\n",
        "            datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            if not os.path.exists(PDF_PATH):\n",
        "                logging.error(f\"PDF file not found: {PDF_PATH}\")\n",
        "                return\n",
        "\n",
        "            # 原有的KPI提取流程\n",
        "            df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
        "            save_results(df_auto, EXPORT_AUTO_XLSX, PDF_PATH)\n",
        "            logging.info(f\"KPI extraction completed: {len(df_auto)} KPIs extracted\")\n",
        "\n",
        "            # 增强的验证流程（文本 + 元数据）\n",
        "            if MANUAL_XLSX and Path(MANUAL_XLSX).exists():\n",
        "                print(\"\\n🔍 Running complete validation (text + metadata)...\")\n",
        "                complete_validation_results = run_complete_validation_with_metadata(\n",
        "                    df_auto, MANUAL_XLSX, \"complete_validation\"\n",
        "                )\n",
        "\n",
        "                if complete_validation_results:\n",
        "                    print(\"✅ Complete validation finished with detailed analysis!\")\n",
        "                    print(f\"📁 Detailed results saved to: complete_validation/\")\n",
        "                else:\n",
        "                    print(\"⚠️ Validation encountered issues\")\n",
        "            else:\n",
        "                logging.info(\"Manual KPI file not found, skipping validation.\")\n",
        "\n",
        "            # 显示提取摘要（保持原有逻辑）\n",
        "            if not df_auto.empty:\n",
        "                print(f\"\\n=== Extraction Summary ===\")\n",
        "                print(f\"Total KPIs extracted: {len(df_auto)}\")\n",
        "\n",
        "                if 'source_type' in df_auto.columns:\n",
        "                    source_counts = df_auto['source_type'].value_counts()\n",
        "                    print(f\"From text/tables: {source_counts.get('text', 0)}\")\n",
        "                    print(f\"From images/charts: {source_counts.get('image', 0)}\")\n",
        "\n",
        "                if 'kpi_theme' in df_auto.columns:\n",
        "                    theme_counts = df_auto['kpi_theme'].value_counts()\n",
        "                    print(f\"\\nKPI Distribution by Theme:\")\n",
        "                    for theme, count in theme_counts.items():\n",
        "                        print(f\"  {theme}: {count}\")\n",
        "            else:\n",
        "                print(\"\\nNo KPIs were extracted from the document.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error in enhanced main execution: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "    return enhanced_main_with_metadata\n"
      ],
      "metadata": {
        "id": "HfzJhUgy3OtK"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchKPIProcessor:\n",
        "    \"\"\"批量KPI处理器 - 支持多个PDF和Manual文件\"\"\"\n",
        "\n",
        "    def __init__(self, base_output_dir: str = \"batch_kpi_results\"):\n",
        "        \"\"\"\n",
        "        初始化批量处理器\n",
        "\n",
        "        Args:\n",
        "            base_output_dir: 批量处理结果的基础目录\n",
        "        \"\"\"\n",
        "        self.base_output_dir = Path(base_output_dir)\n",
        "        self.base_output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # 存储所有文件配对\n",
        "        self.file_pairs = []\n",
        "        self.batch_results = []\n",
        "\n",
        "        # 创建时间戳用于本次批量处理\n",
        "        self.batch_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        self.current_batch_dir = self.base_output_dir / f\"batch_{self.batch_timestamp}\"\n",
        "        self.current_batch_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        logging.info(f\"批量处理器初始化完成，结果保存到: {self.current_batch_dir}\")\n",
        "\n",
        "    def add_file_pair(self, pdf_path: str, manual_path: str, document_name: str = None):\n",
        "        \"\"\"\n",
        "        添加一对PDF和Manual文件\n",
        "\n",
        "        Args:\n",
        "            pdf_path: PDF文件路径\n",
        "            manual_path: Manual标注文件路径\n",
        "            document_name: 文档名称（可选，默认使用PDF文件名）\n",
        "        \"\"\"\n",
        "        pdf_path = Path(pdf_path)\n",
        "        manual_path = Path(manual_path)\n",
        "\n",
        "        # 验证文件存在\n",
        "        if not pdf_path.exists():\n",
        "            logging.error(f\"PDF文件不存在: {pdf_path}\")\n",
        "            return False\n",
        "\n",
        "        if not manual_path.exists():\n",
        "            logging.error(f\"Manual文件不存在: {manual_path}\")\n",
        "            return False\n",
        "\n",
        "        # 自动生成文档名称\n",
        "        if document_name is None:\n",
        "            document_name = pdf_path.stem\n",
        "\n",
        "        file_pair = {\n",
        "            'pdf_path': str(pdf_path),\n",
        "            'manual_path': str(manual_path),\n",
        "            'document_name': document_name,\n",
        "            'doc_id': len(self.file_pairs) + 1\n",
        "        }\n",
        "\n",
        "        self.file_pairs.append(file_pair)\n",
        "        logging.info(f\"添加文件对 {len(self.file_pairs)}: {document_name}\")\n",
        "        return True\n",
        "\n",
        "    def add_multiple_pairs_from_directory(self, pdf_dir: str, manual_dir: str,\n",
        "                                         pdf_pattern: str = \"*.pdf\",\n",
        "                                         manual_pattern: str = \"*.xlsx\"):\n",
        "        \"\"\"\n",
        "        从目录批量添加文件对（按文件名匹配）\n",
        "\n",
        "        Args:\n",
        "            pdf_dir: PDF文件目录\n",
        "            manual_dir: Manual文件目录\n",
        "            pdf_pattern: PDF文件匹配模式\n",
        "            manual_pattern: Manual文件匹配模式\n",
        "        \"\"\"\n",
        "        pdf_dir = Path(pdf_dir)\n",
        "        manual_dir = Path(manual_dir)\n",
        "\n",
        "        if not pdf_dir.exists() or not manual_dir.exists():\n",
        "            logging.error(f\"目录不存在: {pdf_dir} 或 {manual_dir}\")\n",
        "            return 0\n",
        "\n",
        "        # 获取所有PDF文件\n",
        "        pdf_files = list(pdf_dir.glob(pdf_pattern))\n",
        "        added_count = 0\n",
        "\n",
        "        for pdf_file in pdf_files:\n",
        "            # 尝试找到对应的Manual文件\n",
        "            base_name = pdf_file.stem\n",
        "\n",
        "            # 尝试多种匹配模式\n",
        "            possible_manual_names = [\n",
        "                f\"{base_name}.xlsx\",\n",
        "                f\"{base_name}_manual.xlsx\",\n",
        "                f\"manual_{base_name}.xlsx\",\n",
        "                f\"{base_name}.xls\"\n",
        "            ]\n",
        "\n",
        "            manual_file = None\n",
        "            for manual_name in possible_manual_names:\n",
        "                potential_manual = manual_dir / manual_name\n",
        "                if potential_manual.exists():\n",
        "                    manual_file = potential_manual\n",
        "                    break\n",
        "\n",
        "            if manual_file:\n",
        "                if self.add_file_pair(str(pdf_file), str(manual_file), base_name):\n",
        "                    added_count += 1\n",
        "            else:\n",
        "                logging.warning(f\"未找到 {base_name} 对应的Manual文件\")\n",
        "\n",
        "        logging.info(f\"从目录批量添加了 {added_count} 个文件对\")\n",
        "        return added_count\n",
        "\n",
        "    def list_file_pairs(self):\n",
        "        \"\"\"显示所有已添加的文件对\"\"\"\n",
        "        if not self.file_pairs:\n",
        "            print(\"❌ 没有添加任何文件对\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\n📋 已添加的文件对 (共 {len(self.file_pairs)} 对):\")\n",
        "        print(\"-\" * 80)\n",
        "        for pair in self.file_pairs:\n",
        "            print(f\"{pair['doc_id']:2d}. 文档: {pair['document_name']}\")\n",
        "            print(f\"    PDF:    {pair['pdf_path']}\")\n",
        "            print(f\"    Manual: {pair['manual_path']}\")\n",
        "            print()\n",
        "\n",
        "    def process_single_document(self, file_pair: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        处理单个文档（PDF + Manual）\n",
        "\n",
        "        Args:\n",
        "            file_pair: 文件对信息\n",
        "\n",
        "        Returns:\n",
        "            处理结果字典\n",
        "        \"\"\"\n",
        "        doc_name = file_pair['document_name']\n",
        "        pdf_path = file_pair['pdf_path']\n",
        "        manual_path = file_pair['manual_path']\n",
        "        doc_id = file_pair['doc_id']\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"📄 处理文档 {doc_id}/{len(self.file_pairs)}: {doc_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # 为每个文档创建独立目录\n",
        "        doc_output_dir = self.current_batch_dir / f\"doc_{doc_id}_{doc_name}\"\n",
        "        doc_output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        start_time = time.time()\n",
        "        result = {\n",
        "            'doc_id': doc_id,\n",
        "            'document_name': doc_name,\n",
        "            'pdf_path': pdf_path,\n",
        "            'manual_path': manual_path,\n",
        "            'output_dir': str(doc_output_dir),\n",
        "            'start_time': datetime.now().isoformat(),\n",
        "            'status': 'processing'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Step 1: 临时修改全局PDF路径\n",
        "            global PDF_PATH\n",
        "            original_pdf_path = PDF_PATH\n",
        "            PDF_PATH = pdf_path\n",
        "\n",
        "            print(f\"📊 Step 1: 提取KPI from {Path(pdf_path).name}...\")\n",
        "\n",
        "            # Step 2: 运行KPI提取\n",
        "            df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
        "\n",
        "            # Step 3: 保存自动提取结果\n",
        "            auto_excel_path = doc_output_dir / f\"{doc_name}_auto_kpis.xlsx\"\n",
        "            save_results(df_auto, str(auto_excel_path), PDF_PATH)\n",
        "\n",
        "            print(f\"✅ 提取完成: {len(df_auto)} KPIs\")\n",
        "\n",
        "            # Step 4: 运行验证\n",
        "            print(f\"🔍 Step 2: 运行验证 against {Path(manual_path).name}...\")\n",
        "            validation_output_dir = doc_output_dir / \"validation\"\n",
        "            validation_results = enhanced_compare_with_manual_kpis(\n",
        "                df_auto, manual_path, str(validation_output_dir)\n",
        "            )\n",
        "\n",
        "            # Step 5: 收集结果\n",
        "            processing_time = time.time() - start_time\n",
        "\n",
        "            result.update({\n",
        "                'status': 'completed',\n",
        "                'processing_time_seconds': processing_time,\n",
        "                'extracted_kpis_count': len(df_auto),\n",
        "                'auto_excel_path': str(auto_excel_path),\n",
        "                'validation_output_dir': str(validation_output_dir),\n",
        "                'end_time': datetime.now().isoformat()\n",
        "            })\n",
        "\n",
        "            # 添加验证指标\n",
        "            if validation_results and 'best_metrics' in validation_results:\n",
        "                metrics = validation_results['best_metrics']\n",
        "                result.update({\n",
        "                    'validation_f1_score': metrics.get('f1_score', 0),\n",
        "                    'validation_precision': metrics.get('precision', 0),\n",
        "                    'validation_recall': metrics.get('recall', 0),\n",
        "                    'true_positives': metrics.get('true_positives', 0),\n",
        "                    'false_positives': metrics.get('false_positives', 0),\n",
        "                    'false_negatives': metrics.get('false_negatives', 0)\n",
        "                })\n",
        "\n",
        "                print(f\"🎯 验证完成:\")\n",
        "                print(f\"   F1 Score: {metrics.get('f1_score', 0):.3f}\")\n",
        "                print(f\"   Precision: {metrics.get('precision', 0):.3f}\")\n",
        "                print(f\"   Recall: {metrics.get('recall', 0):.3f}\")\n",
        "\n",
        "            print(f\"⏱️  处理耗时: {processing_time:.1f}秒\")\n",
        "            print(f\"📁 结果保存到: {doc_output_dir}\")\n",
        "\n",
        "            # 恢复原始PDF路径\n",
        "            PDF_PATH = original_pdf_path\n",
        "\n",
        "        except Exception as e:\n",
        "            processing_time = time.time() - start_time\n",
        "            error_msg = str(e)\n",
        "\n",
        "            result.update({\n",
        "                'status': 'failed',\n",
        "                'processing_time_seconds': processing_time,\n",
        "                'error_message': error_msg,\n",
        "                'end_time': datetime.now().isoformat()\n",
        "            })\n",
        "\n",
        "            print(f\"❌ 处理失败: {error_msg}\")\n",
        "\n",
        "            # 恢复原始PDF路径\n",
        "            PDF_PATH = original_pdf_path\n",
        "\n",
        "            # 保存错误日志\n",
        "            error_log_path = doc_output_dir / \"error_log.txt\"\n",
        "            with open(error_log_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(f\"文档: {doc_name}\\n\")\n",
        "                f.write(f\"错误时间: {datetime.now()}\\n\")\n",
        "                f.write(f\"错误信息: {error_msg}\\n\")\n",
        "                f.write(f\"PDF路径: {pdf_path}\\n\")\n",
        "                f.write(f\"Manual路径: {manual_path}\\n\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def run_batch_processing(self, max_workers: int = 1):\n",
        "        \"\"\"\n",
        "        运行批量处理\n",
        "\n",
        "        Args:\n",
        "            max_workers: 最大并发处理数（建议保持为1，避免API限制）\n",
        "        \"\"\"\n",
        "        if not self.file_pairs:\n",
        "            print(\"❌ 没有要处理的文件对\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\n🚀 开始批量处理 {len(self.file_pairs)} 个文档...\")\n",
        "        print(f\"📁 结果将保存到: {self.current_batch_dir}\")\n",
        "\n",
        "        batch_start_time = time.time()\n",
        "\n",
        "        # 处理每个文档\n",
        "        for file_pair in self.file_pairs:\n",
        "            result = self.process_single_document(file_pair)\n",
        "            self.batch_results.append(result)\n",
        "\n",
        "            # 实时保存进度（防止中断丢失结果）\n",
        "            self.save_batch_progress()\n",
        "\n",
        "        # 生成最终报告\n",
        "        batch_total_time = time.time() - batch_start_time\n",
        "        self.generate_batch_summary(batch_total_time)\n",
        "\n",
        "        print(f\"\\n🎉 批量处理完成!\")\n",
        "        print(f\"⏱️  总耗时: {batch_total_time:.1f}秒 ({batch_total_time/60:.1f}分钟)\")\n",
        "        print(f\"📊 处理统计: {self.get_batch_statistics()}\")\n",
        "        print(f\"📁 完整结果查看: {self.current_batch_dir}\")\n",
        "\n",
        "    def save_batch_progress(self):\n",
        "        \"\"\"保存批量处理进度\"\"\"\n",
        "        progress_file = self.current_batch_dir / \"batch_progress.json\"\n",
        "        with open(progress_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump({\n",
        "                'batch_timestamp': self.batch_timestamp,\n",
        "                'file_pairs': self.file_pairs,\n",
        "                'results': self.batch_results,\n",
        "                'last_updated': datetime.now().isoformat()\n",
        "            }, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    def get_batch_statistics(self) -> str:\n",
        "        \"\"\"获取批量处理统计信息\"\"\"\n",
        "        if not self.batch_results:\n",
        "            return \"无结果\"\n",
        "\n",
        "        total = len(self.batch_results)\n",
        "        completed = len([r for r in self.batch_results if r['status'] == 'completed'])\n",
        "        failed = len([r for r in self.batch_results if r['status'] == 'failed'])\n",
        "\n",
        "        # 计算平均验证指标\n",
        "        completed_results = [r for r in self.batch_results if r['status'] == 'completed']\n",
        "        if completed_results:\n",
        "            avg_f1 = sum(r.get('validation_f1_score', 0) for r in completed_results) / len(completed_results)\n",
        "            avg_precision = sum(r.get('validation_precision', 0) for r in completed_results) / len(completed_results)\n",
        "            avg_recall = sum(r.get('validation_recall', 0) for r in completed_results) / len(completed_results)\n",
        "            total_kpis = sum(r.get('extracted_kpis_count', 0) for r in completed_results)\n",
        "        else:\n",
        "            avg_f1 = avg_precision = avg_recall = total_kpis = 0\n",
        "\n",
        "        return f\"\"\"\n",
        "        成功: {completed}/{total} ({completed/total*100:.1f}%)\n",
        "        失败: {failed}/{total} ({failed/total*100:.1f}%)\n",
        "        总KPI数: {total_kpis}\n",
        "        平均F1: {avg_f1:.3f}\n",
        "        平均精确率: {avg_precision:.3f}\n",
        "        平均召回率: {avg_recall:.3f}\n",
        "        \"\"\"\n",
        "\n",
        "    def generate_batch_summary(self, total_time: float):\n",
        "        \"\"\"生成批量处理汇总报告\"\"\"\n",
        "        # 1. 保存详细结果到Excel\n",
        "        results_df = pd.DataFrame(self.batch_results)\n",
        "        excel_path = self.current_batch_dir / \"batch_summary.xlsx\"\n",
        "\n",
        "        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
        "            # 主要结果\n",
        "            results_df.to_excel(writer, sheet_name='处理结果', index=False)\n",
        "\n",
        "            # 验证指标汇总\n",
        "            if not results_df.empty:\n",
        "                completed_df = results_df[results_df['status'] == 'completed']\n",
        "                if not completed_df.empty:\n",
        "                    validation_columns = ['document_name', 'extracted_kpis_count',\n",
        "                                        'validation_f1_score', 'validation_precision',\n",
        "                                        'validation_recall', 'true_positives',\n",
        "                                        'false_positives', 'false_negatives']\n",
        "\n",
        "                    validation_df = completed_df[validation_columns].copy()\n",
        "                    validation_df.to_excel(writer, sheet_name='验证指标', index=False)\n",
        "\n",
        "        # 2. 生成Markdown报告\n",
        "        report_path = self.current_batch_dir / \"batch_report.md\"\n",
        "        with open(report_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"\"\"# 批量KPI提取与验证报告\n",
        "\n",
        "## 处理概览\n",
        "- **处理时间**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "- **批次ID**: {self.batch_timestamp}\n",
        "- **总文档数**: {len(self.file_pairs)}\n",
        "- **总耗时**: {total_time:.1f}秒 ({total_time/60:.1f}分钟)\n",
        "\n",
        "## 处理统计\n",
        "{self.get_batch_statistics()}\n",
        "\n",
        "## 详细结果\n",
        "\n",
        "| 文档ID | 文档名称 | 状态 | KPI数量 | F1分数 | 精确率 | 召回率 | 处理时间(秒) |\n",
        "|--------|----------|------|---------|--------|--------|--------|-------------|\n",
        "\"\"\")\n",
        "\n",
        "            for result in self.batch_results:\n",
        "                f.write(f\"| {result['doc_id']} | {result['document_name']} | {result['status']} | \"\n",
        "                       f\"{result.get('extracted_kpis_count', 'N/A')} | \"\n",
        "                       f\"{result.get('validation_f1_score', 0):.3f} | \"\n",
        "                       f\"{result.get('validation_precision', 0):.3f} | \"\n",
        "                       f\"{result.get('validation_recall', 0):.3f} | \"\n",
        "                       f\"{result.get('processing_time_seconds', 0):.1f} |\\n\")\n",
        "\n",
        "            if any(r['status'] == 'failed' for r in self.batch_results):\n",
        "                f.write(f\"\\n## 失败的文档\\n\")\n",
        "                for result in self.batch_results:\n",
        "                    if result['status'] == 'failed':\n",
        "                        f.write(f\"- **{result['document_name']}**: {result.get('error_message', '未知错误')}\\n\")\n",
        "\n",
        "        print(f\"📋 批量处理报告生成: {report_path}\")\n",
        "        print(f\"📊 详细结果Excel: {excel_path}\")"
      ],
      "metadata": {
        "id": "sJ5B5aGfIZsZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 修改批量处理器，显示详细的单文档验证信息\n",
        "class BatchKPIProcessorWithDetailedOutput(BatchKPIProcessor):\n",
        "    \"\"\"\n",
        "    扩展批量处理器，支持详细的验证输出\n",
        "    \"\"\"\n",
        "\n",
        "    def process_single_document(self, file_pair: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        处理单个文档，显示详细的验证信息\n",
        "        \"\"\"\n",
        "        doc_name = file_pair['document_name']\n",
        "        pdf_path = file_pair['pdf_path']\n",
        "        manual_path = file_pair['manual_path']\n",
        "        doc_id = file_pair['doc_id']\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"📄 Processing Document {doc_id}/{len(self.file_pairs)}: {doc_name}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        # 为每个文档创建独立目录\n",
        "        doc_output_dir = self.current_batch_dir / f\"doc_{doc_id}_{doc_name}\"\n",
        "        doc_output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        start_time = time.time()\n",
        "        result = {\n",
        "            'doc_id': doc_id,\n",
        "            'document_name': doc_name,\n",
        "            'pdf_path': pdf_path,\n",
        "            'manual_path': manual_path,\n",
        "            'output_dir': str(doc_output_dir),\n",
        "            'start_time': datetime.now().isoformat(),\n",
        "            'status': 'processing'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Step 1: 临时修改全局PDF路径\n",
        "            global PDF_PATH\n",
        "            original_pdf_path = PDF_PATH\n",
        "            PDF_PATH = pdf_path\n",
        "\n",
        "            print(f\"📊 Step 1: Extracting KPIs from {Path(pdf_path).name}...\")\n",
        "\n",
        "            # Step 2: 运行KPI提取\n",
        "            df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
        "\n",
        "            # Step 3: 保存自动提取结果\n",
        "            auto_excel_path = doc_output_dir / f\"{doc_name}_auto_kpis.xlsx\"\n",
        "            save_results(df_auto, str(auto_excel_path), PDF_PATH)\n",
        "\n",
        "            print(f\"✅ Extraction completed: {len(df_auto)} KPIs\")\n",
        "\n",
        "            # Step 4: 运行详细验证\n",
        "            print(f\"\\n🔍 Step 2: Running comprehensive validation against {Path(manual_path).name}...\")\n",
        "            validation_output_dir = doc_output_dir / \"validation\"\n",
        "            validation_start_time = time.time()\n",
        "\n",
        "            complete_validation_results = enhanced_compare_with_manual_kpis(\n",
        "                df_auto, manual_path, str(validation_output_dir)\n",
        "            )\n",
        "\n",
        "            validation_time = time.time() - validation_start_time\n",
        "\n",
        "            # Step 5: 收集结果\n",
        "            processing_time = time.time() - start_time\n",
        "\n",
        "            result.update({\n",
        "                'status': 'completed',\n",
        "                'processing_time_seconds': processing_time,\n",
        "                'validation_time_seconds': validation_time,\n",
        "                'extracted_kpis_count': len(df_auto),\n",
        "                'auto_excel_path': str(auto_excel_path),\n",
        "                'validation_output_dir': str(validation_output_dir),\n",
        "                'end_time': datetime.now().isoformat()\n",
        "            })\n",
        "\n",
        "            # 添加详细的验证指标\n",
        "            if complete_validation_results:\n",
        "                # 文本验证指标\n",
        "                if 'text_validation' in complete_validation_results:\n",
        "                    text_metrics = complete_validation_results['text_validation'].get('best_metrics', {})\n",
        "                    result.update({\n",
        "                        'text_validation_f1_score': text_metrics.get('f1_score', 0),\n",
        "                        'text_validation_precision': text_metrics.get('precision', 0),\n",
        "                        'text_validation_recall': text_metrics.get('recall', 0),\n",
        "                        'true_positives': text_metrics.get('true_positives', 0),\n",
        "                        'false_positives': text_metrics.get('false_positives', 0),\n",
        "                        'false_negatives': text_metrics.get('false_negatives', 0),\n",
        "                        'manual_kpis_count': text_metrics.get('total_manual', 0),\n",
        "                        'auto_kpis_count': text_metrics.get('total_auto', 0)\n",
        "                    })\n",
        "\n",
        "                # 元数据验证指标\n",
        "                if 'metadata_validation' in complete_validation_results:\n",
        "                    metadata_scores = complete_validation_results['metadata_validation'].get('overall_scores', {})\n",
        "                    result.update({\n",
        "                        'metadata_overall_score': metadata_scores.get('overall_metadata_score', 0),\n",
        "                        'document_fields_score': metadata_scores.get('document_fields', 0),\n",
        "                        'classification_fields_score': metadata_scores.get('classification_fields', 0),\n",
        "                        'quantitative_fields_score': metadata_scores.get('quantitative_fields', 0),\n",
        "                        'analysis_fields_score': metadata_scores.get('analysis_fields', 0)\n",
        "                    })\n",
        "\n",
        "                    # 添加问题字段分析\n",
        "                    problematic_fields = complete_validation_results['metadata_validation'].get('problematic_field_analysis', {})\n",
        "                    for field_name in ['Absolute Page Number', 'kpi_theme', 'quantitative_value']:\n",
        "                        if field_name in problematic_fields:\n",
        "                            field_key = field_name.lower().replace(' ', '_')\n",
        "                            result[f'{field_key}_accuracy'] = problematic_fields[field_name]['accuracy']\n",
        "                            result[f'{field_key}_priority'] = problematic_fields[field_name]['improvement_priority']\n",
        "\n",
        "                # 显示最终摘要\n",
        "                print(f\"\\n🎯 Document {doc_id} Verification Completed:\")\n",
        "                print(f\"   📊 Dataset: {result.get('manual_kpis_count', 0)} manual vs {result.get('auto_kpis_count', 0)} auto KPIs\")\n",
        "                print(f\"   🎯 Text F1 Score: {result.get('text_validation_f1_score', 0):.3f}\")\n",
        "                print(f\"   📈 Text Precision: {result.get('text_validation_precision', 0):.3f}\")\n",
        "                print(f\"   📉 Text Recall: {result.get('text_validation_recall', 0):.3f}\")\n",
        "                print(f\"   ✅ True Positives: {result.get('true_positives', 0)}\")\n",
        "                print(f\"   ❌ False Positives: {result.get('false_positives', 0)}\")\n",
        "                print(f\"   ⚠️  False Negatives: {result.get('false_negatives', 0)}\")\n",
        "                print(f\"   📊 Metadata Overall Score: {result.get('metadata_overall_score', 0):.3f}\")\n",
        "\n",
        "            print(f\"⏱️  Processing time: {processing_time:.1f}秒 (validation: {validation_time:.1f}秒)\")\n",
        "            print(f\"📁 Results saved to: {doc_output_dir}\")\n",
        "\n",
        "            # 恢复原始PDF路径\n",
        "            PDF_PATH = original_pdf_path\n",
        "\n",
        "        except Exception as e:\n",
        "            processing_time = time.time() - start_time\n",
        "            error_msg = str(e)\n",
        "\n",
        "            result.update({\n",
        "                'status': 'failed',\n",
        "                'processing_time_seconds': processing_time,\n",
        "                'error_message': error_msg,\n",
        "                'end_time': datetime.now().isoformat()\n",
        "            })\n",
        "\n",
        "            print(f\"❌ Processing failed: {error_msg}\")\n",
        "\n",
        "            # 恢复原始PDF路径\n",
        "            PDF_PATH = original_pdf_path\n",
        "\n",
        "            # 保存错误日志\n",
        "            error_log_path = doc_output_dir / \"error_log.txt\"\n",
        "            with open(error_log_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(f\"Document: {doc_name}\\n\")\n",
        "                f.write(f\"Error time: {datetime.now()}\\n\")\n",
        "                f.write(f\"Error message: {error_msg}\\n\")\n",
        "                f.write(f\"PDF Path: {pdf_path}\\n\")\n",
        "                f.write(f\"Manual Path: {manual_path}\\n\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def generate_batch_summary(self, total_time: float):\n",
        "        \"\"\"生成详细的批量处理汇总报告\"\"\"\n",
        "        # 调用父类方法生成基础报告\n",
        "        super().generate_batch_summary(total_time)\n",
        "\n",
        "        # 生成详细的批量摘要\n",
        "        completed_results = [r for r in self.batch_results if r['status'] == 'completed']\n",
        "\n",
        "        if completed_results:\n",
        "            print(f\"\\n{'='*80}\")\n",
        "            print(\"BATCH PROCESSING DETAILED SUMMARY\")\n",
        "            print(f\"{'='*80}\")\n",
        "\n",
        "            # 计算批量统计\n",
        "            total_manual_kpis = sum(r.get('manual_kpis_count', 0) for r in completed_results)\n",
        "            total_auto_kpis = sum(r.get('auto_kpis_count', 0) for r in completed_results)\n",
        "            total_true_positives = sum(r.get('true_positives', 0) for r in completed_results)\n",
        "            total_false_positives = sum(r.get('false_positives', 0) for r in completed_results)\n",
        "            total_false_negatives = sum(r.get('false_negatives', 0) for r in completed_results)\n",
        "\n",
        "            # 计算平均指标\n",
        "            avg_f1 = sum(r.get('text_validation_f1_score', 0) for r in completed_results) / len(completed_results)\n",
        "            avg_precision = sum(r.get('text_validation_precision', 0) for r in completed_results) / len(completed_results)\n",
        "            avg_recall = sum(r.get('text_validation_recall', 0) for r in completed_results) / len(completed_results)\n",
        "            avg_metadata_score = sum(r.get('metadata_overall_score', 0) for r in completed_results) / len(completed_results)\n",
        "\n",
        "            print(f\"📊 Batch Dataset Summary:\")\n",
        "            print(f\"   Total Documents Processed: {len(completed_results)}\")\n",
        "            print(f\"   Total Manual KPIs: {total_manual_kpis}\")\n",
        "            print(f\"   Total Auto KPIs: {total_auto_kpis}\")\n",
        "            print(f\"   Total True Positives: {total_true_positives}\")\n",
        "            print(f\"   Total False Positives: {total_false_positives}\")\n",
        "            print(f\"   Total False Negatives: {total_false_negatives}\")\n",
        "\n",
        "            print(f\"\\n🎯 Batch Average Performance:\")\n",
        "            print(f\"   Average Text F1 Score: {avg_f1:.3f}\")\n",
        "            print(f\"   Average Text Precision: {avg_precision:.3f}\")\n",
        "            print(f\"   Average Text Recall: {avg_recall:.3f}\")\n",
        "            print(f\"   Average Metadata Overall Score: {avg_metadata_score:.3f}\")\n",
        "\n",
        "            # 显示各文档表现\n",
        "            print(f\"\\n📋 Individual Document Performance:\")\n",
        "            print(\"=\"*80)\n",
        "\n",
        "            for result in completed_results:\n",
        "                doc_name = result['document_name'][:30]  # 限制长度\n",
        "                f1 = result.get('text_validation_f1_score', 0)\n",
        "                precision = result.get('text_validation_precision', 0)\n",
        "                recall = result.get('text_validation_recall', 0)\n",
        "                metadata = result.get('metadata_overall_score', 0)\n",
        "                tp = result.get('true_positives', 0)\n",
        "                fp = result.get('false_positives', 0)\n",
        "                fn = result.get('false_negatives', 0)\n",
        "\n",
        "                print(f\"{doc_name:<30} | F1: {f1:.3f} | P: {precision:.3f} | R: {recall:.3f} | Meta: {metadata:.3f} | TP:{tp:2d} FP:{fp:2d} FN:{fn:2d}\")\n",
        "\n",
        "            print(\"=\"*80)\n",
        "            print(f\"📁 Detailed results saved to: {self.current_batch_dir}\")\n",
        "            print(\"=\"*80)\n",
        "\n",
        "# 创建详细输出的批量处理器\n",
        "def create_batch_processor_with_detailed_output():\n",
        "    \"\"\"创建支持详细输出的批量处理器\"\"\"\n",
        "    return BatchKPIProcessorWithDetailedOutput()\n",
        "\n",
        "# 修改集成执行函数\n",
        "def integrated_main_execution_detailed():\n",
        "    \"\"\"集成的主执行函数 - 详细输出版本\"\"\"\n",
        "    print(\"🚀 Enhanced KPI Extraction System - Detailed Output\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"1. Single PDF processing (detailed text + metadata validation)\")\n",
        "    print(\"2. Batch PDF processing (detailed text + metadata validation)\")\n",
        "    print(\"3. Quick batch from directories (detailed output)\")\n",
        "    print(\"4. Quick metadata validation test\")\n",
        "    print(\"5. View usage examples\")\n",
        "\n",
        "    try:\n",
        "        choice = input(\"Please select processing mode (1-5): \")\n",
        "\n",
        "        if choice == '1':\n",
        "            # 单个PDF处理，详细输出\n",
        "            enhanced_main_with_detailed_output()\n",
        "\n",
        "        elif choice == '2':\n",
        "            # 批量处理，详细输出\n",
        "            processor = create_batch_processor_with_detailed_output()\n",
        "\n",
        "            # 让用户手动添加文件对\n",
        "            while True:\n",
        "                pdf_path = input(\"Enter PDF file path (press Enter to finish): \").strip()\n",
        "                if not pdf_path:\n",
        "                    break\n",
        "                manual_path = input(\"Enter corresponding Manual file path: \").strip()\n",
        "                doc_name = input(\"Document name (press Enter for default): \").strip() or None\n",
        "\n",
        "                processor.add_file_pair(pdf_path, manual_path, doc_name)\n",
        "\n",
        "            if processor.file_pairs:\n",
        "                processor.list_file_pairs()\n",
        "                confirm = input(f\"\\nStart detailed processing of these {len(processor.file_pairs)} documents? (y/n): \")\n",
        "                if confirm.lower() == 'y':\n",
        "                    processor.run_batch_processing()\n",
        "                else:\n",
        "                    print(\"Batch processing cancelled.\")\n",
        "            else:\n",
        "                print(\"❌ No file pairs added.\")\n",
        "\n",
        "        elif choice == '3':\n",
        "            # 快速目录批量处理，详细输出\n",
        "            pdf_dir = input(\"PDF files directory path: \").strip()\n",
        "            manual_dir = input(\"Manual files directory path: \").strip()\n",
        "\n",
        "            processor = create_batch_processor_with_detailed_output()\n",
        "            added_count = processor.add_multiple_pairs_from_directory(pdf_dir, manual_dir)\n",
        "\n",
        "            if added_count == 0:\n",
        "                print(\"❌ No matching PDF and Manual file pairs found.\")\n",
        "                return None\n",
        "\n",
        "            processor.list_file_pairs()\n",
        "            response = input(f\"\\nStart detailed processing of these {added_count} documents? (y/n): \")\n",
        "            if response.lower() == 'y':\n",
        "                processor.run_batch_processing()\n",
        "            else:\n",
        "                print(\"Batch processing cancelled.\")\n",
        "\n",
        "        elif choice == '4':\n",
        "            # 快速元数据验证测试\n",
        "            quick_test_metadata_validation()\n",
        "\n",
        "        elif choice == '5':\n",
        "            # 显示使用示例\n",
        "            metadata_validation_usage_examples()\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid selection, running default single PDF processing\")\n",
        "            enhanced_main_with_detailed_output()\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nUser cancelled operation\")\n",
        "    except Exception as e:\n",
        "        print(f\"Execution error: {e}\")\n",
        "        print(\"Running default single PDF processing\")\n",
        "        enhanced_main_with_detailed_output()"
      ],
      "metadata": {
        "id": "fP6Nh5cT_rRu"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FileUploadManager:\n",
        "    \"\"\"文件上传和管理器\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.uploaded_files = []\n",
        "        self.pdf_files = []\n",
        "        self.manual_files = []\n",
        "        self.file_pairs = []\n",
        "\n",
        "        # 创建工作目录\n",
        "        self.work_dir = Path(\"/content/kpi_files\")\n",
        "        self.work_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        print(f\"📁 工作目录: {self.work_dir}\")\n",
        "\n",
        "    def upload_files_directly(self):\n",
        "        \"\"\"直接上传文件到Colab\"\"\"\n",
        "        print(\"📤 请选择要上传的文件...\")\n",
        "        print(\"可以同时选择多个PDF和Excel文件\")\n",
        "\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        for filename, content in uploaded.items():\n",
        "            file_path = self.work_dir / filename\n",
        "            with open(file_path, 'wb') as f:\n",
        "                f.write(content)\n",
        "\n",
        "            self.uploaded_files.append(str(file_path))\n",
        "            print(f\"✅ 已上传: {filename}\")\n",
        "\n",
        "        self._categorize_files()\n",
        "        return len(uploaded)\n",
        "\n",
        "    def mount_google_drive(self):\n",
        "        \"\"\"挂载Google Drive\"\"\"\n",
        "        try:\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"✅ Google Drive已挂载\")\n",
        "            print(\"📁 你的文件在: /content/drive/MyDrive/\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Drive挂载失败: {e}\")\n",
        "            return False\n",
        "\n",
        "    def scan_drive_directory(self, drive_path: str):\n",
        "        \"\"\"扫描Drive目录中的文件\"\"\"\n",
        "        drive_path = Path(drive_path)\n",
        "\n",
        "        if not drive_path.exists():\n",
        "            print(f\"❌ 目录不存在: {drive_path}\")\n",
        "            return 0\n",
        "\n",
        "        # 扫描PDF文件\n",
        "        pdf_files = list(drive_path.glob(\"*.pdf\"))\n",
        "        excel_files = list(drive_path.glob(\"*.xlsx\")) + list(drive_path.glob(\"*.xls\"))\n",
        "\n",
        "        print(f\"📊 发现文件:\")\n",
        "        print(f\"   PDF文件: {len(pdf_files)}个\")\n",
        "        print(f\"   Excel文件: {len(excel_files)}个\")\n",
        "\n",
        "        # 复制到工作目录\n",
        "        for pdf_file in pdf_files:\n",
        "            dest = self.work_dir / pdf_file.name\n",
        "            shutil.copy2(pdf_file, dest)\n",
        "            self.uploaded_files.append(str(dest))\n",
        "            print(f\"📄 复制PDF: {pdf_file.name}\")\n",
        "\n",
        "        for excel_file in excel_files:\n",
        "            dest = self.work_dir / excel_file.name\n",
        "            shutil.copy2(excel_file, dest)\n",
        "            self.uploaded_files.append(str(dest))\n",
        "            print(f\"📊 复制Excel: {excel_file.name}\")\n",
        "\n",
        "        self._categorize_files()\n",
        "        return len(pdf_files) + len(excel_files)\n",
        "\n",
        "    def _categorize_files(self):\n",
        "        \"\"\"分类文件\"\"\"\n",
        "        self.pdf_files = []\n",
        "        self.manual_files = []\n",
        "\n",
        "        for file_path in self.uploaded_files:\n",
        "            path = Path(file_path)\n",
        "            if path.suffix.lower() == '.pdf':\n",
        "                self.pdf_files.append(file_path)\n",
        "            elif path.suffix.lower() in ['.xlsx', '.xls']:\n",
        "                self.manual_files.append(file_path)\n",
        "\n",
        "        print(f\"\\n📋 文件分类完成:\")\n",
        "        print(f\"   PDF文件: {len(self.pdf_files)}个\")\n",
        "        print(f\"   Manual文件: {len(self.manual_files)}个\")\n",
        "\n",
        "    def auto_match_files(self) -> List[Tuple[str, str, str]]:\n",
        "        \"\"\"自动匹配PDF和Manual文件\"\"\"\n",
        "        matches = []\n",
        "\n",
        "        for pdf_path in self.pdf_files:\n",
        "            pdf_name = Path(pdf_path).stem\n",
        "\n",
        "            # 尝试多种匹配模式\n",
        "            potential_matches = []\n",
        "\n",
        "            for manual_path in self.manual_files:\n",
        "                manual_name = Path(manual_path).stem\n",
        "\n",
        "                # 匹配模式1: 完全相同\n",
        "                if pdf_name.lower() == manual_name.lower():\n",
        "                    potential_matches.append((manual_path, 1.0, \"完全匹配\"))\n",
        "\n",
        "                # 匹配模式2: PDF名称_manual\n",
        "                elif manual_name.lower() == f\"{pdf_name.lower()}_manual\":\n",
        "                    potential_matches.append((manual_path, 0.9, \"后缀匹配\"))\n",
        "\n",
        "                # 匹配模式3: manual_PDF名称\n",
        "                elif manual_name.lower() == f\"manual_{pdf_name.lower()}\":\n",
        "                    potential_matches.append((manual_path, 0.9, \"前缀匹配\"))\n",
        "\n",
        "                # 匹配模式4: 包含关系\n",
        "                elif pdf_name.lower() in manual_name.lower() or manual_name.lower() in pdf_name.lower():\n",
        "                    potential_matches.append((manual_path, 0.7, \"部分匹配\"))\n",
        "\n",
        "            # 选择最佳匹配\n",
        "            if potential_matches:\n",
        "                best_match = max(potential_matches, key=lambda x: x[1])\n",
        "                matches.append((pdf_path, best_match[0], pdf_name))\n",
        "                print(f\"✅ 匹配: {pdf_name} → {Path(best_match[0]).name} ({best_match[2]})\")\n",
        "            else:\n",
        "                print(f\"❌ 未找到匹配: {pdf_name}\")\n",
        "\n",
        "        self.file_pairs = matches\n",
        "        return matches\n",
        "\n",
        "    def manual_pair_files(self):\n",
        "        \"\"\"手动配对文件\"\"\"\n",
        "        print(\"\\n🔧 手动文件配对\")\n",
        "        print(\"=\" * 40)\n",
        "\n",
        "        self.file_pairs = []\n",
        "\n",
        "        print(\"可用的PDF文件:\")\n",
        "        for i, pdf_path in enumerate(self.pdf_files, 1):\n",
        "            print(f\"  {i}. {Path(pdf_path).name}\")\n",
        "\n",
        "        print(\"\\n可用的Manual文件:\")\n",
        "        for i, manual_path in enumerate(self.manual_files, 1):\n",
        "            print(f\"  {i}. {Path(manual_path).name}\")\n",
        "\n",
        "        for pdf_path in self.pdf_files:\n",
        "            pdf_name = Path(pdf_path).name\n",
        "            print(f\"\\n为PDF文件 '{pdf_name}' 选择Manual文件:\")\n",
        "\n",
        "            for i, manual_path in enumerate(self.manual_files, 1):\n",
        "                print(f\"  {i}. {Path(manual_path).name}\")\n",
        "\n",
        "            try:\n",
        "                choice = int(input(\"请输入Manual文件编号 (0跳过): \"))\n",
        "                if choice > 0 and choice <= len(self.manual_files):\n",
        "                    manual_path = self.manual_files[choice - 1]\n",
        "                    doc_name = Path(pdf_path).stem\n",
        "                    self.file_pairs.append((pdf_path, manual_path, doc_name))\n",
        "                    print(f\"✅ 配对成功: {pdf_name} → {Path(manual_path).name}\")\n",
        "                else:\n",
        "                    print(f\"⏭️ 跳过: {pdf_name}\")\n",
        "            except ValueError:\n",
        "                print(f\"⏭️ 输入无效，跳过: {pdf_name}\")\n",
        "\n",
        "    def validate_manual_files(self) -> Dict[str, bool]:\n",
        "        \"\"\"验证Manual文件格式\"\"\"\n",
        "        validation_results = {}\n",
        "\n",
        "        print(\"\\n🔍 验证Manual文件格式...\")\n",
        "\n",
        "        for manual_path in self.manual_files:\n",
        "            file_name = Path(manual_path).name\n",
        "            try:\n",
        "                df = pd.read_excel(manual_path)\n",
        "\n",
        "                # 检查必需列\n",
        "                required_columns = ['kpi_text']\n",
        "                missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "\n",
        "                if missing_columns:\n",
        "                    print(f\"❌ {file_name}: 缺少列 {missing_columns}\")\n",
        "                    validation_results[manual_path] = False\n",
        "                else:\n",
        "                    # 检查数据\n",
        "                    non_empty_rows = df['kpi_text'].notna().sum()\n",
        "                    print(f\"✅ {file_name}: {len(df)}行数据, {non_empty_rows}个有效KPI\")\n",
        "                    validation_results[manual_path] = True\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ {file_name}: 读取失败 - {e}\")\n",
        "                validation_results[manual_path] = False\n",
        "\n",
        "        return validation_results\n",
        "\n",
        "    def show_file_summary(self):\n",
        "        \"\"\"显示文件汇总\"\"\"\n",
        "        print(f\"\\n📊 文件上传汇总\")\n",
        "        print(\"=\" * 40)\n",
        "        print(f\"总文件数: {len(self.uploaded_files)}\")\n",
        "        print(f\"PDF文件: {len(self.pdf_files)}\")\n",
        "        print(f\"Manual文件: {len(self.manual_files)}\")\n",
        "        print(f\"配对文件: {len(self.file_pairs)}\")\n",
        "\n",
        "        if self.file_pairs:\n",
        "            print(f\"\\n📋 配对结果:\")\n",
        "            for i, (pdf_path, manual_path, doc_name) in enumerate(self.file_pairs, 1):\n",
        "                print(f\"  {i}. {doc_name}\")\n",
        "                print(f\"     PDF: {Path(pdf_path).name}\")\n",
        "                print(f\"     Manual: {Path(manual_path).name}\")\n",
        "\n",
        "    def get_file_pairs_for_batch_processing(self) -> List[Tuple[str, str, str]]:\n",
        "        \"\"\"获取用于批量处理的文件对\"\"\"\n",
        "        return self.file_pairs\n",
        "\n",
        "    def create_batch_processor_from_uploads(self):\n",
        "        \"\"\"从上传的文件创建批量处理器\"\"\"\n",
        "        if not self.file_pairs:\n",
        "            print(\"❌ 没有可用的文件对\")\n",
        "            return None\n",
        "\n",
        "        # 导入批量处理器\n",
        "        from your_main_script import create_batch_processor  # 需要替换为实际的导入\n",
        "\n",
        "        processor = create_batch_processor()\n",
        "\n",
        "        for pdf_path, manual_path, doc_name in self.file_pairs:\n",
        "            processor.add_file_pair(pdf_path, manual_path, doc_name)\n",
        "\n",
        "        return processor"
      ],
      "metadata": {
        "id": "InVqb0qIB7a0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ 便捷使用函数 ============\n",
        "\n",
        "def create_batch_processor():\n",
        "    \"\"\"A convenience function to create a batch KPI processor\"\"\"\n",
        "    return BatchKPIProcessor()\n",
        "\n",
        "def quick_batch_from_directories(pdf_dir: str, manual_dir: str):\n",
        "    \"\"\"Quickly create a batch process from two directories\"\"\"\n",
        "    processor = BatchKPIProcessor()\n",
        "\n",
        "    # 添加文件对\n",
        "    added_count = processor.add_multiple_pairs_from_directory(pdf_dir, manual_dir)\n",
        "\n",
        "    if added_count == 0:\n",
        "        print(\"❌ No matching PDF and Manual file pairs found.\")\n",
        "        return None\n",
        "\n",
        "    # 显示文件列表\n",
        "    processor.list_file_pairs()\n",
        "\n",
        "    # 询问是否继续\n",
        "    response = input(f\"\\nStart processing these {added_count} documents? (y/n): \")\n",
        "    if response.lower() == 'y':\n",
        "        processor.run_batch_processing()\n",
        "        return processor\n",
        "    else:\n",
        "        print(\"Batch processing cancelled.\")\n",
        "        return processor\n",
        "\n",
        "def manual_batch_setup():\n",
        "    \"\"\"Interactive function for manually configuring a batch process\"\"\"\n",
        "    processor = BatchKPIProcessor()\n",
        "\n",
        "    print(\"📋 Manual Batch Processing Setup\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    while True:\n",
        "        print(f\"\\nCurrently added {len(processor.file_pairs)} file pair(s)\")\n",
        "        print(\"1. Add a single file pair\")\n",
        "        print(\"2. Add multiple file pairs from directories\")\n",
        "        print(\"3. View added file pairs\")\n",
        "        print(\"4. Start batch processing\")\n",
        "        print(\"5. Exit\")\n",
        "\n",
        "        choice = input(\"Select an option (1-5): \")\n",
        "\n",
        "        if choice == '1':\n",
        "            pdf_path = input(\"PDF file path: \")\n",
        "            manual_path = input(\"Manual file path: \")\n",
        "            doc_name = input(\"Document name (press Enter for default): \").strip()\n",
        "\n",
        "            if not doc_name:\n",
        "                doc_name = None\n",
        "\n",
        "            processor.add_file_pair(pdf_path, manual_path, doc_name)\n",
        "\n",
        "        elif choice == '2':\n",
        "            pdf_dir = input(\"PDF directory: \")\n",
        "            manual_dir = input(\"Manual directory: \")\n",
        "            added = processor.add_multiple_pairs_from_directory(pdf_dir, manual_dir)\n",
        "            print(f\"{added} file pair(s) added.\")\n",
        "\n",
        "        elif choice == '3':\n",
        "            processor.list_file_pairs()\n",
        "\n",
        "        elif choice == '4':\n",
        "            if processor.file_pairs:\n",
        "                processor.run_batch_processing()\n",
        "                break\n",
        "            else:\n",
        "                print(\"❌ No file pairs added.\")\n",
        "\n",
        "        elif choice == '5':\n",
        "            print(\"Exiting batch setup.\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Invalid selection. Please try again.\")\n",
        "\n",
        "    return processor"
      ],
      "metadata": {
        "id": "Mc6cffCKCHDk"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced comparison function for the main code\n",
        "def enhanced_compare_with_manual_kpis(df_auto: pd.DataFrame, manual_xlsx_path: str,\n",
        "                                     output_dir: str = \"validation_results\") -> Dict[str, any]:\n",
        "    \"\"\"\n",
        "    增强版比较函数 - 显示详细的文本验证结果 + 元数据验证\n",
        "    \"\"\"\n",
        "    if not Path(manual_xlsx_path).exists():\n",
        "        logging.warning(f\"Manual KPI file not found: {manual_xlsx_path}\")\n",
        "        return {}\n",
        "\n",
        "    # Save auto KPIs to temporary file for validation pipeline\n",
        "    temp_auto_path = Path(output_dir) / \"temp_auto_kpis.xlsx\"\n",
        "    temp_auto_path.parent.mkdir(exist_ok=True)\n",
        "    df_auto.to_excel(temp_auto_path, index=False)\n",
        "\n",
        "    try:\n",
        "        # Step 1: 运行详细的文本验证\n",
        "        print(f\"\\n🔍 Running detailed text validation against {Path(manual_xlsx_path).name}...\")\n",
        "\n",
        "        validator = KPIValidationPipeline(\n",
        "            manual_excel_path=manual_xlsx_path,\n",
        "            auto_excel_path=str(temp_auto_path),\n",
        "            output_dir=output_dir\n",
        "        )\n",
        "\n",
        "        # Run full validation\n",
        "        text_validation_results = validator.run_full_validation()\n",
        "\n",
        "        # 显示详细的文本验证摘要\n",
        "        if text_validation_results and 'best_metrics' in text_validation_results:\n",
        "            best_metrics = text_validation_results['best_metrics']\n",
        "\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"KPI EXTRACTION VALIDATION SUMMARY\")\n",
        "            print(\"=\"*60)\n",
        "            print(f\"📊 Dataset: {len(validator.manual_df)} manual vs {len(validator.auto_df)} auto KPIs\")\n",
        "            print(f\"🎯 Best F1 Score: {best_metrics['f1_score']:.3f}\")\n",
        "            print(f\"📈 Precision: {best_metrics['precision']:.3f}\")\n",
        "            print(f\"📉 Recall: {best_metrics['recall']:.3f}\")\n",
        "            print(f\"✅ True Positives: {best_metrics['true_positives']}\")\n",
        "            print(f\"❌ False Positives: {best_metrics['false_positives']}\")\n",
        "            print(f\"⚠️  False Negatives: {best_metrics['false_negatives']}\")\n",
        "            print(\"=\"*60)\n",
        "            print(f\"📁 Results saved to: {output_dir}\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "        # Step 2: 运行元数据验证\n",
        "        print(f\"\\n📊 Running metadata validation...\")\n",
        "\n",
        "        metadata_validator = MetadataValidationExtension(validator)\n",
        "        metadata_results = metadata_validator.validate_metadata_for_matched_pairs()\n",
        "\n",
        "        # 保存元数据验证结果\n",
        "        metadata_saved_files = metadata_validator.save_metadata_validation_results(f\"{output_dir}/metadata_validation\")\n",
        "\n",
        "        # 显示元数据验证摘要\n",
        "        if metadata_results:\n",
        "            overall_scores = metadata_results.get('overall_scores', {})\n",
        "\n",
        "            print(f\"\\n📋 METADATA VALIDATION SUMMARY\")\n",
        "            print(\"=\"*60)\n",
        "            print(f\"📊 Metadata Overall Score: {overall_scores.get('overall_metadata_score', 0):.3f}\")\n",
        "            print(f\"🎯 Validated KPI Pairs: {metadata_results.get('total_pairs', 0)}\")\n",
        "\n",
        "            print(f\"\\n📂 Metadata Category Scores:\")\n",
        "            for category, score in overall_scores.items():\n",
        "                if category.endswith('_fields'):\n",
        "                    category_name = category.replace('_', ' ').title()\n",
        "                    print(f\"   {category_name}: {score:.3f}\")\n",
        "\n",
        "            # 显示问题字段\n",
        "            if metadata_results.get('problematic_field_analysis'):\n",
        "                print(f\"\\n⚠️  Fields Needing Attention:\")\n",
        "                for field_name, analysis in metadata_results['problematic_field_analysis'].items():\n",
        "                    priority_emoji = \"🔴\" if analysis['improvement_priority'] == 'high' else \"🟡\" if analysis['improvement_priority'] == 'medium' else \"🟢\"\n",
        "                    print(f\"   {priority_emoji} {field_name}: {analysis['accuracy']:.3f} ({analysis['improvement_priority']} priority)\")\n",
        "\n",
        "            print(\"=\"*60)\n",
        "            print(f\"📁 Metadata results saved to: {output_dir}/metadata_validation\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "        # 组合结果\n",
        "        complete_results = {\n",
        "            'text_validation': text_validation_results,\n",
        "            'metadata_validation': metadata_results,\n",
        "            'saved_files': {\n",
        "                'text_validation_files': text_validation_results.get('saved_files', {}),\n",
        "                'metadata_validation_files': metadata_saved_files\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Clean up temporary file\n",
        "        if temp_auto_path.exists():\n",
        "            temp_auto_path.unlink()\n",
        "\n",
        "        return complete_results\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Enhanced validation failed: {e}\")\n",
        "        if temp_auto_path.exists():\n",
        "            temp_auto_path.unlink()\n",
        "        return {}\n",
        "\n",
        "\n",
        "# Integration function for the main pipeline\n",
        "def run_kpi_extraction_with_validation():\n",
        "    \"\"\"Run KPI extraction with comprehensive validation\"\"\"\n",
        "    print(\"🚀 Starting KPI extraction with automated validation...\")\n",
        "\n",
        "    # Validate environment\n",
        "    if not validate_environment():\n",
        "        print(\"Please fix the environment issues before running.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Step 1: Run KPI extraction\n",
        "        print(\"\\n📊 Step 1: Extracting KPIs...\")\n",
        "        df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
        "\n",
        "        # Save auto results\n",
        "        save_results(df_auto, EXPORT_AUTO_XLSX, PDF_PATH)\n",
        "        print(f\"✅ Extracted {len(df_auto)} KPIs and saved to {EXPORT_AUTO_XLSX}\")\n",
        "\n",
        "        # Step 2: Run validation if manual file exists\n",
        "        if MANUAL_XLSX and Path(MANUAL_XLSX).exists():\n",
        "            print(f\"\\n🔍 Step 2: Running validation against {MANUAL_XLSX}...\")\n",
        "            validation_results = enhanced_compare_with_manual_kpis(\n",
        "                df_auto, MANUAL_XLSX, \"validation_results\"\n",
        "            )\n",
        "\n",
        "            if validation_results:\n",
        "                best_metrics = validation_results['best_metrics']\n",
        "                print(f\"\\n🎯 Validation completed!\")\n",
        "                print(f\"   F1 Score: {best_metrics['f1_score']:.3f}\")\n",
        "                print(f\"   Precision: {best_metrics['precision']:.3f}\")\n",
        "                print(f\"   Recall: {best_metrics['recall']:.3f}\")\n",
        "\n",
        "                return {\n",
        "                    'extracted_kpis': df_auto,\n",
        "                    'validation_results': validation_results\n",
        "                }\n",
        "            else:\n",
        "                print(\"⚠️ Validation failed, but extraction completed successfully\")\n",
        "                return {'extracted_kpis': df_auto}\n",
        "        else:\n",
        "            print(f\"\\n⚠️ Manual KPI file not found ({MANUAL_XLSX}), skipping validation\")\n",
        "            return {'extracted_kpis': df_auto}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Pipeline failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "\n",
        "# Batch validation function for multiple documents\n",
        "def run_batch_validation(pdf_list: List[str], manual_list: List[str],\n",
        "                        output_base_dir: str = \"batch_validation\"):\n",
        "    \"\"\"\n",
        "    Run validation across multiple PDF documents\n",
        "\n",
        "    Args:\n",
        "        pdf_list: List of PDF file paths\n",
        "        manual_list: List of corresponding manual annotation files\n",
        "        output_base_dir: Base directory for validation results\n",
        "    \"\"\"\n",
        "    batch_results = []\n",
        "\n",
        "    for i, (pdf_path, manual_path) in enumerate(zip(pdf_list, manual_list)):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing document {i+1}/{len(pdf_list)}: {Path(pdf_path).name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        try:\n",
        "            # Set up paths for this document\n",
        "            doc_name = Path(pdf_path).stem\n",
        "            doc_output_dir = Path(output_base_dir) / doc_name\n",
        "            doc_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            # Extract KPIs\n",
        "            global PDF_PATH\n",
        "            original_pdf_path = PDF_PATH\n",
        "            PDF_PATH = pdf_path\n",
        "\n",
        "            df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
        "            auto_excel_path = doc_output_dir / f\"{doc_name}_auto_kpis.xlsx\"\n",
        "            save_results(df_auto, str(auto_excel_path), PDF_PATH)\n",
        "\n",
        "            # Run validation\n",
        "            validation_results = enhanced_compare_with_manual_kpis(\n",
        "                df_auto, manual_path, str(doc_output_dir / \"validation\")\n",
        "            )\n",
        "\n",
        "            # Store results\n",
        "            doc_result = {\n",
        "                'document': doc_name,\n",
        "                'pdf_path': pdf_path,\n",
        "                'manual_path': manual_path,\n",
        "                'extracted_kpis': len(df_auto),\n",
        "                'validation_results': validation_results.get('best_metrics', {}),\n",
        "                'output_dir': str(doc_output_dir)\n",
        "            }\n",
        "            batch_results.append(doc_result)\n",
        "\n",
        "            # Restore original PDF path\n",
        "            PDF_PATH = original_pdf_path\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to process {doc_name}: {e}\")\n",
        "            batch_results.append({\n",
        "                'document': doc_name,\n",
        "                'pdf_path': pdf_path,\n",
        "                'manual_path': manual_path,\n",
        "                'error': str(e)\n",
        "            })\n",
        "\n",
        "    # Generate batch summary\n",
        "    batch_summary_path = Path(output_base_dir) / \"batch_summary.xlsx\"\n",
        "    batch_df = pd.DataFrame(batch_results)\n",
        "    batch_df.to_excel(batch_summary_path, index=False)\n",
        "\n",
        "    print(f\"\\n🎉 Batch validation completed!\")\n",
        "    print(f\"📊 Processed {len(pdf_list)} documents\")\n",
        "    print(f\"📁 Results saved to {output_base_dir}\")\n",
        "    print(f\"📋 Summary available at {batch_summary_path}\")\n",
        "\n",
        "    return batch_results\n",
        "\n",
        "\n",
        "# Quick validation function for testing\n",
        "def quick_validation_test(manual_xlsx: str = None, auto_xlsx: str = None):\n",
        "    \"\"\"Quick validation test with existing files\"\"\"\n",
        "    manual_file = manual_xlsx or MANUAL_XLSX\n",
        "    auto_file = auto_xlsx or EXPORT_AUTO_XLSX\n",
        "\n",
        "    if not Path(manual_file).exists():\n",
        "        print(f\"❌ Manual file not found: {manual_file}\")\n",
        "        return None\n",
        "\n",
        "    if not Path(auto_file).exists():\n",
        "        print(f\"❌ Auto file not found: {auto_file}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"🔍 Quick validation test:\")\n",
        "    print(f\"  Manual: {manual_file}\")\n",
        "    print(f\"  Auto: {auto_file}\")\n",
        "\n",
        "    try:\n",
        "        validator = KPIValidationPipeline(\n",
        "            manual_excel_path=manual_file,\n",
        "            auto_excel_path=auto_file,\n",
        "            output_dir=\"quick_validation\"\n",
        "        )\n",
        "\n",
        "        results = validator.run_full_validation()\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Quick validation failed: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Performance benchmarking function\n",
        "def benchmark_extraction_methods():\n",
        "    \"\"\"Benchmark different extraction methods with validation\"\"\"\n",
        "    methods = {\n",
        "        'text_only': process_text_only,\n",
        "        'with_images': process_sustainability_report_with_enhanced_images,\n",
        "        'optimized': process_sustainability_report_OPTIMIZED\n",
        "    }\n",
        "\n",
        "    benchmark_results = {}\n",
        "\n",
        "    for method_name, method_func in methods.items():\n",
        "        print(f\"\\n🧪 Benchmarking {method_name}...\")\n",
        "\n",
        "        try:\n",
        "            import time\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Run extraction\n",
        "            df_result = method_func(PDF_PATH)\n",
        "            extraction_time = time.time() - start_time\n",
        "\n",
        "            # Save results\n",
        "            method_output = f\"{method_name}_{EXPORT_AUTO_XLSX}\"\n",
        "            save_results(df_result, method_output, PDF_PATH)\n",
        "\n",
        "            # Run validation if manual file exists\n",
        "            validation_metrics = {}\n",
        "            if MANUAL_XLSX and Path(MANUAL_XLSX).exists():\n",
        "                validation_results = enhanced_compare_with_manual_kpis(\n",
        "                    df_result, MANUAL_XLSX, f\"benchmark_{method_name}\"\n",
        "                )\n",
        "                if validation_results:\n",
        "                    validation_metrics = validation_results['best_metrics']\n",
        "\n",
        "            benchmark_results[method_name] = {\n",
        "                'extraction_time': extraction_time,\n",
        "                'kpi_count': len(df_result),\n",
        "                'kpis_per_second': len(df_result) / extraction_time,\n",
        "                'validation_metrics': validation_metrics\n",
        "            }\n",
        "\n",
        "            print(f\"✅ {method_name}: {len(df_result)} KPIs in {extraction_time:.1f}s\")\n",
        "            if validation_metrics:\n",
        "                print(f\"   F1: {validation_metrics.get('f1_score', 0):.3f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ {method_name} failed: {e}\")\n",
        "            benchmark_results[method_name] = {'error': str(e)}\n",
        "\n",
        "    # Save benchmark results\n",
        "    benchmark_df = pd.DataFrame(benchmark_results).T\n",
        "    benchmark_df.to_excel(\"extraction_benchmark.xlsx\")\n",
        "\n",
        "    print(f\"\\n🏆 Benchmark completed!\")\n",
        "    print(f\"📊 Results saved to extraction_benchmark.xlsx\")\n",
        "\n",
        "    return benchmark_results\n",
        "\n",
        "\n",
        "# Usage examples and documentation\n",
        "def validation_usage_examples():\n",
        "    \"\"\"Show usage examples for the validation pipeline\"\"\"\n",
        "    print(\"\"\"\n",
        "# KPI Validation Pipeline Usage Examples\n",
        "\n",
        "## 1. Basic validation with existing files\n",
        "```python\n",
        "validator = KPIValidationPipeline(\n",
        "    manual_excel_path=\"manual_kpis.xlsx\",\n",
        "    auto_excel_path=\"auto_kpis.xlsx\"\n",
        ")\n",
        "results = validator.run_full_validation()\n",
        "```\n",
        "\n",
        "## 2. Integrated extraction + validation\n",
        "```python\n",
        "results = run_kpi_extraction_with_validation()\n",
        "```\n",
        "\n",
        "## 3. Quick validation test\n",
        "```python\n",
        "results = quick_validation_test(\"manual.xlsx\", \"auto.xlsx\")\n",
        "```\n",
        "\n",
        "## 4. Batch validation for multiple documents\n",
        "```python\n",
        "pdf_files = [\"doc1.pdf\", \"doc2.pdf\", \"doc3.pdf\"]\n",
        "manual_files = [\"manual1.xlsx\", \"manual2.xlsx\", \"manual3.xlsx\"]\n",
        "batch_results = run_batch_validation(pdf_files, manual_files)\n",
        "```\n",
        "\n",
        "## 5. Benchmark different extraction methods\n",
        "```python\n",
        "benchmark_results = benchmark_extraction_methods()\n",
        "```\n",
        "\n",
        "## 6. Custom threshold analysis\n",
        "```python\n",
        "validator = KPIValidationPipeline(\"manual.xlsx\", \"auto.xlsx\")\n",
        "validator.run_comprehensive_evaluation()\n",
        "\n",
        "# Check performance at different thresholds\n",
        "for threshold in [0.5, 0.7, 0.9]:\n",
        "    metrics = validator.calculate_metrics_at_threshold(threshold)\n",
        "    print(f\"Threshold {threshold}: F1={metrics['f1_score']:.3f}\")\n",
        "```\n",
        "\n",
        "## Output Files Generated:\n",
        "- validation_results.json - Complete results in JSON format\n",
        "- detailed_matches.xlsx - All matched KPIs with similarity scores\n",
        "- error_analysis.xlsx - False positives and false negatives\n",
        "- validation_report.md - Human-readable report\n",
        "- threshold_analysis.xlsx - Performance across different thresholds\n",
        "- validation_visualizations.png - Comprehensive charts and graphs\n",
        "\n",
        "## Key Metrics Explained:\n",
        "- **Precision**: % of auto KPIs that match manual annotations\n",
        "- **Recall**: % of manual KPIs found by automatic extraction\n",
        "- **F1 Score**: Harmonic mean of precision and recall\n",
        "- **True Positives**: Correctly identified KPIs\n",
        "- **False Positives**: Auto KPIs not in manual annotations\n",
        "- **False Negatives**: Manual KPIs missed by extraction\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "vv0x-kG2_odA"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 修改主函数，显示完整的详细信息\n",
        "def enhanced_main_with_detailed_output():\n",
        "    \"\"\"增强版main函数，显示详细的验证输出\"\"\"\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format=\"%(asctime)s - %(levelname)s: %(message)s\",\n",
        "        datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(PDF_PATH):\n",
        "            logging.error(f\"PDF file not found: {PDF_PATH}\")\n",
        "            return\n",
        "\n",
        "        # Step 1: KPI提取\n",
        "        print(f\"\\n🚀 Starting KPI extraction from {Path(PDF_PATH).name}...\")\n",
        "        df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
        "        save_results(df_auto, EXPORT_AUTO_XLSX, PDF_PATH)\n",
        "        print(f\"✅ KPI extraction completed: {len(df_auto)} KPIs extracted\")\n",
        "\n",
        "        # Step 2: 详细验证（如果有manual文件）\n",
        "        if MANUAL_XLSX and Path(MANUAL_XLSX).exists():\n",
        "            validation_start_time = time.time()\n",
        "\n",
        "            print(f\"\\n🔍 Starting comprehensive validation against {Path(MANUAL_XLSX).name}...\")\n",
        "            complete_validation_results = enhanced_compare_with_manual_kpis(\n",
        "                df_auto, MANUAL_XLSX, \"comprehensive_validation\"\n",
        "            )\n",
        "\n",
        "            validation_time = time.time() - validation_start_time\n",
        "\n",
        "            if complete_validation_results:\n",
        "                # 显示综合摘要\n",
        "                text_metrics = complete_validation_results['text_validation'].get('best_metrics', {})\n",
        "                metadata_scores = complete_validation_results['metadata_validation'].get('overall_scores', {})\n",
        "\n",
        "                print(f\"\\n🎯 Verification completed:\")\n",
        "                print(f\"   Text F1 Score: {text_metrics.get('f1_score', 0):.3f}\")\n",
        "                print(f\"   Text Precision: {text_metrics.get('precision', 0):.3f}\")\n",
        "                print(f\"   Text Recall: {text_metrics.get('recall', 0):.3f}\")\n",
        "                print(f\"   Metadata Overall Score: {metadata_scores.get('overall_metadata_score', 0):.3f}\")\n",
        "                print(f\"⏱️  Validation time: {validation_time:.1f}秒\")\n",
        "                print(f\"📁 Complete results saved to: comprehensive_validation/\")\n",
        "\n",
        "            else:\n",
        "                print(\"⚠️ Validation encountered issues, but extraction completed successfully\")\n",
        "        else:\n",
        "            logging.info(\"Manual KPI file not found, skipping validation.\")\n",
        "\n",
        "        # Step 3: 显示提取摘要\n",
        "        if not df_auto.empty:\n",
        "            print(f\"\\n=== EXTRACTION SUMMARY ===\")\n",
        "            print(f\"Total KPIs extracted: {len(df_auto)}\")\n",
        "\n",
        "            if 'source_type' in df_auto.columns:\n",
        "                source_counts = df_auto['source_type'].value_counts()\n",
        "                print(f\"From text/tables: {source_counts.get('text', 0)}\")\n",
        "                print(f\"From images/charts: {source_counts.get('image', 0)}\")\n",
        "\n",
        "            if 'kpi_theme' in df_auto.columns:\n",
        "                theme_counts = df_auto['kpi_theme'].value_counts()\n",
        "                print(f\"\\nKPI Distribution by Theme:\")\n",
        "                for theme, count in theme_counts.items():\n",
        "                    print(f\"  {theme}: {count}\")\n",
        "        else:\n",
        "            print(\"\\nNo KPIs were extracted from the document.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in enhanced main execution: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "cZqmV6ik8iHV"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建详细输出的批量处理器\n",
        "def create_batch_processor_with_detailed_output():\n",
        "    \"\"\"创建支持详细输出的批量处理器\"\"\"\n",
        "    return BatchKPIProcessorWithDetailedOutput()\n",
        "\n",
        "# 修改集成执行函数\n",
        "def integrated_main_execution_detailed():\n",
        "    \"\"\"集成的主执行函数 - 详细输出版本\"\"\"\n",
        "    print(\"🚀 Enhanced KPI Extraction System - Detailed Output\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"1. Single PDF processing (detailed text + metadata validation)\")\n",
        "    print(\"2. Batch PDF processing (detailed text + metadata validation)\")\n",
        "    print(\"3. Quick batch from directories (detailed output)\")\n",
        "    print(\"4. Quick metadata validation test\")\n",
        "    print(\"5. View usage examples\")\n",
        "\n",
        "    try:\n",
        "        choice = input(\"Please select processing mode (1-5): \")\n",
        "\n",
        "        if choice == '1':\n",
        "            # 单个PDF处理，详细输出\n",
        "            enhanced_main_with_detailed_output()\n",
        "\n",
        "        elif choice == '2':\n",
        "            # 批量处理，详细输出\n",
        "            processor = create_batch_processor_with_detailed_output()\n",
        "\n",
        "            # 让用户手动添加文件对\n",
        "            while True:\n",
        "                pdf_path = input(\"Enter PDF file path (press Enter to finish): \").strip()\n",
        "                if not pdf_path:\n",
        "                    break\n",
        "                manual_path = input(\"Enter corresponding Manual file path: \").strip()\n",
        "                doc_name = input(\"Document name (press Enter for default): \").strip() or None\n",
        "\n",
        "                processor.add_file_pair(pdf_path, manual_path, doc_name)\n",
        "\n",
        "            if processor.file_pairs:\n",
        "                processor.list_file_pairs()\n",
        "                confirm = input(f\"\\nStart detailed processing of these {len(processor.file_pairs)} documents? (y/n): \")\n",
        "                if confirm.lower() == 'y':\n",
        "                    processor.run_batch_processing()\n",
        "                else:\n",
        "                    print(\"Batch processing cancelled.\")\n",
        "            else:\n",
        "                print(\"❌ No file pairs added.\")\n",
        "\n",
        "        elif choice == '3':\n",
        "            # 快速目录批量处理，详细输出\n",
        "            pdf_dir = input(\"PDF files directory path: \").strip()\n",
        "            manual_dir = input(\"Manual files directory path: \").strip()\n",
        "\n",
        "            processor = create_batch_processor_with_detailed_output()\n",
        "            added_count = processor.add_multiple_pairs_from_directory(pdf_dir, manual_dir)\n",
        "\n",
        "            if added_count == 0:\n",
        "                print(\"❌ No matching PDF and Manual file pairs found.\")\n",
        "                return None\n",
        "\n",
        "            processor.list_file_pairs()\n",
        "            response = input(f\"\\nStart detailed processing of these {added_count} documents? (y/n): \")\n",
        "            if response.lower() == 'y':\n",
        "                processor.run_batch_processing()\n",
        "            else:\n",
        "                print(\"Batch processing cancelled.\")\n",
        "\n",
        "        elif choice == '4':\n",
        "            # 快速元数据验证测试\n",
        "            quick_test_metadata_validation()\n",
        "\n",
        "        elif choice == '5':\n",
        "            # 显示使用示例\n",
        "            metadata_validation_usage_examples()\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid selection, running default single PDF processing\")\n",
        "            enhanced_main_with_detailed_output()\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nUser cancelled operation\")\n",
        "    except Exception as e:\n",
        "        print(f\"Execution error: {e}\")\n",
        "        print(\"Running default single PDF processing\")\n",
        "        enhanced_main_with_detailed_output()\n",
        "\n",
        "# 更新主执行部分\n",
        "if __name__ == \"__main__\":\n",
        "    # 使用详细输出的集成执行函数\n",
        "    integrated_main_execution_detailed()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbBRhgS1ARfC",
        "outputId": "e31986cf-493e-4f2f-f27b-9056f29c8968"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Enhanced KPI Extraction System - Detailed Output\n",
            "============================================================\n",
            "1. Single PDF processing (detailed text + metadata validation)\n",
            "2. Batch PDF processing (detailed text + metadata validation)\n",
            "3. Quick batch from directories (detailed output)\n",
            "4. Quick metadata validation test\n",
            "5. View usage examples\n",
            "Please select processing mode (1-5): 2\n",
            "Enter PDF file path (press Enter to finish): /content/NewNew.pdf\n",
            "Enter corresponding Manual file path: /content/NewNew.xlsx\n",
            "Document name (press Enter for default): NewNew\n",
            "Enter PDF file path (press Enter to finish): /content/fujit.pdf\n",
            "Enter corresponding Manual file path: /content/fujit.xlsx\n",
            "Document name (press Enter for default): fujit\n",
            "Enter PDF file path (press Enter to finish): /content/Cineplex.pdf\n",
            "Enter corresponding Manual file path: /content/Cineplex.xlsx\n",
            "Document name (press Enter for default): Cineplex\n",
            "Enter PDF file path (press Enter to finish): \n",
            "\n",
            "📋 已添加的文件对 (共 3 对):\n",
            "--------------------------------------------------------------------------------\n",
            " 1. 文档: NewNew\n",
            "    PDF:    /content/NewNew.pdf\n",
            "    Manual: /content/NewNew.xlsx\n",
            "\n",
            " 2. 文档: fujit\n",
            "    PDF:    /content/fujit.pdf\n",
            "    Manual: /content/fujit.xlsx\n",
            "\n",
            " 3. 文档: Cineplex\n",
            "    PDF:    /content/Cineplex.pdf\n",
            "    Manual: /content/Cineplex.xlsx\n",
            "\n",
            "\n",
            "Start detailed processing of these 3 documents? (y/n): y\n",
            "\n",
            "🚀 开始批量处理 3 个文档...\n",
            "📁 结果将保存到: batch_kpi_results/batch_20250731_110839\n",
            "\n",
            "================================================================================\n",
            "📄 Processing Document 1/3: NewNew\n",
            "================================================================================\n",
            "📊 Step 1: Extracting KPIs from NewNew.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extraction completed: 55 KPIs\n",
            "\n",
            "🔍 Step 2: Running comprehensive validation against NewNew.xlsx...\n",
            "\n",
            "🔍 Running detailed text validation against NewNew.xlsx...\n",
            "\n",
            "============================================================\n",
            "KPI EXTRACTION VALIDATION SUMMARY\n",
            "============================================================\n",
            "📊 Dataset: 46 manual vs 55 auto KPIs\n",
            "🎯 Best F1 Score: 0.911\n",
            "📈 Precision: 0.836\n",
            "📉 Recall: 1.000\n",
            "✅ True Positives: 46\n",
            "❌ False Positives: 9\n",
            "⚠️  False Negatives: 0\n",
            "============================================================\n",
            "📁 Results saved to: batch_kpi_results/batch_20250731_110839/doc_1_NewNew/validation\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "KPI EXTRACTION VALIDATION SUMMARY\n",
            "============================================================\n",
            "📊 Dataset: 46 manual vs 55 auto KPIs\n",
            "🎯 Best F1 Score: 0.911\n",
            "📈 Precision: 0.836\n",
            "📉 Recall: 1.000\n",
            "✅ True Positives: 46\n",
            "❌ False Positives: 9\n",
            "⚠️  False Negatives: 0\n",
            "============================================================\n",
            "📁 Results saved to: batch_kpi_results/batch_20250731_110839/doc_1_NewNew/validation\n",
            "============================================================\n",
            "\n",
            "📊 Running metadata validation...\n",
            "\n",
            "📋 METADATA VALIDATION SUMMARY\n",
            "============================================================\n",
            "📊 Metadata Overall Score: 0.925\n",
            "🎯 Validated KPI Pairs: 46\n",
            "\n",
            "📂 Metadata Category Scores:\n",
            "   Document Fields: 1.000\n",
            "   Classification Fields: 1.000\n",
            "   Quantitative Fields: 0.748\n",
            "   Analysis Fields: 0.986\n",
            "\n",
            "⚠️  Fields Needing Attention:\n",
            "   🟢 Absolute Page Number: 1.000 (low priority)\n",
            "   🟢 kpi_theme: 1.000 (low priority)\n",
            "   🔴 quantitative_value: 0.696 (high priority)\n",
            "============================================================\n",
            "📁 Metadata results saved to: batch_kpi_results/batch_20250731_110839/doc_1_NewNew/validation/metadata_validation\n",
            "============================================================\n",
            "\n",
            "🎯 Document 1 Verification Completed:\n",
            "   📊 Dataset: 46 manual vs 55 auto KPIs\n",
            "   🎯 Text F1 Score: 0.911\n",
            "   📈 Text Precision: 0.836\n",
            "   📉 Text Recall: 1.000\n",
            "   ✅ True Positives: 46\n",
            "   ❌ False Positives: 9\n",
            "   ⚠️  False Negatives: 0\n",
            "   📊 Metadata Overall Score: 0.925\n",
            "⏱️  Processing time: 522.2秒 (validation: 127.5秒)\n",
            "📁 Results saved to: batch_kpi_results/batch_20250731_110839/doc_1_NewNew\n",
            "\n",
            "================================================================================\n",
            "📄 Processing Document 2/3: fujit\n",
            "================================================================================\n",
            "📊 Step 1: Extracting KPIs from fujit.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:API response not JSON list: Based on the provided text, here are the extracted KPIs:\n",
            "\n",
            "```json\n",
            "[\n",
            "    {\n",
            "        \"kpi_text\": \"In FY...\n",
            "WARNING:root:API response not JSON list: Based on the provided text, here are the extracted KPIs:\n",
            "\n",
            "```json\n",
            "[]\n",
            "```\n",
            "\n",
            "The text provided does not...\n",
            "WARNING:root:API response not JSON list: Based on the provided text, there are no specific quantifiable KPIs that meet the criteria outlined....\n",
            "WARNING:root:API response not JSON list: The provided text does not contain any specific numbers, percentages, or measurable quantities that ...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It prim...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It prim...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains a table with quantifiable performance data related to environmental management sy...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains a table with quantifiable performance data related to environmental management sy...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a page from the Fujitsu Group Environmental Report 2015, focusing on Environme...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a page from an environmental report, primarily containing text and diagrams re...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It prim...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a page from a report, primarily containing text and a table. Here's the analys...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a text-based document from the Fujitsu Group Environmental Report 2015, focusi...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a text-based document from the Fujitsu Group Environmental Report 2015, focusi...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It show...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It show...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a page from a report, but it does not contain any charts or graphs with quanti...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a page from an environmental report, but it does not contain any charts or gra...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image does not contain any charts or graphs with quantifiable performance data. It appears to be...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a page from a report, but it does not contain any charts or graphs with quanti...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It prim...\n",
            "WARNING:root:Image analysis response not JSON list: I'm unable to analyze the image as it doesn't contain any charts or graphs. If you have a different ...\n",
            "WARNING:root:Image analysis response not JSON list: I'm unable to analyze the image as it doesn't contain any charts or graphs. If you have a different ...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any visible charts or graphs with quantifiable performance data....\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any visible charts or graphs with quantifiable performance data....\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a page from a report and does not contain any charts or graphs with quantifiab...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It prim...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extraction completed: 31 KPIs\n",
            "\n",
            "🔍 Step 2: Running comprehensive validation against fujit.xlsx...\n",
            "\n",
            "🔍 Running detailed text validation against fujit.xlsx...\n",
            "\n",
            "============================================================\n",
            "KPI EXTRACTION VALIDATION SUMMARY\n",
            "============================================================\n",
            "📊 Dataset: 12 manual vs 31 auto KPIs\n",
            "🎯 Best F1 Score: 0.326\n",
            "📈 Precision: 0.226\n",
            "📉 Recall: 0.583\n",
            "✅ True Positives: 7\n",
            "❌ False Positives: 24\n",
            "⚠️  False Negatives: 5\n",
            "============================================================\n",
            "📁 Results saved to: batch_kpi_results/batch_20250731_110839/doc_2_fujit/validation\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "KPI EXTRACTION VALIDATION SUMMARY\n",
            "============================================================\n",
            "📊 Dataset: 12 manual vs 31 auto KPIs\n",
            "🎯 Best F1 Score: 0.326\n",
            "📈 Precision: 0.226\n",
            "📉 Recall: 0.583\n",
            "✅ True Positives: 7\n",
            "❌ False Positives: 24\n",
            "⚠️  False Negatives: 5\n",
            "============================================================\n",
            "📁 Results saved to: batch_kpi_results/batch_20250731_110839/doc_2_fujit/validation\n",
            "============================================================\n",
            "\n",
            "📊 Running metadata validation...\n",
            "\n",
            "📋 METADATA VALIDATION SUMMARY\n",
            "============================================================\n",
            "📊 Metadata Overall Score: 0.945\n",
            "🎯 Validated KPI Pairs: 7\n",
            "\n",
            "📂 Metadata Category Scores:\n",
            "   Document Fields: 1.000\n",
            "   Classification Fields: 1.000\n",
            "   Quantitative Fields: 0.807\n",
            "   Analysis Fields: 1.000\n",
            "\n",
            "⚠️  Fields Needing Attention:\n",
            "   🟢 Absolute Page Number: 1.000 (low priority)\n",
            "   🟢 kpi_theme: 1.000 (low priority)\n",
            "   🟢 quantitative_value: 0.286 (urgent priority)\n",
            "============================================================\n",
            "📁 Metadata results saved to: batch_kpi_results/batch_20250731_110839/doc_2_fujit/validation/metadata_validation\n",
            "============================================================\n",
            "\n",
            "🎯 Document 2 Verification Completed:\n",
            "   📊 Dataset: 12 manual vs 31 auto KPIs\n",
            "   🎯 Text F1 Score: 0.326\n",
            "   📈 Text Precision: 0.226\n",
            "   📉 Text Recall: 0.583\n",
            "   ✅ True Positives: 7\n",
            "   ❌ False Positives: 24\n",
            "   ⚠️  False Negatives: 5\n",
            "   📊 Metadata Overall Score: 0.945\n",
            "⏱️  Processing time: 342.5秒 (validation: 31.8秒)\n",
            "📁 Results saved to: batch_kpi_results/batch_20250731_110839/doc_2_fujit\n",
            "\n",
            "================================================================================\n",
            "📄 Processing Document 3/3: Cineplex\n",
            "================================================================================\n",
            "📊 Step 1: Extracting KPIs from Cineplex.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:API response not JSON list: The provided text does not contain any specific numbers, percentages, or measurable quantities that ...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a logo and does not contain any charts or graphs with quantifiable performance...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a logo and does not contain any charts or graphs with quantifiable performance...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a text document discussing Cineplex's Corporate Social Responsibility approach...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a text document discussing Cineplex's approach to Corporate Social Responsibil...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a text document and does not contain any charts or graphs with quantifiable pe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a text document and does not contain any charts or graphs. Therefore, there ar...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains text with quantifiable performance data related to inclusivity, diversity, and ac...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains text with quantifiable performance data related to inclusivity, diversity, and cu...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It is a...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It is a...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extraction completed: 4 KPIs\n",
            "\n",
            "🔍 Step 2: Running comprehensive validation against Cineplex.xlsx...\n",
            "\n",
            "🔍 Running detailed text validation against Cineplex.xlsx...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:PDF file not found: /content/Test_Unknown_northwest-sustainability-report-2022_fbqow68f-60-74.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "KPI EXTRACTION VALIDATION SUMMARY\n",
            "============================================================\n",
            "📊 Dataset: 6 manual vs 4 auto KPIs\n",
            "🎯 Best F1 Score: 0.800\n",
            "📈 Precision: 1.000\n",
            "📉 Recall: 0.667\n",
            "✅ True Positives: 4\n",
            "❌ False Positives: 0\n",
            "⚠️  False Negatives: 2\n",
            "============================================================\n",
            "📁 Results saved to: batch_kpi_results/batch_20250731_110839/doc_3_Cineplex/validation\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "KPI EXTRACTION VALIDATION SUMMARY\n",
            "============================================================\n",
            "📊 Dataset: 6 manual vs 4 auto KPIs\n",
            "🎯 Best F1 Score: 0.800\n",
            "📈 Precision: 1.000\n",
            "📉 Recall: 0.667\n",
            "✅ True Positives: 4\n",
            "❌ False Positives: 0\n",
            "⚠️  False Negatives: 2\n",
            "============================================================\n",
            "📁 Results saved to: batch_kpi_results/batch_20250731_110839/doc_3_Cineplex/validation\n",
            "============================================================\n",
            "\n",
            "📊 Running metadata validation...\n",
            "\n",
            "📋 METADATA VALIDATION SUMMARY\n",
            "============================================================\n",
            "📊 Metadata Overall Score: 0.835\n",
            "🎯 Validated KPI Pairs: 4\n",
            "\n",
            "📂 Metadata Category Scores:\n",
            "   Document Fields: 1.000\n",
            "   Classification Fields: 0.430\n",
            "   Quantitative Fields: 0.914\n",
            "   Analysis Fields: 1.000\n",
            "\n",
            "⚠️  Fields Needing Attention:\n",
            "   🟢 Absolute Page Number: 1.000 (low priority)\n",
            "   🟡 kpi_theme: 0.500 (medium priority)\n",
            "   🟢 quantitative_value: 1.000 (low priority)\n",
            "============================================================\n",
            "📁 Metadata results saved to: batch_kpi_results/batch_20250731_110839/doc_3_Cineplex/validation/metadata_validation\n",
            "============================================================\n",
            "\n",
            "🎯 Document 3 Verification Completed:\n",
            "   📊 Dataset: 6 manual vs 4 auto KPIs\n",
            "   🎯 Text F1 Score: 0.800\n",
            "   📈 Text Precision: 1.000\n",
            "   📉 Text Recall: 0.667\n",
            "   ✅ True Positives: 4\n",
            "   ❌ False Positives: 0\n",
            "   ⚠️  False Negatives: 2\n",
            "   📊 Metadata Overall Score: 0.835\n",
            "⏱️  Processing time: 110.8秒 (validation: 5.8秒)\n",
            "📁 Results saved to: batch_kpi_results/batch_20250731_110839/doc_3_Cineplex\n",
            "Execution error: \"['validation_f1_score', 'validation_precision', 'validation_recall'] not in index\"\n",
            "Running default single PDF processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oe8TGqRNEk_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ibmTPOJu5b5r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1IoxrHG3joQz9CaGgenAnsjP3sGaEFA9h",
      "authorship_tag": "ABX9TyMe8E15UOSL0zCA5sln0oC1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}