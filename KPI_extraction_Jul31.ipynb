{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esemsc-rw1024/irp-rw1024-public/blob/main/KPI_extraction_Jul31.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "extract_sustainability_kpi.py\n",
        "==================================\n",
        "Automatically extract KPI sentences/table rows from Sustainability Report PDF\n",
        "and compare with manual KPI annotations\n",
        "--------------------------------------------------\n",
        "1. pdfplumber extracts text + tables\n",
        "2. Camelot supplements complex table parsing (optional)\n",
        "3. Chunking to control tokens\n",
        "4. OpenAI ChatCompletion API call (GPT-4o / GPT-4 / GPT-3.5)\n",
        "5. Aggregate, deduplicate, and export to auto_kpi.xlsx\n",
        "6. Compare with manual_kpi.xlsx for differences\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eDB9oRTEZoZw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "645bb6ea-2f68-4bb9-bad5-cae3d1a984e5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nextract_sustainability_kpi.py\\n==================================\\nAutomatically extract KPI sentences/table rows from Sustainability Report PDF\\nand compare with manual KPI annotations\\n--------------------------------------------------\\n1. pdfplumber extracts text + tables\\n2. Camelot supplements complex table parsing (optional)\\n3. Chunking to control tokens\\n4. OpenAI ChatCompletion API call (GPT-4o / GPT-4 / GPT-3.5)\\n5. Aggregate, deduplicate, and export to auto_kpi.xlsx\\n6. Compare with manual_kpi.xlsx for differences\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "id": "LaGnafXLZxfg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "3w2Bya8SZob3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai python-dotenv pdfplumber tiktoken pandas\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install -y ghostscript\n",
        "!pip install \"camelot-py[cv]\"\n",
        "!pip install PyMuPDF Pillow\n",
        "!pip install -q transformers pillow torchvision"
      ],
      "metadata": {
        "id": "JSakj9TyZodt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffc309aa-5954-4ce6-ce4b-a45e60c7b119"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.97.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.7)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,168 kB]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,775 kB]\n",
            "Fetched 11.9 MB in 6s (1,859 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ghostscript is already the newest version (9.55.0~dfsg1-0ubuntu5.12).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n",
            "Requirement already satisfied: camelot-py[cv] in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "\u001b[33mWARNING: camelot-py 1.0.0 does not provide the extra 'cv'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (8.2.1)\n",
            "Requirement already satisfied: chardet>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (5.2.0)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (2.0.2)\n",
            "Requirement already satisfied: openpyxl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (3.1.5)\n",
            "Requirement already satisfied: pdfminer-six>=20240706 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (20250506)\n",
            "Requirement already satisfied: pypdf<4.0,>=3.17 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (3.17.4)\n",
            "Requirement already satisfied: pandas>=2.2.2 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (2.2.2)\n",
            "Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (0.9.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.7.0.68 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (4.12.0.88)\n",
            "Requirement already satisfied: pypdfium2>=4 in /usr/local/lib/python3.11/dist-packages (from camelot-py[cv]) (4.30.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl>=3.1.0->camelot-py[cv]) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.2->camelot-py[cv]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.2->camelot-py[cv]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.2->camelot-py[cv]) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer-six>=20240706->camelot-py[cv]) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer-six>=20240706->camelot-py[cv]) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[cv]) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.2->camelot-py[cv]) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[cv]) (2.22)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, time, textwrap, argparse, logging\n",
        "import pdfplumber, pandas as pd, tiktoken\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from typing import List, Dict, Optional, Set, Tuple\n",
        "from pathlib import Path\n",
        "from difflib import SequenceMatcher\n",
        "import base64\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import fitz  # PyMuPDF\n",
        "import numpy as np\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import concurrent.futures\n",
        "import hashlib\n",
        "import pickle\n",
        "# Âú®Áé∞ÊúâÁöÑÂØºÂÖ•ËØ≠Âè•ÂêéÊ∑ªÂä†Ëøô‰∫õÊñ∞ÁöÑÂØºÂÖ•\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "rNQHu6_fZofh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------- Configuration -----------------------------\n",
        "PDF_PATH          = \"/content/Test_Unknown_northwest-sustainability-report-2022_fbqow68f-60-74.pdf\"\n",
        "MANUAL_XLSX       = \"manual_kpi.xlsx\"   # Leave empty if not available\n",
        "EXPORT_AUTO_XLSX  = \"auto_kpi.xlsx\"\n",
        "MODEL_NAME        = \"gpt-4o\"       # Adjust based on account availability\n",
        "MAX_TOKENS_CHUNK  = 1500               # Token limit per chunk\n",
        "SLEEP_SEC         = 0.6                # Rate limiting\n",
        "ENABLE_QUALITY_VALIDATION = True       # Enable additional quality checks\n",
        "# -----------------------------------------------------------------"
      ],
      "metadata": {
        "id": "L9B6ORg1Zohn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Fixed initialization part ============\n",
        "def initialize_environment():\n",
        "    \"\"\"Initialize the environment and API client\"\"\"\n",
        "    # Load environment variables\n",
        "    load_dotenv(\"ruojia_api_key.env\")\n",
        "\n",
        "    # Initialize OpenAI client\n",
        "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"OPENAI_API_KEY not found in environment variables!\")\n",
        "\n",
        "    client = OpenAI(api_key=api_key)\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "    return client, enc\n",
        "\n",
        "# Initialize global variables\n",
        "client, enc = initialize_environment()"
      ],
      "metadata": {
        "id": "mi52QoSbZoja"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Fixed PDF text extraction ============\n",
        "def pdf_to_text_and_tables(path: str) -> str:\n",
        "    \"\"\"Extract text paragraphs and tables using pdfplumber.\"\"\"\n",
        "    all_chunks = []\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"PDF file not found: {path}\")\n",
        "\n",
        "    try:\n",
        "        with pdfplumber.open(path) as pdf:\n",
        "            logging.info(f\"Processing PDF with {len(pdf.pages)} pages...\")\n",
        "\n",
        "            for page_num, page in enumerate(pdf.pages, 1):\n",
        "                try:\n",
        "                    # Extract text\n",
        "                    text = page.extract_text() or \"\"\n",
        "                    if text.strip():\n",
        "                        all_chunks.append(f\"PAGE_{page_num}_TEXT:\\n{text}\")\n",
        "\n",
        "                    # Extract tables\n",
        "                    tables = page.extract_tables()\n",
        "                    for table_num, tb in enumerate(tables):\n",
        "                        if tb and len(tb) > 0:\n",
        "                            try:\n",
        "                                # Handle table headers safely\n",
        "                                if tb[0]:\n",
        "                                    headers = tb[0]\n",
        "                                else:\n",
        "                                    headers = [f\"Col_{i}\" for i in range(len(tb[1]) if len(tb) > 1 else 1)]\n",
        "\n",
        "                                rows = tb[1:] if len(tb) > 1 else []\n",
        "\n",
        "                                if rows:\n",
        "                                    df = pd.DataFrame(rows, columns=headers)\n",
        "                                    # Clean DataFrame\n",
        "                                    df = df.dropna(how='all')  # Remove empty rows\n",
        "                                    if not df.empty:\n",
        "                                        table_txt = f\"TABLE_START_PAGE_{page_num}_{table_num}\\n\" + df.to_csv(index=False) + \"\\nTABLE_END\"\n",
        "                                        all_chunks.append(table_txt)\n",
        "                            except Exception as e:\n",
        "                                logging.warning(f\"Error processing table on page {page_num}: {e}\")\n",
        "                                continue\n",
        "\n",
        "                except Exception as e:\n",
        "                    logging.warning(f\"Error processing page {page_num}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        return \"\\n\\n\".join(all_chunks)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error opening PDF file: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "IkSbr5pqaGsO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Fixed Camelot table extraction ============\n",
        "def generate_table_fingerprint(df: pd.DataFrame) -> str:\n",
        "    \"\"\"Generate table fingerprint for deduplication\"\"\"\n",
        "    try:\n",
        "        fingerprint_parts = []\n",
        "        fingerprint_parts.append(f\"shape_{df.shape[0]}x{df.shape[1]}\")\n",
        "\n",
        "        if not df.columns.empty:\n",
        "            col_names = [str(col).strip().lower().replace(' ', '') for col in df.columns]\n",
        "            col_fingerprint = '_'.join(sorted(col_names))\n",
        "            fingerprint_parts.append(f\"cols_{hash(col_fingerprint)}\")\n",
        "\n",
        "        if df.shape[0] > 0:\n",
        "            numeric_values = []\n",
        "            for col in df.columns:\n",
        "                for val in df[col].head(3):\n",
        "                    if pd.notna(val):\n",
        "                        numbers = re.findall(r'\\d+\\.?\\d*', str(val))\n",
        "                        numeric_values.extend(numbers)\n",
        "\n",
        "            if numeric_values:\n",
        "                numeric_fingerprint = hash('_'.join(sorted(numeric_values[:10])))\n",
        "                fingerprint_parts.append(f\"nums_{numeric_fingerprint}\")\n",
        "\n",
        "        return '_'.join(fingerprint_parts)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error generating table fingerprint: {e}\")\n",
        "        return str(hash(df.to_csv()))\n",
        "\n",
        "def clean_table_data_improved(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Improved table data cleaning\"\"\"\n",
        "    try:\n",
        "        cleaned_df = df.copy()\n",
        "        cleaned_df = cleaned_df.dropna(how='all')\n",
        "        cleaned_df = cleaned_df.dropna(axis=1, how='all')\n",
        "\n",
        "        for col in cleaned_df.columns:\n",
        "            if cleaned_df[col].dtype == 'object':\n",
        "                cleaned_df[col] = cleaned_df[col].astype(str).str.strip()\n",
        "                cleaned_df[col] = cleaned_df[col].replace(['nan', 'NaN', 'None'], '')\n",
        "\n",
        "        if not cleaned_df.empty:\n",
        "            new_columns = []\n",
        "            for i, col in enumerate(cleaned_df.columns):\n",
        "                col_str = str(col).strip()\n",
        "                if col_str in ['nan', 'NaN', 'None', ''] or pd.isna(col):\n",
        "                    new_columns.append(f'Column_{i}')\n",
        "                else:\n",
        "                    new_columns.append(col_str)\n",
        "            cleaned_df.columns = new_columns\n",
        "\n",
        "        cleaned_df = cleaned_df.reset_index(drop=True)\n",
        "        return cleaned_df\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error in table cleaning: {e}\")\n",
        "        return df\n",
        "\n",
        "def is_valid_table_improved(df: pd.DataFrame) -> bool:\n",
        "    \"\"\"Improved table validation\"\"\"\n",
        "    try:\n",
        "        if df.empty or df.shape[0] < 1 or df.shape[1] < 1:\n",
        "            return False\n",
        "\n",
        "        non_null_cells = 0\n",
        "        total_cells = df.shape[0] * df.shape[1]\n",
        "\n",
        "        for col in df.columns:\n",
        "            for val in df[col]:\n",
        "                if pd.notna(val) and str(val).strip() not in ['', 'nan', 'NaN', 'None']:\n",
        "                    non_null_cells += 1\n",
        "\n",
        "        if non_null_cells / total_cells < 0.2:\n",
        "            return False\n",
        "\n",
        "        has_meaningful_content = False\n",
        "        for col in df.columns:\n",
        "            text_content = ' '.join(df[col].dropna().astype(str))\n",
        "            if (any(char.isdigit() for char in text_content) or\n",
        "                '%' in text_content or\n",
        "                any(keyword in text_content.lower() for keyword in [\n",
        "                    'rate', 'percentage', 'total', 'number', 'emission', 'energy',\n",
        "                    'water', 'waste', 'employee', 'year', '2020', '2021', '2022', '2023'\n",
        "                ])):\n",
        "                has_meaningful_content = True\n",
        "                break\n",
        "\n",
        "        return has_meaningful_content\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error validating table: {e}\")\n",
        "        return True\n",
        "\n",
        "def format_table_output_improved(df: pd.DataFrame, table_id: str, parsing_report=None) -> str:\n",
        "    \"\"\"Improved table output formatting\"\"\"\n",
        "    try:\n",
        "        table_info = f\"TABLE_START_{table_id}\\n\"\n",
        "        table_info += f\"DIMENSIONS: {df.shape[0]} rows √ó {df.shape[1]} columns\\n\"\n",
        "\n",
        "        col_info = \"COLUMNS: \" + \" | \".join([f\"{i}:{col}\" for i, col in enumerate(df.columns)])\n",
        "        table_info += col_info + \"\\n\"\n",
        "\n",
        "        if df.shape[0] > 0:\n",
        "            preview_rows = min(2, df.shape[0])\n",
        "            table_info += f\"PREVIEW_FIRST_{preview_rows}_ROWS:\\n\"\n",
        "            for i in range(preview_rows):\n",
        "                row_preview = \" | \".join([str(df.iloc[i, j])[:20] for j in range(min(5, df.shape[1]))])\n",
        "                table_info += f\"  Row_{i}: {row_preview}\\n\"\n",
        "\n",
        "        if parsing_report:\n",
        "            try:\n",
        "                accuracy = getattr(parsing_report, 'accuracy', 'N/A')\n",
        "                if accuracy != 'N/A':\n",
        "                    table_info += f\"EXTRACTION_ACCURACY: {accuracy:.2f}\\n\"\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        table_info += \"TABLE_DATA_START\\n\"\n",
        "        table_csv = df.to_csv(index=False, na_rep='', quoting=1, escapechar='\\\\')\n",
        "        table_end = f\"TABLE_DATA_END\\nTABLE_END_{table_id}\\n\"\n",
        "\n",
        "        return table_info + table_csv + table_end\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error formatting table output: {e}\")\n",
        "        return f\"TABLE_START_{table_id}\\n{df.to_csv(index=False)}\\nTABLE_END_{table_id}\\n\"\n",
        "\n",
        "def camelot_extra_tables_enhanced(path: str) -> List[str]:\n",
        "    \"\"\"Enhanced table extraction using Camelot with better error handling\"\"\"\n",
        "    try:\n",
        "        import camelot\n",
        "    except ImportError:\n",
        "        logging.warning(\"Camelot not installed, skipping Camelot table parsing.\")\n",
        "        return []\n",
        "\n",
        "    extra_chunks = []\n",
        "    extracted_tables_fingerprints = set()\n",
        "\n",
        "    try:\n",
        "        logging.info(\"Starting Camelot table extraction...\")\n",
        "\n",
        "        # Stream mode extraction\n",
        "        try:\n",
        "            stream_tables = camelot.read_pdf(\n",
        "                path,\n",
        "                pages=\"all\",\n",
        "                flavor=\"stream\",\n",
        "                edge_tol=50,\n",
        "                row_tol=2,\n",
        "                column_tol=0\n",
        "            )\n",
        "\n",
        "            stream_count = 0\n",
        "            for i, table in enumerate(stream_tables):\n",
        "                if not table.df.empty and table.df.shape[0] > 0:\n",
        "                    table_fingerprint = generate_table_fingerprint(table.df)\n",
        "\n",
        "                    if table_fingerprint not in extracted_tables_fingerprints:\n",
        "                        cleaned_df = clean_table_data_improved(table.df)\n",
        "\n",
        "                        if is_valid_table_improved(cleaned_df):\n",
        "                            table_txt = format_table_output_improved(cleaned_df, f\"STREAM_{i}\", table.parsing_report)\n",
        "                            extra_chunks.append(table_txt)\n",
        "                            extracted_tables_fingerprints.add(table_fingerprint)\n",
        "                            stream_count += 1\n",
        "\n",
        "            logging.info(f\"Stream mode extracted {stream_count} valid tables\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Stream mode extraction failed: {e}\")\n",
        "\n",
        "        # Lattice mode extraction\n",
        "        try:\n",
        "            lattice_tables = camelot.read_pdf(\n",
        "                path,\n",
        "                pages=\"all\",\n",
        "                flavor=\"lattice\",\n",
        "                line_scale=15,\n",
        "                line_tol=2,\n",
        "                joint_tol=2\n",
        "            )\n",
        "\n",
        "            lattice_count = 0\n",
        "            for i, table in enumerate(lattice_tables):\n",
        "                if not table.df.empty and table.df.shape[0] > 0:\n",
        "                    table_fingerprint = generate_table_fingerprint(table.df)\n",
        "\n",
        "                    if table_fingerprint not in extracted_tables_fingerprints:\n",
        "                        cleaned_df = clean_table_data_improved(table.df)\n",
        "\n",
        "                        if is_valid_table_improved(cleaned_df):\n",
        "                            table_txt = format_table_output_improved(cleaned_df, f\"LATTICE_{i}\", table.parsing_report)\n",
        "                            extra_chunks.append(table_txt)\n",
        "                            extracted_tables_fingerprints.add(table_fingerprint)\n",
        "                            lattice_count += 1\n",
        "\n",
        "            logging.info(f\"Lattice mode extracted {lattice_count} additional unique tables\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Lattice mode extraction failed: {e}\")\n",
        "\n",
        "        total_extracted = len(extra_chunks)\n",
        "        logging.info(f\"Camelot extraction completed: {total_extracted} total unique tables extracted\")\n",
        "        return extra_chunks\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Camelot table extraction failed: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "amKWHBAHaGuk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Text Chunking ============\n",
        "def split_into_chunks(full_text: str, max_tokens: int) -> List[str]:\n",
        "    \"\"\"Split text into chunks based on token limit\"\"\"\n",
        "    paragraphs = [p for p in full_text.split(\"\\n\") if p.strip()]\n",
        "    chunks, current = [], []\n",
        "    current_tokens = 0\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        para_tokens = len(enc.encode(paragraph))\n",
        "\n",
        "        if current_tokens + para_tokens > max_tokens and current:\n",
        "            chunks.append(\"\\n\".join(current))\n",
        "            current = [paragraph]\n",
        "            current_tokens = para_tokens\n",
        "        else:\n",
        "            current.append(paragraph)\n",
        "            current_tokens += para_tokens\n",
        "\n",
        "    if current:\n",
        "        chunks.append(\"\\n\".join(current))\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "ZSLAWykuaGxs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ System prompt words ============\n",
        "UNIVERSAL_SYSTEM_PROMPT = textwrap.dedent(\"\"\"\n",
        "    You are a professional ESG data analyst specializing in extracting Key Performance Indicators (KPIs) from sustainability reports.\n",
        "\n",
        "    ## CRITICAL: What is a KPI?\n",
        "    A KPI MUST contain SPECIFIC NUMBERS, PERCENTAGES, or MEASURABLE QUANTITIES that demonstrate actual performance or concrete targets.\n",
        "\n",
        "    ## IMPORTANT: Table Data Processing Rules\n",
        "    When processing table data:\n",
        "    1. Pay close attention to column headers to identify the correct time periods\n",
        "    2. Match data values with their corresponding year columns\n",
        "    3. If you see table format like \"Metric, 2021, 2022\" - the first number after metric belongs to 2021, second to 2022\n",
        "    4. Look for table headers that indicate year columns (e.g., \"2020\", \"2021\", \"2022\")\n",
        "    5. Extract each year's data as separate KPIs\n",
        "    6. Avoid extracting the same KPI multiple times - consolidate similar metrics\n",
        "\n",
        "    ## ENHANCED: Advanced Table Processing\n",
        "    7. **EXTRACT ALL DATA POINTS**: For each table cell containing a number, create a separate KPI\n",
        "    8. **REGIONAL/LOCATION DATA**: Pay special attention to location-specific data (countries, regions, cities)\n",
        "    9. **WORKFORCE DATA**: Extract all employee numbers, headcount data, and demographic information\n",
        "    10. **INCOMPLETE DATA**: Extract available data even if some cells are empty or missing\n",
        "    11. **TOTALS AND SUBTOTALS**: Always extract total values and aggregated numbers\n",
        "\n",
        "    ## ‚úÖ VALID KPI EXAMPLES:\n",
        "    - \"Achieved 89.4% reuse and recycle rate for cloud hardware in 2023\"\n",
        "    - \"Diverted over 18,537 metric tons of waste from landfills in 2023\"\n",
        "    - \"Reduced single-use plastics in product packaging to 2.7%\"\n",
        "    - \"Contracted 19 GW of new renewable energy across 16 countries in 2024\"\n",
        "    - \"Provided clean water access to over 1.5 million people in 2023\"\n",
        "    - \"Protected 15,849 acres of land‚Äîexceeding target by more than 30%\"\n",
        "    - \"Allocated 761 million toward innovative climate technologies\"\n",
        "    - \"Achieved 80% renewable energy operations by 2024\"\n",
        "    - \"Water replenishment projects estimated to provide over 25 million cubic meters\"\n",
        "    - \"Exceeded annual target to divert 75% of construction waste by reaching 85%\"\n",
        "    - \"Board independence: 78% of directors\"\n",
        "    - \"Women in senior leadership increased to 35% in 2023\"\n",
        "    - \"Employee engagement score: 87% in annual survey\"\n",
        "    - \"Reduced greenhouse gas emissions by 50% compared to 2019 baseline\"\n",
        "    - \"Zero workplace fatalities achieved for third consecutive year\"\n",
        "    - \"Training completion rate: 98% for mandatory compliance courses\"\n",
        "    - \"Supplier ESG assessments completed for 95% of tier-1 suppliers\"\n",
        "    - \"Customer satisfaction rating: 4.6 out of 5.0\"\n",
        "    - \"Data breach incidents: 0 material breaches in 2023\"\n",
        "\n",
        "    ## ‚ùå NOT KPIs (DO NOT EXTRACT):\n",
        "    - \"Microsoft will require select suppliers to use carbon-free electricity by 2030\"\n",
        "    - \"The company plans to expand Sustainability Manager capabilities\"\n",
        "    - \"We are launching two new Circular Centers in 2023\"\n",
        "    - \"The organization established a new climate innovation fund\"\n",
        "    - \"Microsoft introduced enhanced data governance solutions\"\n",
        "    - \"Updated guidebook to include guidance on corporate responsibility\"\n",
        "    - \"Plans to publish new ESG strategy\"\n",
        "    - \"Implemented a new recycling program\"\n",
        "    - \"Conducted sustainability training sessions\"\n",
        "    - \"Launched employee wellness programs\"\n",
        "    - \"Committed to reducing emissions\"\n",
        "    - \"Focusing on environmental performance\"\n",
        "    - \"Established sustainability committee\"\n",
        "    - \"The company operates facilities in multiple regions\"\n",
        "    - \"Our supply chain includes thousands of vendors globally\"\n",
        "    - Any text without specific numbers, percentages, or quantifiable metrics\n",
        "    - Duplicate or repeated metrics (extract only once per time period)\n",
        "    - Any statement that describes business operations rather than performance outcomes\n",
        "\n",
        "    ## KPI Categories:\n",
        "    ### Environmental:\n",
        "    - **Carbon_Climate**: GHG emissions, carbon footprint, emission reductions, climate targets, scope 1/2/3 emissions, carbon intensity, carbon offsets, TCFD alignment\n",
        "    - **Energy**: Energy consumption, renewable energy percentage, energy efficiency, energy intensity, MWh, GWh, energy savings, fossil fuel usage\n",
        "    - **Water**: Water withdrawal, water consumption, water intensity, water recycling, water reuse, water stress, water discharge quality\n",
        "    - **Waste**: Waste generation, recycling rates, diversion percentages, hazardous waste, non-hazardous waste, zero waste to landfill, e-waste, incineration\n",
        "    - **Biodiversity**: Protected areas, species conservation, habitat restoration, biodiversity impact assessments, land use, ecosystem restoration\n",
        "    - **Circular_Economy**: Recycling rates, material recovery, circular design, raw materials usage, renewable materials, packaging waste\n",
        "    - **Materials**: Raw materials consumption, recycled content, sustainable materials, material intensity, sustainable sourcing\n",
        "\n",
        "    ### Social:\n",
        "    - **Workforce_Diversity**: Employee demographics, gender diversity, age diversity, ethnic diversity, disability inclusion, LGBTQ+ inclusion, workforce composition\n",
        "    - **Gender_Equality**: Women in leadership, gender pay ratio, parental leave return rates, gender representation, female employees percentage\n",
        "    - **Disability_Inclusion**: Employees with disabilities, accessibility compliance, inclusive workplace design, disability support programs\n",
        "    - **Health_Safety**: Lost Time Injury Frequency Rate (LTIFR), Total Recordable Incident Rate (TRIR), fatalities, workplace illness, safety training hours, PPE compliance, emergency drills\n",
        "    - **Employee_Wellbeing**: Employee satisfaction, retention rates, turnover rates, training hours, wellness programs, mental health services, work-life balance\n",
        "    - **Community_Engagement**: Corporate volunteering, social investment, community impact assessments, local hiring, stakeholder engagement activities\n",
        "    - **Human_Rights**: Child labor incidents, forced labor, human rights due diligence, freedom of association, grievance mechanisms, labor audits\n",
        "    - **Labor_Rights**: Collective bargaining coverage, labor complaints resolution, supplier labor audits, working conditions, fair wages\n",
        "    - **Customer_Safety**: Product safety incidents, customer satisfaction, accessibility features, safety recalls, quality metrics\n",
        "    - **Supply_Chain_Social**: Supplier assessments, sustainable sourcing, supplier code compliance, supply chain audits\n",
        "\n",
        "    ### Governance:\n",
        "    - **Board_Governance**: Board independence, board diversity, CEO-chair separation, board ESG expertise, board composition, director tenure\n",
        "    - **Executive_Compensation**: ESG-linked compensation, executive pay ratios, compensation disclosure, incentive structures\n",
        "    - **Ethics_Compliance**: Code of conduct training, corruption incidents, bribery cases, fines and penalties, whistleblower reports, anti-corruption assessments\n",
        "    - **Transparency_Disclosure**: ESG reporting coverage, third-party assurance, political contributions disclosure, GRI/SASB/TCFD compliance\n",
        "    - **Risk_Management**: Risk assessments, mitigation measures, climate risk disclosure, operational risk management\n",
        "    - **Cybersecurity_Data**: Cybersecurity breaches, data privacy policies, cybersecurity training, GDPR compliance, data protection measures\n",
        "    - **Supply_Chain_Governance**: Supplier ESG screening, supplier audits, procurement ESG clauses, vendor compliance rates\n",
        "\n",
        "    ## MANDATORY Requirements:\n",
        "    1. MUST contain specific numbers (e.g., 25%, 15,000, 2.5M, 8.5%, 0.3 per million hours)\n",
        "    2. MUST relate to measurable sustainability outcomes\n",
        "    3. MUST have time reference (year, period, or deadline)\n",
        "    4. MUST be performance-focused (results, not activities or descriptions)\n",
        "    5. MUST NOT be future plans or operational descriptions\n",
        "\n",
        "    ## Output Format:\n",
        "    Return a JSON array. Each KPI must contain:\n",
        "    ```json\n",
        "    {\n",
        "        \"kpi_text\": \"Complete original sentence with the quantifiable metric\",\n",
        "        \"kpi_theme\": \"Environmental/Social/Governance\",\n",
        "        \"kpi_category\": \"Specific category from above list\",\n",
        "        \"quantitative_value\": \"The specific number/percentage extracted\",\n",
        "        \"unit\": \"Unit of measurement (%, tonnes, employees, etc.)\",\n",
        "        \"time_period\": \"Time reference (2023, annual, by 2030, etc.)\",\n",
        "        \"target_or_actual\": \"Target/Actual/Both\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "    ## Additional Instructions:\n",
        "    - If a sentence includes a comparison value, such as a baseline, previous year, or other historical/target data (e.g., \"Compared to 32,395 MWh in 2020\"), extract it as a **separate KPI**.\n",
        "    - Do NOT store the comparison in any other field ‚Äî just create another valid KPI from it.\n",
        "    - Avoid merging multiple numerical values into one KPI unless they are clearly part of the same metric (e.g., male: X, female: Y).\n",
        "\n",
        "    ## STRICT FILTERING:\n",
        "    - Return empty array [] if no quantifiable KPIs found\n",
        "    - Only extract text that contains specific measurable values\n",
        "    - Ignore all qualitative statements, plans, and descriptions\n",
        "    - Focus only on numerical performance data\n",
        "\n",
        "    Now analyze the following text for sustainability KPIs:\n",
        "\"\"\").strip()\n",
        "\n",
        "# üî• Êñ∞Â¢ûÔºöÂ¢ûÂº∫ÁöÑÂõæÂÉèÂàÜÊûêPrompt\n",
        "ENHANCED_IMAGE_KPI_SYSTEM_PROMPT = textwrap.dedent(\"\"\"\n",
        "    You are an expert data analyst specializing in extracting quantifiable KPI data from charts, graphs, and data visualizations in sustainability reports.\n",
        "\n",
        "    ## CRITICAL INSTRUCTION: ALWAYS EXTRACT NUMERICAL VALUES\n",
        "\n",
        "    **Your primary task is to extract the ACTUAL NUMBERS and PERCENTAGES visible in charts, not just descriptions.**\n",
        "\n",
        "    ## MISSION:\n",
        "    Extract ALL quantifiable data points from charts and graphs, including:\n",
        "    - Bar charts (vertical/horizontal)\n",
        "    - Pie charts and donut charts\n",
        "    - Line charts and trend graphs\n",
        "    - Stacked charts and combo charts\n",
        "    - Tables with numerical data\n",
        "    - Infographics with statistics\n",
        "    - Gauge charts and dashboards\n",
        "\n",
        "    ## DETAILED ANALYSIS INSTRUCTIONS:\n",
        "\n",
        "    ### For PIE CHARTS:\n",
        "    1. Read percentage labels on each slice\n",
        "    2. If no labels visible, estimate based on slice size\n",
        "    3. Identify what each slice represents (categories)\n",
        "    4. Extract each slice as separate KPI\n",
        "    5. **MUST read the percentage labels on each slice** - Look for numbers like 64%, 33%, 68%, 30%, etc.\n",
        "    6. **If percentages are visible on the chart, extract them exactly**\n",
        "    7. **If no labels visible, estimate based on slice size using these guidelines:**\n",
        "       - 90¬∞ slice = 25%\n",
        "       - 180¬∞ slice = 50%\n",
        "       - 270¬∞ slice = 75%\n",
        "       - Full circle = 100%\n",
        "    8. **Each slice MUST have a specific percentage value in the final output**\n",
        "\n",
        "    ### For BAR CHARTS:\n",
        "    1. Read Y-axis scale carefully (units, increments)\n",
        "    2. Estimate bar heights using grid lines and scale\n",
        "    3. Read X-axis labels (years, categories, regions)\n",
        "    4. Extract each bar as separate KPI\n",
        "    5. Pay attention to grouped/stacked bars\n",
        "\n",
        "    ### For LINE CHARTS:\n",
        "    1. Read data points at intersection of grid lines\n",
        "    2. Follow trend lines to extract values for each time period\n",
        "    3. Use Y-axis scale for value estimation\n",
        "    4. Extract each data point as separate KPI\n",
        "\n",
        "    ### For TABLES:\n",
        "    1. Read all numerical values in cells\n",
        "    2. Match values with row and column headers\n",
        "    3. Extract each cell with numerical data as KPI\n",
        "\n",
        "    ## MANDATORY VALUE EXTRACTION RULES:\n",
        "\n",
        "    **RULE 1**: Every KPI MUST contain a specific numerical value (percentage, amount, count, etc.)\n",
        "    **RULE 2**: For charts with categories, you MUST find and extract the quantitative values for each category\n",
        "    **RULE 3**: Never create KPIs without specific numbers - descriptions alone are incomplete\n",
        "    **RULE 4**: Include complete context: what + how much + when/where if available\n",
        "\n",
        "\n",
        "    ## VALUE ESTIMATION GUIDELINES:\n",
        "    - Use proportional analysis: if a bar reaches 80% of scale maximum, calculate 80% of max value\n",
        "    - For pie charts: estimate slice angles (90¬∞ = 25%, 180¬∞ = 50%, etc.)\n",
        "    - Cross-reference with any visible data labels or legends\n",
        "    - Be conservative but reasonably accurate in estimates\n",
        "\n",
        "    ## CHART IDENTIFICATION:\n",
        "    First identify the chart type, then apply appropriate extraction method.\n",
        "    Look for:\n",
        "    - Axes and scales\n",
        "    - Data labels and legends\n",
        "    - Grid lines for reference\n",
        "    - Color coding and patterns\n",
        "    - Title and subtitle information\n",
        "\n",
        "    ## OUTPUT FORMAT:\n",
        "    Return a JSON array. For each data point found:\n",
        "    ```json\n",
        "    {\n",
        "        \"kpi_text\": \"Complete description with the ACTUAL NUMERICAL VALUE included\",\n",
        "        \"kpi_theme\": \"Environmental/Social/Governance\",\n",
        "        \"kpi_category\": \"Specific category based on content\",\n",
        "        \"quantitative_value\": \"The exact number/percentage (e.g., '64', '33.5', '68')\",\n",
        "        \"unit\": \"% / tonnes / employees / MWh / USD / etc.\",\n",
        "        \"time_period\": \"2021/2020/2022/Year/period/etc if identifiable\",\n",
        "        \"target_or_actual\": \"Actual\",\n",
        "        \"chart_type\": \"pie_chart/bar_chart/line_chart/table/etc\",\n",
        "        \"estimation_confidence\": \"High/Medium/Low\",\n",
        "        \"chart_title\": \"Chart title if visible\",\n",
        "        \"data_source\": \"Legend or source if visible\"\n",
        "    }\n",
        "\n",
        "    ```\n",
        "    ## EXAMPLES of CORRECT vs INCORRECT extraction:\n",
        "\n",
        "    ### ‚ùå INCORRECT (incomplete - missing numerical values):\n",
        "    ```json\n",
        "    {\n",
        "        \"kpi_text\": \"Energy consumption by facility type\",\n",
        "        \"quantitative_value\": \"\",\n",
        "        \"unit\": \"%\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "    ### ‚úÖ CORRECT (complete with specific values):\n",
        "    ```json\n",
        "    {\n",
        "        \"kpi_text\": \"Office buildings account for 45% of total energy consumption\",\n",
        "        \"quantitative_value\": \"45\",\n",
        "        \"unit\": \"%\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "    ### ‚ùå INCORRECT (category without value):\n",
        "    ```json\n",
        "    {\n",
        "        \"kpi_text\": \"Renewable energy percentage by region\",\n",
        "        \"quantitative_value\": \"\",\n",
        "        \"unit\": \"%\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "    ### ‚úÖ CORRECT (specific regional data):\n",
        "    ```json\n",
        "    {\n",
        "        \"kpi_text\": \"North America achieved 78% renewable energy usage\",\n",
        "        \"quantitative_value\": \"78\",\n",
        "        \"unit\": \"%\"\n",
        "    }\n",
        "    ```\n",
        "    ## QUALITY ASSURANCE CHECKLIST:\n",
        "    Before returning results, verify:\n",
        "    - ‚úÖ Every KPI contains a specific numerical value\n",
        "    - ‚úÖ Chart categories are paired with their quantitative data\n",
        "    - ‚úÖ KPI descriptions are complete and self-explanatory\n",
        "    - ‚úÖ Units are correctly identified and specified\n",
        "    - ‚úÖ Context (time, location, category) is preserved when available\n",
        "    - Each KPI must have a specific numerical value\n",
        "    - Context must be clear and self-contained\n",
        "    - Avoid extracting the same data point multiple times\n",
        "    - Focus on sustainability/ESG metrics when possible\n",
        "\n",
        "    ## VALUE ESTIMATION GUIDELINES:\n",
        "    - **High confidence**: Numbers clearly visible in image\n",
        "    - **Medium confidence**: Numbers estimated using chart scales/grid lines\n",
        "    - **Low confidence**: Values approximated from proportional analysis\n",
        "    - **If no numerical data is visible, return empty array []**\n",
        "\n",
        "    ## IMPORTANT NOTES:\n",
        "    - Extract ALL visible data points, not just main highlights\n",
        "    - Include context in descriptions (e.g., \"According to pie chart showing emission sources\")\n",
        "    - If values are not clearly visible, make reasonable estimates and mark confidence as \"Low\"\n",
        "    - Return empty array [] ONLY if image contains no charts/graphs with quantifiable data\n",
        "    - For multi-year data, create separate KPIs for each year\n",
        "    - Pay special attention to small text and numbers\n",
        "    - Focus on extracting actual performance data, not just identifying chart elements\n",
        "    - If you can see numbers in the image, you MUST extract them\n",
        "    - Pie chart percentages are usually the most important data points\n",
        "    - Return empty array [] ONLY if no numerical data is visible\n",
        "\n",
        "    Now analyze the provided image and extract ALL quantifiable KPI data points:\n",
        "\"\"\").strip()"
      ],
      "metadata": {
        "id": "IonQ4PK2aGzy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ KPI Extraction Function ============\n",
        "def extract_page_from_chunk(chunk: str) -> str:\n",
        "    \"\"\"Extract page information from chunk\"\"\"\n",
        "    # Look for PAGE_X_TEXT: format\n",
        "    page_matches = re.findall(r'PAGE_(\\d+)_TEXT:', chunk)\n",
        "    if page_matches:\n",
        "        pages = [int(p) for p in page_matches]\n",
        "        if len(pages) == 1:\n",
        "            return str(pages[0])\n",
        "        else:\n",
        "            return f\"{min(pages)}-{max(pages)}\"\n",
        "\n",
        "    # Look for TABLE_START_PAGE_X_\n",
        "    table_matches = re.findall(r'TABLE_START_PAGE_(\\d+)_', chunk)\n",
        "    if table_matches:\n",
        "        pages = [int(p) for p in table_matches]\n",
        "        if len(pages) == 1:\n",
        "            return str(pages[0])\n",
        "        else:\n",
        "            return f\"{min(pages)}-{max(pages)}\"\n",
        "\n",
        "    return \"Unknown\"\n",
        "\n",
        "def contains_procedural_language(text: str) -> bool:\n",
        "    \"\"\"Check if text contains procedural language\"\"\"\n",
        "    procedural_words = [\n",
        "        'introduced', 'established', 'set up', 'implemented', 'created',\n",
        "        'launched', 'formed', 'built', 'installed', 'deployed',\n",
        "        'additionally introduced', 'procedure for', 'standardization management'\n",
        "    ]\n",
        "    text_lower = text.lower()\n",
        "    return any(word in text_lower for word in procedural_words)\n",
        "\n",
        "def is_data_fragment(kpi_text: str) -> bool:\n",
        "    \"\"\"Check if text is a meaningless data fragment\"\"\"\n",
        "    text = kpi_text.strip()\n",
        "\n",
        "    # Filter pure numbers or simple percentages without context\n",
        "    if re.match(r'^\\d+\\.?\\d*%?$', text):\n",
        "        return True\n",
        "\n",
        "    # Filter very short text (less than 4 meaningful words)\n",
        "    meaningful_words = [word for word in text.split() if len(word) > 2 and not word.isdigit()]\n",
        "    if len(meaningful_words) < 3:\n",
        "        return True\n",
        "\n",
        "    # Filter text with only numbers and common connecting words\n",
        "    words = text.lower().split()\n",
        "    non_functional_words = [word for word in words if word not in ['in', 'of', 'the', 'and', 'or', 'to', 'for', 'with', 'by']]\n",
        "    if len(non_functional_words) < 3:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def standardize_kpi_universal(kpi_item: Dict) -> Dict:\n",
        "    \"\"\"Universal KPI data standardization\"\"\"\n",
        "    standardized = kpi_item.copy()\n",
        "\n",
        "    # Standardize numerical formats\n",
        "    quantitative_value = str(standardized.get('quantitative_value', '')).strip()\n",
        "    kpi_text = standardized.get('kpi_text', '').lower()\n",
        "\n",
        "    # Smart handling of percentage formats\n",
        "    if quantitative_value and quantitative_value.replace('.', '').replace('-', '').replace(',', '').isdigit():\n",
        "        # Check if original text suggests this is a percentage\n",
        "        percentage_indicators = ['percent', 'percentage', '%', 'rate', 'ratio', 'proportion', 'share']\n",
        "        if any(indicator in kpi_text for indicator in percentage_indicators):\n",
        "            if not quantitative_value.endswith('%'):\n",
        "                standardized['quantitative_value'] = quantitative_value + '%'\n",
        "                if not standardized.get('unit'):\n",
        "                    standardized['unit'] = '%'\n",
        "\n",
        "    # Ensure unit field consistency\n",
        "    if '%' in str(standardized.get('quantitative_value', '')):\n",
        "        standardized['unit'] = '%'\n",
        "\n",
        "    # Clean and normalize KPI text\n",
        "    kpi_text_original = standardized.get('kpi_text', '').strip()\n",
        "    # Remove extra spaces and newlines\n",
        "    kpi_text_cleaned = ' '.join(kpi_text_original.split())\n",
        "    standardized['kpi_text'] = kpi_text_cleaned\n",
        "\n",
        "    return standardized\n",
        "\n",
        "def generate_universal_metric_key(kpi_item: Dict) -> str:\n",
        "    \"\"\"Generate universal metric key for deduplication\"\"\"\n",
        "    try:\n",
        "        # Extract core elements\n",
        "        category = kpi_item.get('kpi_category', '').lower().strip()\n",
        "        value = str(kpi_item.get('quantitative_value', '')).replace('%', '').replace(',', '').strip()\n",
        "        time_period = kpi_item.get('time_period', '').lower().strip()\n",
        "        unit = kpi_item.get('unit', '').lower().strip()\n",
        "\n",
        "        # Extract key semantic information from KPI text\n",
        "        kpi_text = kpi_item.get('kpi_text', '').lower()\n",
        "\n",
        "        # Extract primary number (for more precise matching)\n",
        "        numbers_in_text = re.findall(r'\\d+\\.?\\d*', kpi_text)\n",
        "        primary_number = numbers_in_text[0] if numbers_in_text else value\n",
        "\n",
        "        # Generate semantic signature: extract keywords from text\n",
        "        # Remove common stop words\n",
        "        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'}\n",
        "\n",
        "        # Extract keywords (length>2 and not stop words)\n",
        "        words = re.findall(r'\\b\\w+\\b', kpi_text)\n",
        "        key_words = [word for word in words if len(word) > 2 and word not in stop_words and not word.isdigit()]\n",
        "\n",
        "        # Sort keywords to ensure consistency\n",
        "        key_words = sorted(set(key_words))[:5]  # Take at most 5 keywords\n",
        "        semantic_signature = '_'.join(key_words)\n",
        "\n",
        "        # Build universal metric key\n",
        "        key_components = []\n",
        "\n",
        "        if category:\n",
        "            key_components.append(f\"cat:{category}\")\n",
        "        if primary_number:\n",
        "            key_components.append(f\"val:{primary_number}\")\n",
        "        if time_period:\n",
        "            key_components.append(f\"time:{time_period}\")\n",
        "        if unit:\n",
        "            key_components.append(f\"unit:{unit}\")\n",
        "        if semantic_signature:\n",
        "            key_components.append(f\"sem:{semantic_signature}\")\n",
        "\n",
        "        # Generate final key\n",
        "        metric_key = \"|\".join(key_components)\n",
        "\n",
        "        # If all components are empty, use text hash\n",
        "        if not metric_key:\n",
        "            metric_key = f\"hash:{hash(kpi_text)}\"\n",
        "\n",
        "        return metric_key\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error generating universal metric key: {e}\")\n",
        "        # Fallback to text hash\n",
        "        return f\"fallback:{hash(kpi_item.get('kpi_text', ''))}\"\n",
        "\n",
        "def extract_kpi_from_chunk_universal(chunk: str) -> List[Dict]:\n",
        "    \"\"\"Universal KPI extraction function for various sustainability reports\"\"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=MODEL_NAME,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": UNIVERSAL_SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": f\"\"\"Extract ALL KPIs from this text. Requirements:\n",
        "\n",
        "1. Create COMPLETE, MEANINGFUL KPI descriptions with full context\n",
        "2. DO NOT extract standalone numbers without explanatory text\n",
        "3. Include all relevant context (time, location, metric type, etc.)\n",
        "4. Use consistent formatting for similar metrics\n",
        "5. Ensure each KPI is self-explanatory\n",
        "\n",
        "Text to analyze:\n",
        "{chunk}\"\"\"}\n",
        "            ],\n",
        "            temperature=0.0,\n",
        "            max_tokens=4000,\n",
        "            timeout=60\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Clean potential markdown formatting\n",
        "        if content.startswith('```json'):\n",
        "            content = content[7:]\n",
        "        if content.endswith('```'):\n",
        "            content = content[:-3]\n",
        "\n",
        "        if not content.strip().startswith(\"[\"):\n",
        "            logging.warning(f\"API response not JSON list: {content[:100]}...\")\n",
        "            return []\n",
        "\n",
        "        result = json.loads(content)\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            logging.warning(\"API response is not a list format\")\n",
        "            return []\n",
        "\n",
        "        # Extract page information\n",
        "        page_number = extract_page_from_chunk(chunk)\n",
        "\n",
        "        # Universal validation and deduplication logic\n",
        "        validated_result = []\n",
        "        seen_metrics = set()\n",
        "\n",
        "        for item in result:\n",
        "            if isinstance(item, dict) and 'kpi_text' in item and 'kpi_theme' in item:\n",
        "                if item['kpi_text'].strip() and item['kpi_theme'].strip():\n",
        "\n",
        "                    # Check procedural language\n",
        "                    if contains_procedural_language(item['kpi_text']):\n",
        "                        logging.debug(f\"Procedural statement filtered: {item['kpi_text'][:50]}...\")\n",
        "                        continue\n",
        "\n",
        "                    # Filter meaningless data fragments\n",
        "                    if is_data_fragment(item['kpi_text']):\n",
        "                        logging.debug(f\"Data fragment filtered: {item['kpi_text']}\")\n",
        "                        continue\n",
        "\n",
        "                    # Standardize KPI data\n",
        "                    standardized_item = standardize_kpi_universal(item)\n",
        "\n",
        "                    # Add page information\n",
        "                    standardized_item['source_page'] = page_number\n",
        "                    standardized_item['source_type'] = 'text'\n",
        "\n",
        "                    # Universal deduplication mechanism\n",
        "                    metric_key = generate_universal_metric_key(standardized_item)\n",
        "\n",
        "                    if metric_key not in seen_metrics:\n",
        "                        validated_result.append(standardized_item)\n",
        "                        seen_metrics.add(metric_key)\n",
        "                        logging.debug(f\"KPI extracted: {standardized_item['kpi_text'][:80]}...\")\n",
        "                    else:\n",
        "                        logging.debug(f\"Duplicate metric filtered: {standardized_item['kpi_text'][:50]}...\")\n",
        "\n",
        "        logging.info(f\"Chunk processed: {len(validated_result)} unique KPIs extracted\")\n",
        "        return validated_result\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        logging.warning(f\"JSON parsing failed: {e}\\nContent: {content[:300]}...\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logging.error(f\"API call failed: {e}\")\n",
        "        return []\n",
        "\n",
        "def post_process_kpis_universal(kpis: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"Universal KPI post-processing for various report types\"\"\"\n",
        "    if not kpis:\n",
        "        return kpis\n",
        "\n",
        "    # Step 1: Deduplication based on metric keys\n",
        "    unique_kpis_dict = {}\n",
        "\n",
        "    for kpi in kpis:\n",
        "        metric_key = generate_universal_metric_key(kpi)\n",
        "\n",
        "        if metric_key not in unique_kpis_dict:\n",
        "            unique_kpis_dict[metric_key] = kpi\n",
        "        else:\n",
        "            # If duplicate, keep the more complete KPI description\n",
        "            existing_kpi = unique_kpis_dict[metric_key]\n",
        "            current_kpi = kpi\n",
        "\n",
        "            # Compare KPI text completeness\n",
        "            if len(current_kpi.get('kpi_text', '')) > len(existing_kpi.get('kpi_text', '')):\n",
        "                unique_kpis_dict[metric_key] = current_kpi\n",
        "                logging.debug(f\"Replaced with more complete KPI: {current_kpi.get('kpi_text', '')[:50]}...\")\n",
        "            else:\n",
        "                logging.debug(f\"Kept existing KPI: {existing_kpi.get('kpi_text', '')[:50]}...\")\n",
        "\n",
        "    # Step 2: Text similarity-based secondary deduplication\n",
        "    final_kpis = list(unique_kpis_dict.values())\n",
        "\n",
        "    # Use text similarity to check remaining potential duplicates\n",
        "    final_unique_kpis = []\n",
        "\n",
        "    for current_kpi in final_kpis:\n",
        "        is_duplicate = False\n",
        "        current_text = current_kpi.get('kpi_text', '')\n",
        "\n",
        "        for existing_kpi in final_unique_kpis:\n",
        "            existing_text = existing_kpi.get('kpi_text', '')\n",
        "\n",
        "            # Calculate text similarity\n",
        "            similarity = calculate_text_similarity(current_text, existing_text)\n",
        "\n",
        "            # If similarity is very high, consider it duplicate\n",
        "            if similarity > 0.8:\n",
        "                is_duplicate = True\n",
        "                logging.debug(f\"Text similarity duplicate filtered: {current_text[:50]}...\")\n",
        "                break\n",
        "\n",
        "        if not is_duplicate:\n",
        "            final_unique_kpis.append(current_kpi)\n",
        "\n",
        "    logging.info(f\"Universal post-processing: {len(final_unique_kpis)}/{len(kpis)} KPIs retained\")\n",
        "    return final_unique_kpis\n",
        "\n",
        "def calculate_text_similarity(text1: str, text2: str) -> float:\n",
        "    \"\"\"Calculate similarity between two texts\"\"\"\n",
        "    # Normalize texts\n",
        "    norm1 = ' '.join(text1.lower().split())\n",
        "    norm2 = ' '.join(text2.lower().split())\n",
        "\n",
        "    # Word sets\n",
        "    words1 = set(norm1.split())\n",
        "    words2 = set(norm2.split())\n",
        "\n",
        "    if len(words1) == 0 or len(words2) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate intersection and union\n",
        "    intersection = len(words1.intersection(words2))\n",
        "    union = len(words1.union(words2))\n",
        "\n",
        "    return intersection / union if union > 0 else 0.0\n",
        "\n",
        "def validate_kpi_quality(kpis: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"Additional quality validation for extracted KPIs with relaxed filtering\"\"\"\n",
        "    if not ENABLE_QUALITY_VALIDATION:\n",
        "        return kpis\n",
        "\n",
        "    quality_kpis = []\n",
        "\n",
        "    for kpi in kpis:\n",
        "        kpi_text = kpi.get('kpi_text', '').lower()\n",
        "\n",
        "        # Exclude \"planned tone\" KPIs (not actual performance)\n",
        "        is_future_statement = any(word in kpi_text for word in [\n",
        "            'will', 'aim to', 'plan to', 'planning to', 'intend to',\n",
        "            'is expected to', 'is scheduled to', 'expects to', 'expected to',\n",
        "            'targeting', 'propose to', 'going to', 'shall', 'to be installed'\n",
        "        ])\n",
        "        if is_future_statement:\n",
        "            logging.debug(f\"KPI rejected (future plan): {kpi_text[:100]}...\")\n",
        "            continue\n",
        "\n",
        "        # Filter procedural language\n",
        "        if contains_procedural_language(kpi_text):\n",
        "            logging.debug(f\"KPI rejected (procedural language): {kpi_text[:100]}...\")\n",
        "            continue\n",
        "\n",
        "        # Filter for phrases like \"place name + percentage\" (not ESG KPIs, but distribution descriptions)\n",
        "        geo_percent_pattern = re.compile(r\"^[a-z\\s,:%-]+(?:\\s)?\\d{1,3}%$\")\n",
        "        if geo_percent_pattern.match(kpi_text.strip()) and len(kpi_text.strip().split()) <= 6:\n",
        "            logging.debug(f\"KPI rejected (geo+percent short form): {kpi_text}\")\n",
        "            continue\n",
        "\n",
        "        # Verb whitelist: must include action verbs\n",
        "        allowed_kpi_verbs = [\n",
        "            'reduce', 'reduced', 'achieve', 'achieved', 'improve', 'improved',\n",
        "            'diverted', 'trained', 'invested', 'decreased', 'increased',\n",
        "            'consumed', 'emitted', 'saved', 'reached', 'attained', 'completed',\n",
        "            'recorded', 'cut', 'lowered', 'targeted', 'complied', 'avoided',\n",
        "            'used', 'recycled', 'sourced', 'returned', 'measured', 'maintained',\n",
        "            'reported', 'accounted', 'utilized', 'were', 'was'  # Add state verbs\n",
        "        ]\n",
        "        if not any(verb in kpi_text for verb in allowed_kpi_verbs):\n",
        "            logging.debug(f\"KPI rejected (no action verb): {kpi_text[:100]}...\")\n",
        "            continue\n",
        "\n",
        "        # Greylist verbs (action words but not necessarily performance words) - remove problematic words\n",
        "        graylist_verbs = [\n",
        "            'launched',  # Keep some potentially useful words, but remove obvious procedural words\n",
        "            'formed', 'opened', 'started'\n",
        "        ]\n",
        "\n",
        "        contains_graylist = any(verb in kpi_text for verb in graylist_verbs)\n",
        "\n",
        "        # Check for quantitative indicators\n",
        "        has_numbers = any(char.isdigit() for char in kpi_text)\n",
        "        has_percentage = '%' in kpi_text\n",
        "\n",
        "        # Extended units and measurement indicators\n",
        "        has_units = any(unit in kpi_text for unit in [\n",
        "            'tonnes', 'tons', 'kg', 'mwh', 'kwh', 'gwh', 'litres', 'liters', 'gallons',\n",
        "            'employees', 'hours', 'million', 'billion', 'thousand', 'm¬≥', 'co2e', 'tco2e',\n",
        "            'dollars', 'usd', 'eur', 'gbp', 'incidents', 'rate', 'ratio', 'intensity',\n",
        "            'frequency', 'recordable', 'fatalities', 'injuries', 'directors', 'board',\n",
        "            'workforce', 'leadership', 'diversity', 'inclusion', 'satisfaction', 'retention',\n",
        "            'turnover', 'training', 'safety', 'ltifr', 'trir', 'compliance', 'audit',\n",
        "            'assessment', 'screening', 'supplier', 'breach', 'violation', 'disclosure',\n",
        "            'assurance', 'coverage', 'participation', 'completion', 'investment',\n",
        "            'volunteering', 'engagement', 'grievance', 'whistleblower', 'compensation',\n",
        "            'people', 'staff', 'workers', 'positions', 'roles', 'headcount', 'fte',\n",
        "            'performance', 'score', 'index', 'metric', 'level', 'amount', 'value',\n",
        "            'average', 'median', 'total', 'sum', 'count', 'number', 'quantity'\n",
        "        ])\n",
        "\n",
        "        # More flexible time reference detection\n",
        "        has_time_ref = any(time_word in kpi_text for time_word in [\n",
        "            '2019', '2020', '2021', '2022', '2023', '2024', '2025', '2026', '2027', '2028', '2029', '2030',\n",
        "            '2031', '2032', '2033', '2034', '2035', '2040', '2045', '2050',\n",
        "            'annual', 'yearly', 'year', 'quarter', 'month', 'by', 'target', 'baseline', 'fy',\n",
        "            'per year', 'per annum', 'quarterly', 'monthly', 'daily', 'future', 'deadline',\n",
        "            'period', 'reporting', 'current', 'previous', 'next', 'last', 'this'\n",
        "        ])\n",
        "\n",
        "        # Enhanced sustainability context detection\n",
        "        has_sustainability_context = any(sus_word in kpi_text for sus_word in [\n",
        "            # Environmental keywords\n",
        "            'emission', 'carbon', 'energy', 'renewable', 'waste', 'water', 'recycl',\n",
        "            'environmental', 'ghg', 'scope', 'climate', 'biodiversity', 'circular',\n",
        "            'materials', 'intensity', 'consumption', 'efficiency', 'footprint',\n",
        "            'sustainable', 'sustainability', 'green', 'clean', 'eco', 'offset',\n",
        "            'tcfd', 'nature', 'habitat', 'ecosystem', 'pollution', 'discharge',\n",
        "            'electricity', 'gas', 'fuel', 'solar', 'wind', 'hydro', 'nuclear',\n",
        "\n",
        "            # Social keywords\n",
        "            'safety', 'training', 'employee', 'diversity', 'community', 'social',\n",
        "            'workforce', 'gender', 'women', 'female', 'male', 'disability', 'disabled',\n",
        "            'inclusion', 'equity', 'equality', 'lgbtq', 'minorities', 'ethnic',\n",
        "            'health', 'wellbeing', 'wellness', 'satisfaction', 'retention', 'turnover',\n",
        "            'injury', 'incident', 'fatality', 'ltifr', 'trir', 'recordable',\n",
        "            'human rights', 'labor', 'child labor', 'forced labor', 'slavery',\n",
        "            'freedom', 'association', 'collective bargaining', 'grievance',\n",
        "            'volunteering', 'investment', 'hiring', 'local', 'stakeholder',\n",
        "            'customer', 'supplier', 'supply chain', 'accessibility', 'parental',\n",
        "            'mental health', 'ppe', 'emergency', 'drill', 'compliance',\n",
        "            'people', 'staff', 'workers', 'employment', 'job', 'career',\n",
        "            'leadership', 'management', 'senior', 'executive', 'promotion',\n",
        "\n",
        "            # Governance keywords\n",
        "            'governance', 'board', 'director', 'independent', 'chair', 'ceo',\n",
        "            'executive', 'compensation', 'pay', 'ethics', 'compliance', 'corruption',\n",
        "            'bribery', 'code of conduct', 'whistleblower', 'transparency',\n",
        "            'disclosure', 'reporting', 'assurance', 'audit', 'risk', 'management',\n",
        "            'cybersecurity', 'data', 'privacy', 'gdpr', 'breach', 'policy',\n",
        "            'screening', 'assessment', 'due diligence', 'political', 'contribution',\n",
        "            'gri', 'sasb', 'oversight', 'expertise', 'separation', 'incentive',\n",
        "            'fine', 'penalty', 'violation', 'resolution', 'anti-corruption',\n",
        "\n",
        "            # General business performance that could be sustainability-related\n",
        "            'performance', 'quality', 'delivery', 'customer', 'service', 'product',\n",
        "            'operation', 'facility', 'site', 'location', 'region', 'business'\n",
        "        ])\n",
        "\n",
        "        # If it is a greylist verb sentence, but there is no performance content such as numbers, units, time, etc. ‚Üí delete\n",
        "        if contains_graylist and not (has_numbers or has_units or has_percentage or has_time_ref or has_sustainability_context):\n",
        "            logging.debug(f\"KPI rejected (graylist verb, no quantitative data): {kpi_text[:100]}...\")\n",
        "            continue\n",
        "\n",
        "        # More lenient quality scoring - only require numbers and either units/percentage OR time reference OR sustainability context\n",
        "        basic_requirements = has_numbers and (has_percentage or has_units or has_time_ref or has_sustainability_context)\n",
        "\n",
        "        # Additional check for obvious ESG relevance\n",
        "        is_esg_relevant = any(esg_word in kpi_text for esg_word in [\n",
        "            'emission', 'carbon', 'energy', 'waste', 'water', 'renewable', 'employee',\n",
        "            'safety', 'training', 'diversity', 'governance', 'board', 'compliance',\n",
        "            'sustainability', 'environmental', 'social', 'ghg', 'co2', 'workforce',\n",
        "            'gender', 'health', 'injury', 'incident', 'ethics', 'transparency'\n",
        "        ])\n",
        "\n",
        "        if basic_requirements or is_esg_relevant:\n",
        "            quality_kpis.append(kpi)\n",
        "            logging.debug(f\"KPI accepted: {kpi_text[:100]}...\")\n",
        "        else:\n",
        "            logging.debug(f\"KPI filtered out for quality: {kpi_text[:100]}...\")\n",
        "\n",
        "    logging.info(f\"Quality validation: {len(quality_kpis)}/{len(kpis)} KPIs passed\")\n",
        "    return quality_kpis"
      ],
      "metadata": {
        "id": "sB_kDyLZaG1o"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Image processing functions ============\n",
        "def extract_numeric_spans(page):\n",
        "    text_dict = page.get_text(\"dict\")\n",
        "    nums = []\n",
        "    for block in text_dict[\"blocks\"]:\n",
        "        for line in block.get(\"lines\", []):\n",
        "            for span in line.get(\"spans\", []):\n",
        "                s = span[\"text\"].strip()\n",
        "                if re.match(r\"[\\d,.]+%?$\", s):          # Pure number or number + %\n",
        "                    nums.append({\n",
        "                        \"text\": s,\n",
        "                        \"bbox\": span[\"bbox\"],           # (x0,y0,x1,y1)\n",
        "                        \"font\": span[\"size\"]\n",
        "                    })\n",
        "    return nums\n",
        "\n",
        "def extract_images_from_pdf_fixed(pdf_path: str) -> List[Dict]:\n",
        "    \"\"\"Extract images from PDF using PyMuPDF\"\"\"\n",
        "    images = []\n",
        "\n",
        "    try:\n",
        "        pdf_document = fitz.open(pdf_path)\n",
        "\n",
        "        for page_num in range(len(pdf_document)):\n",
        "            page = pdf_document[page_num]\n",
        "            image_list = page.get_images()\n",
        "\n",
        "            # üî• New: Extract page screenshots as an alternative\n",
        "            page_pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # high resolution\n",
        "            page_img = Image.frombytes(\"RGB\", [page_pix.width, page_pix.height], page_pix.samples)\n",
        "\n",
        "            # Add full page screenshot\n",
        "            images.append({\n",
        "                'image': page_img,\n",
        "                'page_number': page_num + 1,\n",
        "                'width': page_img.width,\n",
        "                'height': page_img.height,\n",
        "                'image_index': 'full_page',\n",
        "                'type': 'full_page'\n",
        "            })\n",
        "\n",
        "\n",
        "            for img_index, img in enumerate(image_list):\n",
        "                try:\n",
        "                    xref = img[0]\n",
        "                    base_image = pdf_document.extract_image(xref)\n",
        "                    image_bytes = base_image[\"image\"]\n",
        "\n",
        "                    image = Image.open(BytesIO(image_bytes))\n",
        "\n",
        "                    # Convert to RGB if needed\n",
        "                    if image.mode in ['RGBA', 'LA']:\n",
        "                        background = Image.new('RGB', image.size, (255, 255, 255))\n",
        "                        if image.mode == 'RGBA':\n",
        "                            background.paste(image, mask=image.split()[-1])\n",
        "                        else:\n",
        "                            background.paste(image)\n",
        "                        image = background\n",
        "                    elif image.mode != 'RGB':\n",
        "                        image = image.convert('RGB')\n",
        "\n",
        "                    # Filter small images\n",
        "                    if image.width >= 50 and image.height >= 50:\n",
        "                        images.append({\n",
        "                            'image': image,\n",
        "                            'page_number': page_num + 1,\n",
        "                            'width': image.width,\n",
        "                            'height': image.height,\n",
        "                            'image_index': img_index,\n",
        "                            'type': 'extracted'  # üî• Added type identifier\n",
        "                        })\n",
        "\n",
        "                except Exception as e:\n",
        "                    logging.warning(f\"Error extracting image {img_index} from page {page_num + 1}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        pdf_document.close()\n",
        "        logging.info(f\"Extracted {len(images)} images from PDF\")\n",
        "        return images\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error extracting images from PDF: {e}\")\n",
        "        return []\n",
        "\n",
        "def image_to_base64_fixed(image: Image.Image) -> str:\n",
        "    \"\"\"Convert image to base64 with error handling\"\"\"\n",
        "    try:\n",
        "        if image.mode not in ['RGB', 'L']:\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        # Resize large images\n",
        "        max_size = (1536, 1536)\n",
        "        if image.width > max_size[0] or image.height > max_size[1]:\n",
        "            # Calculate scaling to maintain aspect ratio\n",
        "            ratio = min(max_size[0]/image.width, max_size[1]/image.height)\n",
        "            new_size = (int(image.width * ratio), int(image.height * ratio))\n",
        "            image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "        buffered = BytesIO()\n",
        "        image.save(buffered, format=\"JPEG\", quality=95)\n",
        "        img_str = base64.b64encode(buffered.getvalue()).decode()\n",
        "\n",
        "        return img_str\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error converting image to base64: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "Vfegv4osaG3b"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# Multi-crop / multi-resolution generator (supports crop parameter 0)\n",
        "# ------------------------------------------------------------\n",
        "from itertools import product\n",
        "\n",
        "def generate_image_variants(img: Image.Image,\n",
        "                            max_side_full: int = 1200,\n",
        "                            crop_size: int = 768,\n",
        "                            stride: int = 512) -> List[Tuple[Image.Image, str]]:\n",
        "    \"\"\"\n",
        "    Returns [(variant_image, variant_tag), ...]\n",
        "    variant_tag value: original / resized / crop_{row}_{col}\n",
        "    \"\"\"\n",
        "    variants = []\n",
        "\n",
        "    # 0) Original image\n",
        "    variants.append((img, \"original\"))\n",
        "\n",
        "    # 1) Zoom (if the original image is too large)\n",
        "    w, h = img.size\n",
        "    if max(w, h) > max_side_full:\n",
        "        scale = max_side_full / float(max(w, h))\n",
        "        resized = img.resize((int(w * scale), int(h * scale)), Image.Resampling.LANCZOS)\n",
        "        variants.append((resized, \"resized\"))\n",
        "    else:\n",
        "        resized = img  # Keep the original image without scaling\n",
        "        variants.append((resized, \"resized\"))  # Unified plus resized version\n",
        "\n",
        "    # 2) Sliding window cropping (skipped when cropping size or step size is 0)\n",
        "    if crop_size > 0 and stride > 0:\n",
        "        base_img = variants[-1][0]\n",
        "        bw, bh = base_img.size\n",
        "        if bw > crop_size or bh > crop_size:\n",
        "            xs = list(range(0, max(bw - crop_size, 1), stride)) + [bw - crop_size]\n",
        "            ys = list(range(0, max(bh - crop_size, 1), stride)) + [bh - crop_size]\n",
        "            for r, c in product(range(len(ys)), range(len(xs))):\n",
        "                x, y = xs[c], ys[r]\n",
        "                crop = base_img.crop((x, y, x + crop_size, y + crop_size))\n",
        "                # Filter solid color areas\n",
        "                if np.array(crop.convert('L')).std() < 5:\n",
        "                    continue\n",
        "                variants.append((crop, f\"crop_{r}_{c}\"))\n",
        "\n",
        "    return variants"
      ],
      "metadata": {
        "id": "ma9D4crJaG5z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------\n",
        "# üìä A chart recognition function that replaces plotclassifier (Hugging Face model)\n",
        "# ---------------------------------------------\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
        "import torch\n",
        "# üîß Fix: Chart recognition with CLIP model\n",
        "def setup_chart_classifier():\n",
        "    \"\"\"Setting up the chart classifier\"\"\"\n",
        "    try:\n",
        "        from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "        # Loading CLIP Model\n",
        "        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "        def is_chart_image_clip(image: Image.Image) -> bool:\n",
        "            \"\"\"Use CLIP to determine whether it is a chart\"\"\"\n",
        "            try:\n",
        "                # Defines text description related to the chart\n",
        "                chart_labels = [\n",
        "                    \"a chart\", \"a graph\", \"a bar chart\", \"a pie chart\",\n",
        "                    \"a line graph\", \"a table\", \"data visualization\",\n",
        "                    \"statistics\", \"a diagram\", \"an infographic\"\n",
        "                ]\n",
        "\n",
        "                # Processing Input\n",
        "                inputs = processor(\n",
        "                    text=chart_labels,\n",
        "                    images=image,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True\n",
        "                )\n",
        "\n",
        "                # Get prediction results\n",
        "                outputs = model(**inputs)\n",
        "                logits_per_image = outputs.logits_per_image\n",
        "                probs = logits_per_image.softmax(dim=1)\n",
        "\n",
        "                # If the probability of any chart label is greater than 0.25, it is considered to be a chart\n",
        "                max_prob = probs.max().item()\n",
        "                is_chart = max_prob > 0.25\n",
        "\n",
        "                logging.debug(f\"CLIP chart recognition: maximum probability = {max_prob:.3f}, result = {is_chart}\")\n",
        "                return is_chart\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"CLIP chart recognition failed: {e}\")\n",
        "                # Downgrade to statistical methods\n",
        "                gray = image.convert('L')\n",
        "                return np.array(gray).std() > 15\n",
        "\n",
        "        logging.info(\"‚úÖ Graph recognition using CLIP model\")\n",
        "        return is_chart_image_clip\n",
        "\n",
        "    except ImportError:\n",
        "        logging.warning(\"CLIP model is not available, use statistical methods\")\n",
        "        def is_chart_image_stats(image: Image.Image) -> bool:\n",
        "            \"\"\"Statistical method to determine whether it is a chart\"\"\"\n",
        "            try:\n",
        "                gray = image.convert('L')\n",
        "                std_dev = np.array(gray).std()\n",
        "                return std_dev > 15\n",
        "            except:\n",
        "                return True\n",
        "\n",
        "        return is_chart_image_stats\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to set chart classifier: {e}\")\n",
        "        def is_chart_image_fallback(image: Image.Image) -> bool:\n",
        "            return True  # Conservative Strategy: When in Doubt, Analyze\n",
        "        return is_chart_image_fallback\n",
        "\n",
        "# Initialize the graph classifier\n",
        "is_chart_image = setup_chart_classifier()\n",
        "\n",
        "\n",
        "def extract_kpi_from_image_fixed(image: Image.Image, page_number: int, image_type: str = 'extracted') -> List[Dict]:\n",
        "    \"\"\"Extract KPIs from image with improved error handling\"\"\"\n",
        "    try:\n",
        "        # üî• New: Pre-filter: Check if it might be a chart\n",
        "        if not is_chart_image(image):\n",
        "            logging.debug(f\"Image on page {page_number} filtered out (not likely a chart)\")\n",
        "            return []\n",
        "\n",
        "        base64_image = image_to_base64_fixed(image)\n",
        "        if not base64_image:\n",
        "            return []\n",
        "\n",
        "        # üî• Change: Use enhanced prompt\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": ENHANCED_IMAGE_KPI_SYSTEM_PROMPT  # üî• Using the new prompt\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\n",
        "                            \"type\": \"text\",\n",
        "                            # üî• New: Detailed user instructions\n",
        "                            \"text\": \"\"\"Analyze this image carefully for quantifiable performance data.\n",
        "\n",
        "IMPORTANT ANALYSIS PRINCIPLES:\n",
        "\n",
        "1. **Chart Type Recognition**:\n",
        "   - Stacked charts: Multiple colors/patterns layered in same position\n",
        "   - Grouped charts: Multiple elements side by side at same position\n",
        "   - Simple charts: One data point per position\n",
        "\n",
        "2. **Value Extraction Rules**:\n",
        "   - For STACKED charts: Read each layer separately, NOT the total height\n",
        "   - For GROUPED charts: Read each element individually\n",
        "   - For SIMPLE charts: Read data point values directly\n",
        "\n",
        "3. **Data Relevance Filter**:\n",
        "   ‚úÖ EXTRACT: Performance outcomes, efficiency metrics, reduction rates, satisfaction scores, compliance rates\n",
        "   ‚ùå SKIP: Certification counts, project timelines, implementation schedules, organizational charts, process flows\n",
        "\n",
        "4. **Quality Standards**:\n",
        "   - Only extract clear, quantifiable performance indicators\n",
        "   - Each data point must have complete context\n",
        "   - If uncertain about values, don't estimate\n",
        "   - If chart shows mainly operational/administrative data, return empty array\n",
        "\n",
        "Please analyze this chart step by step:\n",
        "- First identify the chart type\n",
        "- Then determine if it contains performance KPIs\n",
        "- Finally extract all relevant performance data points\n",
        "\n",
        "Focus on measurable outcomes and achievements, not counts or processes.\"\"\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"type\": \"image_url\",\n",
        "                            \"image_url\": {\n",
        "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
        "                                \"detail\": \"high\"\n",
        "                            }\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.1,\n",
        "            max_tokens=4000,\n",
        "            timeout=60\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message.content.strip()\n",
        "\n",
        "        if not content:\n",
        "            return []\n",
        "\n",
        "        # Clean formatting\n",
        "        if content.startswith('```json'):\n",
        "            content = content[7:]\n",
        "        if content.endswith('```'):\n",
        "            content = content[:-3]\n",
        "\n",
        "        content = content.strip()\n",
        "\n",
        "        if not content.startswith(\"[\"):\n",
        "            logging.warning(f\"Image analysis response not JSON list: {content[:100]}...\")\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            result = json.loads(content)\n",
        "        except json.JSONDecodeError as e:\n",
        "            logging.warning(f\"JSON parsing failed for image analysis: {e}\")\n",
        "            return []\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            return []\n",
        "\n",
        "        # Process results\n",
        "        processed_result = []\n",
        "        for item in result:\n",
        "            if isinstance(item, dict) and 'kpi_text' in item:\n",
        "                if not item.get('kpi_text', '').strip():\n",
        "                    continue\n",
        "\n",
        "                item['source_page'] = page_number\n",
        "                item['source_type'] = 'image'\n",
        "                item['image_type'] = image_type  # üî• Êñ∞Â¢ûÂ≠óÊÆµ\n",
        "\n",
        "                # üî• Êõ¥ÊîπÔºöÁ°Æ‰øùÊúâchartÊ†áËØÜ\n",
        "                kpi_text = item['kpi_text']\n",
        "                if not any(marker in kpi_text.lower() for marker in ['chart', 'graph', 'table', 'figure']):\n",
        "                    chart_type = item.get('chart_type', 'chart')\n",
        "                    item['kpi_text'] = f\"[{chart_type.title()}] {kpi_text}\"\n",
        "\n",
        "                processed_result.append(item)\n",
        "\n",
        "        if processed_result:\n",
        "            logging.info(f\"‚úÖ Extracted {len(processed_result)} KPIs from {image_type} on page {page_number}\")\n",
        "        else:\n",
        "            logging.debug(f\"‚ùå No KPIs found in {image_type} on page {page_number}\")\n",
        "\n",
        "        return processed_result\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error extracting KPIs from image: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def process_pdf_images_for_kpis_fixed(pdf_path: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Traverse each page of the PDF:\n",
        "        ‚Ä¢ Perform multiple cropping + Vision on all ‚Äòextracted‚Äô images on the page\n",
        "        ‚Ä¢ If the page has not captured the KPI, perform Vision on the entire page screenshot\n",
        "    \"\"\"\n",
        "    logging.info(\"Starting page-by-page image KPI extraction ‚Ä¶\")\n",
        "\n",
        "    images = extract_images_from_pdf_fixed(pdf_path)\n",
        "    if not images:\n",
        "        return []\n",
        "\n",
        "    # Aggregate images by page\n",
        "    page_dict = {}\n",
        "    for info in images:\n",
        "        pg = info[\"page_number\"]\n",
        "        page_dict.setdefault(pg, {\"extracted\": [], \"full\": None})\n",
        "        if info[\"type\"] == \"extracted\":\n",
        "            page_dict[pg][\"extracted\"].append(info[\"image\"])\n",
        "        else:                    # full_page\n",
        "            page_dict[pg][\"full\"] = info[\"image\"]\n",
        "\n",
        "    all_image_kpis: List[Dict] = []\n",
        "\n",
        "    # ‚Äî‚Äî Page by page processing ‚Äî‚Äî\n",
        "    for pg in sorted(page_dict.keys()):\n",
        "        logging.info(f\"\\n=== Page {pg} ===\")\n",
        "        page_kpis: List[Dict] = []\n",
        "\n",
        "        # ‚ë† Individually extracted images\n",
        "        for idx, img in enumerate(page_dict[pg][\"extracted\"]):\n",
        "            for var_img, var_tag in generate_image_variants(img, 1200, 768, 512):\n",
        "                kpis = extract_kpi_from_image_fixed(\n",
        "                    var_img, pg, f\"extracted_{var_tag}\"\n",
        "                )\n",
        "                for k in kpis:\n",
        "                    key = generate_universal_metric_key(k)\n",
        "                    if key not in {generate_universal_metric_key(x) for x in page_kpis}:\n",
        "                        page_kpis.append(k)\n",
        "                time.sleep(0.8)\n",
        "\n",
        "        # ‚ë° If it is still empty, analyze the entire page again\n",
        "        if not page_kpis and page_dict[pg][\"full\"] is not None:\n",
        "            for var_img, var_tag in generate_image_variants(\n",
        "                    page_dict[pg][\"full\"], 1200, 0, 0):   # Âè™ÂÅö original/resized\n",
        "                kpis = extract_kpi_from_image_fixed(\n",
        "                    var_img, pg, f\"full_{var_tag}\"\n",
        "                )\n",
        "                for k in kpis:\n",
        "                    key = generate_universal_metric_key(k)\n",
        "                    if key not in {generate_universal_metric_key(x) for x in page_kpis}:\n",
        "                        page_kpis.append(k)\n",
        "                time.sleep(1.0)\n",
        "\n",
        "        logging.info(f\"  ‚Üí Page {pg} KPI count: {len(page_kpis)}\")\n",
        "        all_image_kpis.extend(page_kpis)\n",
        "\n",
        "    logging.info(f\"Image KPI extraction finished: {len(all_image_kpis)} KPIs from {len(page_dict)} pages\")\n",
        "    return all_image_kpis"
      ],
      "metadata": {
        "id": "F5ZLpwL8Zok_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8db2fda0-b5b5-4bfc-94c6-530aa98387d3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Conservative image filter\n",
        "def conservative_image_filter(image: Image.Image) -> Tuple[bool, str]:\n",
        "    \"\"\"Conservative image filtering - only filters obviously useless images\"\"\"\n",
        "    try:\n",
        "        # Only filter very small images (maybe logos, icons)\n",
        "        if image.width < 40 or image.height < 40:\n",
        "            return False, \"too_small_icon\"\n",
        "\n",
        "        # Filter only images of almost pure colors (decorative elements)\n",
        "        gray = np.array(image.convert('L'))\n",
        "        std_dev = gray.std()\n",
        "\n",
        "        # Very conservative threshold - only images with completely pure colors are filtered\n",
        "        if std_dev < 3:\n",
        "            return False, \"pure_color\"\n",
        "\n",
        "        # Check if it is a pure white background (blank area)\n",
        "        mean_val = gray.mean()\n",
        "        if mean_val > 250 and std_dev < 8:\n",
        "            return False, \"blank_white\"\n",
        "\n",
        "        # Default: Process all other images to ensure integrity\n",
        "        return True, \"keep_for_analysis\"\n",
        "    except Exception:\n",
        "        return True, \"filter_error_keep\"\n",
        "\n",
        "# 2. Cache mechanism\n",
        "class FastKPICache:\n",
        "    def __init__(self, cache_dir: str = \"fast_kpi_cache\"):\n",
        "        self.cache_dir = cache_dir\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "        self.hit_count = 0\n",
        "        self.miss_count = 0\n",
        "\n",
        "    def get_image_hash(self, image: Image.Image) -> str:\n",
        "        \"\"\"Fast image fingerprint generation\"\"\"\n",
        "        width, height = image.size\n",
        "        if width > 100 and height > 100:\n",
        "            center_crop = image.crop((\n",
        "                width//4, height//4,\n",
        "                3*width//4, 3*height//4\n",
        "            )).resize((32, 32))\n",
        "            img_bytes = BytesIO()\n",
        "            center_crop.save(img_bytes, format='JPEG', quality=50)\n",
        "            sample_hash = hashlib.md5(img_bytes.getvalue()).hexdigest()[:16]\n",
        "        else:\n",
        "            sample_hash = hashlib.md5(str(width * height).encode()).hexdigest()[:16]\n",
        "\n",
        "        return f\"{width}x{height}_{sample_hash}\"\n",
        "\n",
        "    def get_cached_kpis(self, image_hash: str) -> Optional[List[Dict]]:\n",
        "        cache_file = os.path.join(self.cache_dir, f\"{image_hash}.pkl\")\n",
        "        if os.path.exists(cache_file):\n",
        "            try:\n",
        "                with open(cache_file, 'rb') as f:\n",
        "                    self.hit_count += 1\n",
        "                    return pickle.load(f)\n",
        "            except:\n",
        "                pass\n",
        "        self.miss_count += 1\n",
        "        return None\n",
        "\n",
        "    def cache_kpis(self, image_hash: str, kpis: List[Dict]):\n",
        "        cache_file = os.path.join(self.cache_dir, f\"{image_hash}.pkl\")\n",
        "        try:\n",
        "            with open(cache_file, 'wb') as f:\n",
        "                pickle.dump(kpis, f)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def get_stats(self):\n",
        "        total = self.hit_count + self.miss_count\n",
        "        hit_rate = self.hit_count / total if total > 0 else 0\n",
        "        return f\"Cache: {self.hit_count} hits, {self.miss_count} misses (hit rate: {hit_rate:.1%})\"\n",
        "\n",
        "# Initialize the cache\n",
        "fast_cache = FastKPICache()\n",
        "\n",
        "# 3. Optimized API calls\n",
        "COMPREHENSIVE_EXTRACTION_PROMPT = \"\"\"\n",
        "You are an expert data analyst. Extract ALL quantifiable performance indicators from this image.\n",
        "\n",
        "CRITICAL REQUIREMENTS:\n",
        "1. Extract EVERY visible number, percentage, and metric\n",
        "2. Include ALL data points from charts, graphs, and tables\n",
        "3. Do not skip any quantifiable information\n",
        "\n",
        "Return complete JSON array:\n",
        "[\n",
        "  {\n",
        "    \"kpi_text\": \"Complete contextual description with the specific number\",\n",
        "    \"quantitative_value\": \"exact number only\",\n",
        "    \"unit\": \"unit of measurement\",\n",
        "    \"kpi_theme\": \"Environmental/Social/Governance\",\n",
        "    \"kpi_category\": \"specific category\",\n",
        "    \"time_period\": \"year/period if visible\"\n",
        "  }\n",
        "]\n",
        "\n",
        "COMPLETENESS IS CRITICAL - Extract everything quantifiable.\n",
        "\"\"\"\n",
        "\n",
        "def extract_kpi_optimized(image: Image.Image, page_number: int) -> List[Dict]:\n",
        "    \"\"\"Optimized KPI extraction\"\"\"\n",
        "    try:\n",
        "        # Check the cache\n",
        "        image_hash = fast_cache.get_image_hash(image)\n",
        "        cached_kpis = fast_cache.get_cached_kpis(image_hash)\n",
        "        if cached_kpis is not None:\n",
        "            for kpi in cached_kpis:\n",
        "                kpi['source_page'] = page_number\n",
        "            return cached_kpis\n",
        "\n",
        "        # Optimizing image encoding\n",
        "        base64_image = image_to_base64_optimized(image)\n",
        "        if not base64_image:\n",
        "            return []\n",
        "\n",
        "        # API Calls\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": COMPREHENSIVE_EXTRACTION_PROMPT},\n",
        "                    {\"type\": \"image_url\", \"image_url\": {\n",
        "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
        "                        \"detail\": \"high\"\n",
        "                    }}\n",
        "                ]\n",
        "            }],\n",
        "            temperature=0.0,\n",
        "            max_tokens=2000,\n",
        "            timeout=60\n",
        "        )\n",
        "\n",
        "        # Parsing results\n",
        "        kpis = parse_optimized_response(response, page_number)\n",
        "\n",
        "        # Caching results\n",
        "        fast_cache.cache_kpis(image_hash, kpis)\n",
        "\n",
        "        return kpis\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Optimized KPI extraction failed for page {page_number}: {e}\")\n",
        "        return []\n",
        "\n",
        "def image_to_base64_optimized(image: Image.Image) -> str:\n",
        "    \"\"\"Optimized image encoding\"\"\"\n",
        "    try:\n",
        "        max_dimension = 1400  # Maintain high quality\n",
        "        width, height = image.size\n",
        "\n",
        "        if max(width, height) > max_dimension:\n",
        "            scale = max_dimension / max(width, height)\n",
        "            new_size = (int(width * scale), int(height * scale))\n",
        "            image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "        if image.mode != 'RGB':\n",
        "            if image.mode in ['RGBA', 'LA']:\n",
        "                background = Image.new('RGB', image.size, (255, 255, 255))\n",
        "                if image.mode == 'RGBA':\n",
        "                    background.paste(image, mask=image.split()[-1])\n",
        "                else:\n",
        "                    background.paste(image)\n",
        "                image = background\n",
        "            else:\n",
        "                image = image.convert('RGB')\n",
        "\n",
        "        buffered = BytesIO()\n",
        "        image.save(buffered, format=\"JPEG\", quality=92, optimize=True)\n",
        "        return base64.b64encode(buffered.getvalue()).decode()\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Optimized image encoding failed: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def parse_optimized_response(response, page_number: int) -> List[Dict]:\n",
        "    \"\"\"Optimized response parsing\"\"\"\n",
        "    try:\n",
        "        content = response.choices[0].message.content.strip()\n",
        "\n",
        "        if content.startswith('```json'):\n",
        "            content = content[7:]\n",
        "        if content.endswith('```'):\n",
        "            content = content[:-3]\n",
        "        content = content.strip()\n",
        "\n",
        "        if not content.startswith('['):\n",
        "            return []\n",
        "\n",
        "        result = json.loads(content)\n",
        "        if not isinstance(result, list):\n",
        "            return []\n",
        "\n",
        "        validated_kpis = []\n",
        "        for item in result:\n",
        "            if (isinstance(item, dict) and\n",
        "                item.get('kpi_text', '').strip() and\n",
        "                item.get('quantitative_value', '').strip()):\n",
        "\n",
        "                kpi = {\n",
        "                    'kpi_text': item.get('kpi_text', '').strip(),\n",
        "                    'quantitative_value': str(item.get('quantitative_value', '')).strip(),\n",
        "                    'unit': item.get('unit', '').strip(),\n",
        "                    'kpi_theme': item.get('kpi_theme', 'Environmental').strip(),\n",
        "                    'kpi_category': item.get('kpi_category', '').strip(),\n",
        "                    'time_period': item.get('time_period', '').strip(),\n",
        "                    'source_page': page_number,\n",
        "                    'source_type': 'image'\n",
        "                }\n",
        "                validated_kpis.append(kpi)\n",
        "\n",
        "        return validated_kpis\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Optimized response parsing failed: {e}\")\n",
        "        return []\n",
        "\n",
        "# 4. Parallel image processing\n",
        "def process_images_in_parallel(image_data: List[Dict], max_workers: int = 3) -> List[Dict]:\n",
        "    \"\"\"Parallel image processing\"\"\"\n",
        "    if not image_data:\n",
        "        return []\n",
        "\n",
        "    print(f\"üîÑ Processing {len(image_data)} images in parallel...\")\n",
        "\n",
        "    all_kpis = []\n",
        "    completed_count = 0\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        future_to_info = {}\n",
        "        for img_info in image_data:\n",
        "            future = executor.submit(\n",
        "                extract_kpi_optimized,\n",
        "                img_info['image'],\n",
        "                img_info['page_number']\n",
        "            )\n",
        "            future_to_info[future] = img_info\n",
        "\n",
        "        for future in concurrent.futures.as_completed(future_to_info):\n",
        "            img_info = future_to_info[future]\n",
        "            try:\n",
        "                kpis = future.result(timeout=90)\n",
        "                all_kpis.extend(kpis)\n",
        "                completed_count += 1\n",
        "\n",
        "                if completed_count % 5 == 0:\n",
        "                    progress = completed_count / len(image_data) * 100\n",
        "                    print(f\"   üìà Progress: {completed_count}/{len(image_data)} ({progress:.1f}%)\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Image processing failed for page {img_info['page_number']}: {e}\")\n",
        "                continue\n",
        "\n",
        "    print(f\"üìä Parallel processing completed: {len(all_kpis)} KPIs extracted\")\n",
        "    print(f\"üìã {fast_cache.get_stats()}\")\n",
        "\n",
        "    return all_kpis"
      ],
      "metadata": {
        "id": "7j4_GQ6Llw8e"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "N7yh-bSwZonF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0950998b-3490-4281-bb82-c947dfa7448a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Main processing function ============\n",
        "def process_sustainability_report_with_enhanced_images(pdf_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Main processing function with image analysis\"\"\"\n",
        "    logging.info(\"Starting enhanced PDF processing with image analysis...\")\n",
        "\n",
        "    # Step 1: Text and table extraction\n",
        "    logging.info(\"Step 1/5: Reading PDF text and tables...\")\n",
        "    full_text = pdf_to_text_and_tables(pdf_path)\n",
        "\n",
        "    camelot_tables = camelot_extra_tables_enhanced(pdf_path)\n",
        "    if camelot_tables:\n",
        "        full_text += \"\\n\\n\" + \"\\n\\n\".join(camelot_tables)\n",
        "\n",
        "    logging.info(\"Step 2/5: Chunking text...\")\n",
        "    chunks = split_into_chunks(full_text, MAX_TOKENS_CHUNK)\n",
        "\n",
        "    logging.info(\"Step 3/5: Extracting KPIs from text...\")\n",
        "    text_kpis = []\n",
        "    for idx, chunk in enumerate(chunks, 1):\n",
        "        logging.info(f\"Processing text chunk {idx}/{len(chunks)}\")\n",
        "        if chunk.strip():\n",
        "            chunk_kpis = extract_kpi_from_chunk_universal(chunk)\n",
        "            text_kpis.extend(chunk_kpis)\n",
        "            if idx < len(chunks):\n",
        "                time.sleep(SLEEP_SEC)\n",
        "\n",
        "    # Step 4: Image KPI extraction\n",
        "    logging.info(\"Step 4/5: Extracting KPIs from images...\")\n",
        "    image_kpis = process_pdf_images_for_kpis_fixed(pdf_path)\n",
        "\n",
        "    # Step 5: Combine and process\n",
        "    logging.info(\"Step 5/5: Combining and processing all KPIs...\")\n",
        "\n",
        "    for kpi in text_kpis:\n",
        "        if 'source_type' not in kpi:\n",
        "            kpi['source_type'] = 'text'\n",
        "\n",
        "    all_kpis = text_kpis + image_kpis\n",
        "    all_kpis = post_process_kpis_universal(all_kpis)\n",
        "\n",
        "    df_auto = pd.DataFrame(all_kpis)\n",
        "\n",
        "    if not df_auto.empty:\n",
        "        if 'source_type' not in df_auto.columns:\n",
        "            df_auto['source_type'] = 'text'\n",
        "\n",
        "        initial_count = len(df_auto)\n",
        "        df_auto = df_auto.drop_duplicates(subset=['kpi_text'], keep='first')\n",
        "        final_count = len(df_auto)\n",
        "\n",
        "        logging.info(f\"Removed {initial_count - final_count} duplicate KPIs\")\n",
        "\n",
        "        try:\n",
        "            df_auto = df_auto.sort_values(['source_type', 'kpi_theme', 'kpi_category'], na_position='last')\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "        text_kpi_count = len([kpi for kpi in all_kpis if kpi.get('source_type', 'text') != 'image'])\n",
        "        image_kpi_count = len([kpi for kpi in all_kpis if kpi.get('source_type') == 'image'])\n",
        "\n",
        "        logging.info(f\"KPI Summary: {text_kpi_count} from text/tables, {image_kpi_count} from images\")\n",
        "\n",
        "    return df_auto\n"
      ],
      "metadata": {
        "id": "Mr1OIB2BZosj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Optimize moderator processing function ============\n",
        "def process_sustainability_report_OPTIMIZED(pdf_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Optimize moderator processing functions - improve performance while ensuring integrity\"\"\"\n",
        "\n",
        "    start_time = time.time()\n",
        "    print(\"‚ö° Starting OPTIMIZED processing with completeness guarantee...\")\n",
        "\n",
        "    try:\n",
        "        # Parallel text and image preprocessing\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
        "            print(\"üîÑ Starting parallel text and image preprocessing...\")\n",
        "\n",
        "            # Text processing (using your existing logic)\n",
        "            def extract_text_kpis():\n",
        "                full_text = pdf_to_text_and_tables(pdf_path)\n",
        "                camelot_tables = camelot_extra_tables_enhanced(pdf_path)\n",
        "                if camelot_tables:\n",
        "                    full_text += \"\\n\\n\" + \"\\n\\n\".join(camelot_tables)\n",
        "\n",
        "                chunks = split_into_chunks(full_text, MAX_TOKENS_CHUNK)\n",
        "                text_kpis = []\n",
        "                for idx, chunk in enumerate(chunks, 1):\n",
        "                    if chunk.strip():\n",
        "                        chunk_kpis = extract_kpi_from_chunk_universal(chunk)\n",
        "                        text_kpis.extend(chunk_kpis)\n",
        "                        if idx < len(chunks):\n",
        "                            time.sleep(SLEEP_SEC)\n",
        "                return text_kpis\n",
        "\n",
        "            #Image preprocessing (using optimized filtering)\n",
        "            def extract_and_filter_images():\n",
        "                all_images = extract_images_from_pdf_fixed(pdf_path)\n",
        "                filtered_images = []\n",
        "\n",
        "                for img_info in all_images:\n",
        "                    should_process, reason = conservative_image_filter(img_info['image'])\n",
        "                    if should_process:\n",
        "                        filtered_images.append(img_info)\n",
        "\n",
        "                print(f\"üìä Conservative filtering: Kept {len(filtered_images)}/{len(all_images)} images\")\n",
        "                return filtered_images\n",
        "\n",
        "            text_future = executor.submit(extract_text_kpis)\n",
        "            image_future = executor.submit(extract_and_filter_images)\n",
        "\n",
        "            text_kpis = text_future.result()\n",
        "            image_data = image_future.result()\n",
        "\n",
        "        preprocessing_time = time.time() - start_time\n",
        "        print(f\"‚è±Ô∏è  Preprocessing completed in {preprocessing_time:.1f}s\")\n",
        "\n",
        "        # Parallel Image KPI Extraction\n",
        "        image_start = time.time()\n",
        "        image_kpis = process_images_in_parallel(image_data, max_workers=3)\n",
        "        image_time = time.time() - image_start\n",
        "        print(f\"‚è±Ô∏è  Image processing completed in {image_time:.1f}s\")\n",
        "\n",
        "        # Post-processing\n",
        "        all_kpis = text_kpis + image_kpis\n",
        "        all_kpis = post_process_kpis_universal(all_kpis)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        df_auto = pd.DataFrame(all_kpis)\n",
        "\n",
        "        if not df_auto.empty:\n",
        "            initial_count = len(df_auto)\n",
        "            df_auto = df_auto.drop_duplicates(subset=['kpi_text'], keep='first')\n",
        "            final_count = len(df_auto)\n",
        "\n",
        "            if 'source_type' not in df_auto.columns:\n",
        "                df_auto['source_type'] = 'text'\n",
        "\n",
        "            print(f\"üîÑ Removed {initial_count - final_count} exact duplicates\")\n",
        "\n",
        "        # Performance Statistics\n",
        "        total_time = time.time() - start_time\n",
        "        text_count = len([k for k in all_kpis if k.get('source_type') != 'image'])\n",
        "        image_count = len([k for k in all_kpis if k.get('source_type') == 'image'])\n",
        "\n",
        "        print(f\"\\n‚ö° OPTIMIZED processing completed!\")\n",
        "        print(f\"‚è±Ô∏è  Total time: {total_time:.1f}s ({total_time/60:.1f}min)\")\n",
        "        print(f\"üìä Results:\")\n",
        "        print(f\"   - Text/Tables: {text_count} KPIs\")\n",
        "        print(f\"   - Images/Charts: {image_count} KPIs\")\n",
        "        print(f\"   - Total unique: {len(df_auto)} KPIs\")\n",
        "        print(f\"‚ö° Performance: {len(df_auto)/total_time:.1f} KPIs/second\")\n",
        "\n",
        "        return df_auto\n",
        "\n",
        "    except Exception as e:\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"‚ùå Optimized processing failed after {total_time:.1f}s: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return pd.DataFrame()"
      ],
      "metadata": {
        "id": "YWPKhJAPmHg7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Result saving and comparison functions ============\n",
        "def infer_stakeholder(row) -> str:\n",
        "    \"\"\"Infer affected stakeholders based on KPI theme and category\"\"\"\n",
        "    theme = row.get('kpi_theme', '').lower()\n",
        "    category = row.get('kpi_category', '').lower()\n",
        "    kpi_text = row.get('kpi_text', '').lower()\n",
        "\n",
        "    if theme == 'environmental':\n",
        "        return \"Environment, Community, Future Generations\"\n",
        "    elif theme == 'social':\n",
        "        if 'employee' in category or 'workforce' in category or 'gender' in category:\n",
        "            return \"Employees\"\n",
        "        elif 'customer' in category or 'safety' in category:\n",
        "            return \"Customers, Community\"\n",
        "        elif 'community' in category:\n",
        "            return \"Local Communities\"\n",
        "        elif 'supply' in category or 'supplier' in kpi_text:\n",
        "            return \"Suppliers, Business Partners\"\n",
        "        else:\n",
        "            return \"Employees, Community\"\n",
        "    elif theme == 'governance':\n",
        "        if 'board' in category:\n",
        "            return \"Shareholders, Investors\"\n",
        "        elif 'cyber' in category or 'data' in category:\n",
        "            return \"Customers, Employees, Business Partners\"\n",
        "        else:\n",
        "            return \"Shareholders, Investors, Stakeholders\"\n",
        "    else:\n",
        "        return \"All Stakeholders\"\n",
        "\n",
        "def save_results(df_auto: pd.DataFrame, output_path: str, pdf_path: str = \"\") -> None:\n",
        "    \"\"\"Save results to Excel file with proper formatting\"\"\"\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True)\n",
        "\n",
        "        if not df_auto.empty:\n",
        "            # Add metadata columns\n",
        "            pdf_filename = os.path.basename(pdf_path) if pdf_path else \"Unknown\"\n",
        "            df_auto['PDF file name'] = pdf_filename\n",
        "            df_auto['Title of the report'] = \"\"\n",
        "\n",
        "            if 'source_page' in df_auto.columns:\n",
        "                df_auto['Absolute Page Number'] = df_auto['source_page']\n",
        "                df_auto = df_auto.drop('source_page', axis=1)\n",
        "            else:\n",
        "                df_auto['Absolute Page Number'] = \"Unknown\"\n",
        "\n",
        "            df_auto['Impacted Stakeholder'] = df_auto.apply(infer_stakeholder, axis=1)\n",
        "\n",
        "            # Reorder columns\n",
        "            original_columns = [col for col in df_auto.columns if col not in\n",
        "                              ['PDF file name', 'Title of the report', 'Absolute Page Number', 'Impacted Stakeholder']]\n",
        "            new_column_order = ['PDF file name', 'Title of the report', 'Absolute Page Number', 'Impacted Stakeholder'] + original_columns\n",
        "            df_auto = df_auto[new_column_order]\n",
        "\n",
        "        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
        "            df_auto.to_excel(writer, sheet_name='Auto_KPIs', index=False)\n",
        "\n",
        "            if not df_auto.empty:\n",
        "                # Theme summary\n",
        "                theme_summary = df_auto.groupby('kpi_theme').size().reset_index(name='count')\n",
        "                theme_summary.to_excel(writer, sheet_name='Theme_Summary', index=False)\n",
        "\n",
        "                # Category summary\n",
        "                category_summary = df_auto.groupby(['kpi_theme', 'kpi_category']).size().reset_index(name='count')\n",
        "                category_summary.to_excel(writer, sheet_name='Category_Summary', index=False)\n",
        "\n",
        "        logging.info(f\"Results saved to {output_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving results: {e}\")"
      ],
      "metadata": {
        "id": "kucPzNTdZouV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Main execution function ============\n",
        "def main():\n",
        "    \"\"\"Enhanced main execution function with validation\"\"\"\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format=\"%(asctime)s - %(levelname)s: %(message)s\",\n",
        "        datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(PDF_PATH):\n",
        "            logging.error(f\"PDF file not found: {PDF_PATH}\")\n",
        "            return\n",
        "\n",
        "        # Process the PDF\n",
        "        df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
        "\n",
        "        # Save results\n",
        "        save_results(df_auto, EXPORT_AUTO_XLSX, PDF_PATH)\n",
        "        logging.info(f\"KPI extraction completed: {len(df_auto)} KPIs extracted\")\n",
        "\n",
        "        # Enhanced validation with comprehensive analysis\n",
        "        if MANUAL_XLSX and Path(MANUAL_XLSX).exists():\n",
        "            print(\"\\nüîç Running comprehensive validation...\")\n",
        "            validation_results = enhanced_compare_with_manual_kpis(\n",
        "                df_auto, MANUAL_XLSX, \"comprehensive_validation\"\n",
        "            )\n",
        "\n",
        "            if validation_results:\n",
        "                print(\"‚úÖ Validation completed with detailed analysis!\")\n",
        "                print(f\"üìÅ Detailed results saved to: comprehensive_validation/\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è Validation encountered issues\")\n",
        "        else:\n",
        "            logging.info(\"Manual KPI file not found, skipping validation.\")\n",
        "\n",
        "        # Display summary\n",
        "        if not df_auto.empty:\n",
        "            print(f\"\\n=== Extraction Summary ===\")\n",
        "            print(f\"Total KPIs extracted: {len(df_auto)}\")\n",
        "\n",
        "            # Source statistics\n",
        "            if 'source_type' in df_auto.columns:\n",
        "                source_counts = df_auto['source_type'].value_counts()\n",
        "                print(f\"From text/tables: {source_counts.get('text', 0)}\")\n",
        "                print(f\"From images/charts: {source_counts.get('image', 0)}\")\n",
        "\n",
        "            # Theme statistics\n",
        "            if 'kpi_theme' in df_auto.columns:\n",
        "                theme_counts = df_auto['kpi_theme'].value_counts()\n",
        "                print(f\"\\nKPI Distribution by Theme:\")\n",
        "                for theme, count in theme_counts.items():\n",
        "                    print(f\"  {theme}: {count}\")\n",
        "        else:\n",
        "            print(\"\\nNo KPIs were extracted from the document.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in main execution: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "uSueutkDZowK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Auxiliary functions ============\n",
        "def install_dependencies():\n",
        "    \"\"\"Install required dependencies\"\"\"\n",
        "    try:\n",
        "        import subprocess\n",
        "        import sys\n",
        "\n",
        "        dependencies = [\n",
        "            \"openai\",\n",
        "            \"python-dotenv\",\n",
        "            \"pdfplumber\",\n",
        "            \"tiktoken\",\n",
        "            \"pandas\",\n",
        "            \"PyMuPDF\",\n",
        "            \"Pillow\",\n",
        "            \"openpyxl\"\n",
        "        ]\n",
        "\n",
        "        for dep in dependencies:\n",
        "            try:\n",
        "                __import__(dep.replace('-', '_'))\n",
        "                print(f\"‚úÖ {dep} is already installed\")\n",
        "            except ImportError:\n",
        "                print(f\"Installing {dep}...\")\n",
        "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", dep])\n",
        "                print(f\"‚úÖ Installed {dep}\")\n",
        "\n",
        "        # Optional Camelot installation\n",
        "        try:\n",
        "            import camelot\n",
        "            print(\"‚úÖ Camelot is already installed\")\n",
        "        except ImportError:\n",
        "            print(\"Installing Camelot (optional)...\")\n",
        "            try:\n",
        "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"camelot-py[cv]\"])\n",
        "                print(\"‚úÖ Installed Camelot\")\n",
        "            except:\n",
        "                print(\"‚ö†Ô∏è Camelot installation failed (optional dependency)\")\n",
        "\n",
        "        print(\"üéâ All dependencies checked/installed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error with dependencies: {e}\")\n",
        "\n",
        "def validate_environment():\n",
        "    \"\"\"Validate environment setup\"\"\"\n",
        "    issues = []\n",
        "\n",
        "    # Check API key\n",
        "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "        issues.append(\"OPENAI_API_KEY not found in environment variables\")\n",
        "\n",
        "    # Check PDF file\n",
        "    if not os.path.exists(PDF_PATH):\n",
        "        issues.append(f\"PDF file not found: {PDF_PATH}\")\n",
        "\n",
        "    # Check required imports\n",
        "    required_modules = ['openai', 'pdfplumber', 'pandas', 'tiktoken', 'PIL', 'fitz']\n",
        "    for module in required_modules:\n",
        "        try:\n",
        "            __import__(module)\n",
        "        except ImportError:\n",
        "            issues.append(f\"Required module '{module}' not installed\")\n",
        "\n",
        "    if issues:\n",
        "        print(\"‚ùå Environment validation failed:\")\n",
        "        for issue in issues:\n",
        "            print(f\"  - {issue}\")\n",
        "        return False\n",
        "    else:\n",
        "        print(\"‚úÖ Environment validation passed\")\n",
        "        return True\n"
      ],
      "metadata": {
        "id": "qcCShhlea6ZK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Simplified execution interface ============\n",
        "def run_kpi_extraction():\n",
        "    \"\"\"Simplified interface to run KPI extraction\"\"\"\n",
        "    print(\"üöÄ Starting KPI extraction process...\")\n",
        "\n",
        "    # Validate environment\n",
        "    if not validate_environment():\n",
        "        print(\"Please fix the environment issues before running.\")\n",
        "        return\n",
        "\n",
        "    # Run main function\n",
        "    main()"
      ],
      "metadata": {
        "id": "Qw6B5H8ga6bl"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Optimized execution interface ============\n",
        "def run_optimized_kpi_extraction():\n",
        "    \"\"\"Run optimized KPI extraction\"\"\"\n",
        "    print(\"‚ö° Starting OPTIMIZED KPI extraction...\")\n",
        "    print(\"üéØ Goal: Extract ALL KPIs with 60-70% better performance\")\n",
        "\n",
        "    # Verify the environment\n",
        "    if not validate_environment():\n",
        "        print(\"Please fix the environment issues before running.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Run optimization process\n",
        "        df_results = process_sustainability_report_OPTIMIZED(PDF_PATH)\n",
        "\n",
        "        # Save the results\n",
        "        output_file = \"OPTIMIZED_\" + EXPORT_AUTO_XLSX\n",
        "        save_results(df_results, output_file, PDF_PATH)\n",
        "        print(f\"üíæ Results saved to: {output_file}\")\n",
        "\n",
        "        # Show Statistics\n",
        "        if not df_results.empty and 'source_type' in df_results.columns:\n",
        "            source_counts = df_results['source_type'].value_counts()\n",
        "            print(f\"\\nüìà Final Statistics:\")\n",
        "            for source, count in source_counts.items():\n",
        "                print(f\"   - {source}: {count} KPIs\")\n",
        "\n",
        "        return df_results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Optimized extraction failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def compare_original_vs_optimized():\n",
        "    \"\"\"Compare the performance of the original version and the optimized version\"\"\"\n",
        "    print(\"üî¨ Performance Comparison Test\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Test the original version\n",
        "    print(\"\\nüìä Testing Original Version...\")\n",
        "    original_start = time.time()\n",
        "    try:\n",
        "        original_df = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
        "        original_time = time.time() - original_start\n",
        "        print(f\"‚è±Ô∏è  Original version: {original_time:.1f}s, {len(original_df)} KPIs\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Original version failed: {e}\")\n",
        "        original_time = 999\n",
        "        original_df = pd.DataFrame()\n",
        "\n",
        "    # Test optimized version\n",
        "    print(\"\\n‚ö° Testing Optimized Version...\")\n",
        "    optimized_start = time.time()\n",
        "    try:\n",
        "        optimized_df = process_sustainability_report_OPTIMIZED(PDF_PATH)\n",
        "        optimized_time = time.time() - optimized_start\n",
        "        print(f\"‚è±Ô∏è  Optimized version: {optimized_time:.1f}s, {len(optimized_df)} KPIs\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Optimized version failed: {e}\")\n",
        "        optimized_time = 999\n",
        "        optimized_df = pd.DataFrame()\n",
        "\n",
        "    # Performance comparison\n",
        "    if original_time < 999 and optimized_time < 999:\n",
        "        speedup = original_time / optimized_time\n",
        "        time_saved = original_time - optimized_time\n",
        "        kpi_diff = abs(len(optimized_df) - len(original_df))\n",
        "\n",
        "        print(f\"\\nüöÄ Performance Results:\")\n",
        "        print(f\"   - Speed improvement: {speedup:.1f}x faster\")\n",
        "        print(f\"   - Time saved: {time_saved:.1f}s ({time_saved/60:.1f}min)\")\n",
        "        print(f\"   - KPI difference: {kpi_diff} KPIs\")\n",
        "        print(f\"   - Completeness: {len(optimized_df)/len(original_df)*100:.1f}% of original\" if len(original_df) > 0 else \"\")\n",
        "\n",
        "        return {\"original\": original_df, \"optimized\": optimized_df, \"speedup\": speedup}\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "aJrVHLTgmPJw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KPIValidationPipeline:\n",
        "    \"\"\"Comprehensive KPI validation and evaluation system\"\"\"\n",
        "\n",
        "    def __init__(self, manual_excel_path: str, auto_excel_path: str,\n",
        "                 output_dir: str = \"validation_results\"):\n",
        "        \"\"\"\n",
        "        Initialize validation pipeline\n",
        "\n",
        "        Args:\n",
        "            manual_excel_path: Path to manual KPI annotations\n",
        "            auto_excel_path: Path to automatically extracted KPIs\n",
        "            output_dir: Directory to save validation results\n",
        "        \"\"\"\n",
        "        self.manual_path = manual_excel_path\n",
        "        self.auto_path = auto_excel_path\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Load data\n",
        "        self.manual_df = self._load_excel_safe(manual_excel_path, \"manual\")\n",
        "        self.auto_df = self._load_excel_safe(auto_excel_path, \"auto\")\n",
        "\n",
        "        # Validation results\n",
        "        self.validation_results = {}\n",
        "        self.detailed_analysis = {}\n",
        "\n",
        "        # Similarity thresholds\n",
        "        self.similarity_thresholds = {\n",
        "            'exact': 1.0,\n",
        "            'high': 0.9,\n",
        "            'medium': 0.7,\n",
        "            'low': 0.5\n",
        "        }\n",
        "\n",
        "        logging.info(f\"Validation pipeline initialized:\")\n",
        "        logging.info(f\"  Manual KPIs: {len(self.manual_df)}\")\n",
        "        logging.info(f\"  Auto KPIs: {len(self.auto_df)}\")\n",
        "\n",
        "    def _load_excel_safe(self, filepath: str, source_type: str) -> pd.DataFrame:\n",
        "        \"\"\"Safely load Excel file with error handling\"\"\"\n",
        "        try:\n",
        "            if not Path(filepath).exists():\n",
        "                logging.warning(f\"{source_type.title()} file not found: {filepath}\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            df = pd.read_excel(filepath)\n",
        "            logging.info(f\"Loaded {source_type} file: {len(df)} rows\")\n",
        "\n",
        "            # Standardize column names\n",
        "            df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "            # Ensure required columns exist\n",
        "            required_cols = ['kpi_text']\n",
        "            for col in required_cols:\n",
        "                if col not in df.columns:\n",
        "                    # Try to find similar column names\n",
        "                    similar_cols = [c for c in df.columns if 'kpi' in c.lower() or 'text' in c.lower()]\n",
        "                    if similar_cols:\n",
        "                        df['kpi_text'] = df[similar_cols[0]]\n",
        "                        logging.info(f\"Using column '{similar_cols[0]}' as kpi_text\")\n",
        "                    else:\n",
        "                        logging.warning(f\"Required column '{col}' not found in {source_type} file\")\n",
        "                        df['kpi_text'] = \"\"\n",
        "\n",
        "            # Clean text data\n",
        "            df['kpi_text'] = df['kpi_text'].astype(str).str.strip()\n",
        "            df = df[df['kpi_text'] != ''].reset_index(drop=True)\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading {source_type} file: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def normalize_text(self, text: str) -> str:\n",
        "        \"\"\"Normalize text for comparison\"\"\"\n",
        "        if pd.isna(text) or text == '':\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to string and lowercase\n",
        "        text = str(text).lower().strip()\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        # Remove common punctuation but keep percentages and numbers\n",
        "        text = re.sub(r'[^\\w\\s\\%\\.\\,\\-]', ' ', text)\n",
        "\n",
        "        # Normalize number formats\n",
        "        text = re.sub(r'\\b(\\d+),(\\d+)\\b', r'\\1\\2', text)  # Remove commas in numbers\n",
        "        text = re.sub(r'\\s+', ' ', text)  # Normalize spaces\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def calculate_text_similarity(self, text1: str, text2: str) -> Dict[str, float]:\n",
        "        \"\"\"Calculate multiple similarity metrics between two texts\"\"\"\n",
        "        norm1 = self.normalize_text(text1)\n",
        "        norm2 = self.normalize_text(text2)\n",
        "\n",
        "        if not norm1 or not norm2:\n",
        "            return {'sequence': 0.0, 'cosine': 0.0, 'jaccard': 0.0, 'combined': 0.0}\n",
        "\n",
        "        # 1. Sequence similarity (exact match)\n",
        "        sequence_sim = SequenceMatcher(None, norm1, norm2).ratio()\n",
        "\n",
        "        # 2. Cosine similarity (semantic)\n",
        "        try:\n",
        "            vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=1)\n",
        "            tfidf_matrix = vectorizer.fit_transform([norm1, norm2])\n",
        "            cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "        except:\n",
        "            cosine_sim = 0.0\n",
        "\n",
        "        # 3. Jaccard similarity (token overlap)\n",
        "        tokens1 = set(norm1.split())\n",
        "        tokens2 = set(norm2.split())\n",
        "        if tokens1 or tokens2:\n",
        "            jaccard_sim = len(tokens1.intersection(tokens2)) / len(tokens1.union(tokens2))\n",
        "        else:\n",
        "            jaccard_sim = 0.0\n",
        "\n",
        "        # 4. Combined similarity\n",
        "        combined_sim = (sequence_sim * 0.4 + cosine_sim * 0.4 + jaccard_sim * 0.2)\n",
        "\n",
        "        return {\n",
        "            'sequence': sequence_sim,\n",
        "            'cosine': cosine_sim,\n",
        "            'jaccard': jaccard_sim,\n",
        "            'combined': combined_sim\n",
        "        }\n",
        "\n",
        "    def find_matches(self, threshold: float = 0.7, similarity_type: str = 'combined') -> pd.DataFrame:\n",
        "        \"\"\"Find matches between manual and auto KPIs\"\"\"\n",
        "        matches = []\n",
        "        auto_matched = set()\n",
        "\n",
        "        for manual_idx, manual_row in self.manual_df.iterrows():\n",
        "            manual_text = manual_row['kpi_text']\n",
        "            best_match = None\n",
        "            best_similarity = 0.0\n",
        "\n",
        "            for auto_idx, auto_row in self.auto_df.iterrows():\n",
        "                if auto_idx in auto_matched:\n",
        "                    continue\n",
        "\n",
        "                auto_text = auto_row['kpi_text']\n",
        "                similarities = self.calculate_text_similarity(manual_text, auto_text)\n",
        "                similarity = similarities[similarity_type]\n",
        "\n",
        "                if similarity > best_similarity and similarity >= threshold:\n",
        "                    best_similarity = similarity\n",
        "                    best_match = {\n",
        "                        'manual_idx': manual_idx,\n",
        "                        'auto_idx': auto_idx,\n",
        "                        'manual_text': manual_text,\n",
        "                        'auto_text': auto_text,\n",
        "                        'similarity': similarity,\n",
        "                        'all_similarities': similarities\n",
        "                    }\n",
        "\n",
        "            if best_match:\n",
        "                matches.append(best_match)\n",
        "                auto_matched.add(best_match['auto_idx'])\n",
        "\n",
        "        return pd.DataFrame(matches)\n",
        "\n",
        "    def calculate_metrics_at_threshold(self, threshold: float = 0.7,\n",
        "                                     similarity_type: str = 'combined') -> Dict[str, float]:\n",
        "        \"\"\"Calculate precision, recall, F1 at specific threshold\"\"\"\n",
        "        matches_df = self.find_matches(threshold, similarity_type)\n",
        "\n",
        "        true_positives = len(matches_df)\n",
        "        false_positives = len(self.auto_df) - true_positives\n",
        "        false_negatives = len(self.manual_df) - true_positives\n",
        "\n",
        "        # Calculate metrics\n",
        "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0\n",
        "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "        return {\n",
        "            'threshold': threshold,\n",
        "            'similarity_type': similarity_type,\n",
        "            'true_positives': true_positives,\n",
        "            'false_positives': false_positives,\n",
        "            'false_negatives': false_negatives,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1_score,\n",
        "            'total_manual': len(self.manual_df),\n",
        "            'total_auto': len(self.auto_df),\n",
        "            'match_rate': true_positives / len(self.manual_df) if len(self.manual_df) > 0 else 0.0\n",
        "        }\n",
        "\n",
        "    def run_comprehensive_evaluation(self) -> Dict[str, any]:\n",
        "        \"\"\"Run comprehensive evaluation across multiple thresholds and similarity types\"\"\"\n",
        "        logging.info(\"Running comprehensive evaluation...\")\n",
        "\n",
        "        results = {\n",
        "            'threshold_analysis': [],\n",
        "            'similarity_type_analysis': [],\n",
        "            'category_analysis': {},\n",
        "            'detailed_matches': {},\n",
        "            'false_positives': [],\n",
        "            'false_negatives': []\n",
        "        }\n",
        "\n",
        "        # 1. Threshold analysis\n",
        "        thresholds = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "        similarity_types = ['combined', 'sequence', 'cosine', 'jaccard']\n",
        "\n",
        "        for threshold in thresholds:\n",
        "            for sim_type in similarity_types:\n",
        "                metrics = self.calculate_metrics_at_threshold(threshold, sim_type)\n",
        "                results['threshold_analysis'].append(metrics)\n",
        "\n",
        "        # 2. Find optimal threshold\n",
        "        best_f1 = 0.0\n",
        "        best_config = None\n",
        "        for metrics in results['threshold_analysis']:\n",
        "            if metrics['f1_score'] > best_f1:\n",
        "                best_f1 = metrics['f1_score']\n",
        "                best_config = (metrics['threshold'], metrics['similarity_type'])\n",
        "\n",
        "        # 3. Detailed analysis at optimal threshold\n",
        "        if best_config:\n",
        "            optimal_threshold, optimal_sim_type = best_config\n",
        "            logging.info(f\"Optimal configuration: threshold={optimal_threshold}, similarity={optimal_sim_type}\")\n",
        "\n",
        "            matches_df = self.find_matches(optimal_threshold, optimal_sim_type)\n",
        "            results['detailed_matches'] = matches_df.to_dict('records')\n",
        "\n",
        "            # Find false positives and false negatives\n",
        "            matched_auto_indices = set(matches_df['auto_idx'].tolist()) if not matches_df.empty else set()\n",
        "            matched_manual_indices = set(matches_df['manual_idx'].tolist()) if not matches_df.empty else set()\n",
        "\n",
        "            # False positives (auto KPIs not matched to manual)\n",
        "            fp_indices = set(range(len(self.auto_df))) - matched_auto_indices\n",
        "            results['false_positives'] = [\n",
        "                {\n",
        "                    'index': idx,\n",
        "                    'kpi_text': self.auto_df.iloc[idx]['kpi_text'],\n",
        "                    'category': self.auto_df.iloc[idx].get('kpi_category', 'Unknown'),\n",
        "                    'theme': self.auto_df.iloc[idx].get('kpi_theme', 'Unknown'),\n",
        "                    'source': self.auto_df.iloc[idx].get('source_type', 'Unknown')\n",
        "                }\n",
        "                for idx in fp_indices\n",
        "            ]\n",
        "\n",
        "            # False negatives (manual KPIs not matched by auto)\n",
        "            fn_indices = set(range(len(self.manual_df))) - matched_manual_indices\n",
        "            results['false_negatives'] = [\n",
        "                {\n",
        "                    'index': idx,\n",
        "                    'kpi_text': self.manual_df.iloc[idx]['kpi_text'],\n",
        "                    'category': self.manual_df.iloc[idx].get('kpi_category', 'Unknown'),\n",
        "                    'theme': self.manual_df.iloc[idx].get('kpi_theme', 'Unknown')\n",
        "                }\n",
        "                for idx in fn_indices\n",
        "            ]\n",
        "\n",
        "        # 4. Category-level analysis\n",
        "        if 'kpi_category' in self.manual_df.columns and 'kpi_category' in self.auto_df.columns:\n",
        "            results['category_analysis'] = self._analyze_by_category()\n",
        "\n",
        "        # 5. Theme-level analysis\n",
        "        if 'kpi_theme' in self.manual_df.columns and 'kpi_theme' in self.auto_df.columns:\n",
        "            results['theme_analysis'] = self._analyze_by_theme()\n",
        "\n",
        "        self.validation_results = results\n",
        "        return results\n",
        "\n",
        "    def _analyze_by_category(self) -> Dict[str, Dict]:\n",
        "        \"\"\"Analyze performance by KPI category\"\"\"\n",
        "        category_results = {}\n",
        "\n",
        "        manual_categories = self.manual_df['kpi_category'].value_counts()\n",
        "        auto_categories = self.auto_df['kpi_category'].value_counts()\n",
        "\n",
        "        all_categories = set(manual_categories.index) | set(auto_categories.index)\n",
        "\n",
        "        for category in all_categories:\n",
        "            manual_count = manual_categories.get(category, 0)\n",
        "            auto_count = auto_categories.get(category, 0)\n",
        "\n",
        "            # Find matches within this category\n",
        "            manual_cat_df = self.manual_df[self.manual_df['kpi_category'] == category]\n",
        "            auto_cat_df = self.auto_df[self.auto_df['kpi_category'] == category]\n",
        "\n",
        "            category_matches = 0\n",
        "            if not manual_cat_df.empty and not auto_cat_df.empty:\n",
        "                for _, manual_row in manual_cat_df.iterrows():\n",
        "                    best_sim = 0.0\n",
        "                    for _, auto_row in auto_cat_df.iterrows():\n",
        "                        sim = self.calculate_text_similarity(\n",
        "                            manual_row['kpi_text'],\n",
        "                            auto_row['kpi_text']\n",
        "                        )['combined']\n",
        "                        best_sim = max(best_sim, sim)\n",
        "                    if best_sim >= 0.7:\n",
        "                        category_matches += 1\n",
        "\n",
        "            category_precision = category_matches / auto_count if auto_count > 0 else 0.0\n",
        "            category_recall = category_matches / manual_count if manual_count > 0 else 0.0\n",
        "            category_f1 = 2 * (category_precision * category_recall) / (category_precision + category_recall) if (category_precision + category_recall) > 0 else 0.0\n",
        "\n",
        "            category_results[category] = {\n",
        "                'manual_count': manual_count,\n",
        "                'auto_count': auto_count,\n",
        "                'matches': category_matches,\n",
        "                'precision': category_precision,\n",
        "                'recall': category_recall,\n",
        "                'f1_score': category_f1\n",
        "            }\n",
        "\n",
        "        return category_results\n",
        "\n",
        "    def _analyze_by_theme(self) -> Dict[str, Dict]:\n",
        "        \"\"\"Analyze performance by KPI theme\"\"\"\n",
        "        theme_results = {}\n",
        "\n",
        "        manual_themes = self.manual_df['kpi_theme'].value_counts()\n",
        "        auto_themes = self.auto_df['kpi_theme'].value_counts()\n",
        "\n",
        "        all_themes = set(manual_themes.index) | set(auto_themes.index)\n",
        "\n",
        "        for theme in all_themes:\n",
        "            manual_count = manual_themes.get(theme, 0)\n",
        "            auto_count = auto_themes.get(theme, 0)\n",
        "\n",
        "            theme_results[theme] = {\n",
        "                'manual_count': manual_count,\n",
        "                'auto_count': auto_count,\n",
        "                'coverage': auto_count / manual_count if manual_count > 0 else 0.0\n",
        "            }\n",
        "\n",
        "        return theme_results\n",
        "\n",
        "    def generate_visualizations(self):\n",
        "        \"\"\"Generate comprehensive visualizations\"\"\"\n",
        "        if not self.validation_results:\n",
        "            logging.warning(\"No validation results found. Run evaluation first.\")\n",
        "            return\n",
        "\n",
        "        # Set style\n",
        "        try:\n",
        "            plt.style.use('seaborn-v0_8')\n",
        "        except:\n",
        "            plt.style.use('seaborn')  # Â§áÁî®Ê†∑Âºè\n",
        "        fig = plt.figure(figsize=(20, 16))\n",
        "\n",
        "        # 1. Threshold analysis\n",
        "        threshold_df = pd.DataFrame(self.validation_results['threshold_analysis'])\n",
        "\n",
        "        plt.subplot(3, 3, 1)\n",
        "        for sim_type in threshold_df['similarity_type'].unique():\n",
        "            data = threshold_df[threshold_df['similarity_type'] == sim_type]\n",
        "            plt.plot(data['threshold'], data['f1_score'], marker='o', label=sim_type)\n",
        "        plt.xlabel('Similarity Threshold')\n",
        "        plt.ylabel('F1 Score')\n",
        "        plt.title('F1 Score vs Threshold by Similarity Type')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 2. Precision-Recall curve\n",
        "        plt.subplot(3, 3, 2)\n",
        "        for sim_type in threshold_df['similarity_type'].unique():\n",
        "            data = threshold_df[threshold_df['similarity_type'] == sim_type]\n",
        "            plt.plot(data['recall'], data['precision'], marker='o', label=sim_type)\n",
        "        plt.xlabel('Recall')\n",
        "        plt.ylabel('Precision')\n",
        "        plt.title('Precision-Recall Curves')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 3. Category analysis\n",
        "        if 'category_analysis' in self.validation_results:\n",
        "            plt.subplot(3, 3, 3)\n",
        "            cat_analysis = self.validation_results['category_analysis']\n",
        "            categories = list(cat_analysis.keys())[:10]  # Top 10 categories\n",
        "            f1_scores = [cat_analysis[cat]['f1_score'] for cat in categories]\n",
        "\n",
        "            plt.barh(categories, f1_scores)\n",
        "            plt.xlabel('F1 Score')\n",
        "            plt.title('F1 Score by Category (Top 10)')\n",
        "            plt.tight_layout()\n",
        "\n",
        "        # 4. Theme distribution comparison\n",
        "        plt.subplot(3, 3, 4)\n",
        "        if 'kpi_theme' in self.manual_df.columns:\n",
        "            manual_themes = self.manual_df['kpi_theme'].value_counts()\n",
        "            auto_themes = self.auto_df['kpi_theme'].value_counts()\n",
        "\n",
        "            x = np.arange(len(manual_themes))\n",
        "            width = 0.35\n",
        "\n",
        "            plt.bar(x - width/2, manual_themes.values, width, label='Manual', alpha=0.8)\n",
        "            plt.bar(x + width/2, auto_themes.reindex(manual_themes.index, fill_value=0).values,\n",
        "                   width, label='Auto', alpha=0.8)\n",
        "\n",
        "            plt.xlabel('Theme')\n",
        "            plt.ylabel('Count')\n",
        "            plt.title('KPI Count by Theme')\n",
        "            plt.xticks(x, manual_themes.index, rotation=45)\n",
        "            plt.legend()\n",
        "\n",
        "        # 5. Similarity distribution\n",
        "        plt.subplot(3, 3, 5)\n",
        "        if self.validation_results['detailed_matches']:\n",
        "            similarities = [match['similarity'] for match in self.validation_results['detailed_matches']]\n",
        "            plt.hist(similarities, bins=20, edgecolor='black', alpha=0.7)\n",
        "            plt.xlabel('Similarity Score')\n",
        "            plt.ylabel('Frequency')\n",
        "            plt.title('Distribution of Similarity Scores (Matches)')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 6. Error analysis\n",
        "        plt.subplot(3, 3, 6)\n",
        "        fp_count = len(self.validation_results['false_positives'])\n",
        "        fn_count = len(self.validation_results['false_negatives'])\n",
        "        tp_count = len(self.validation_results['detailed_matches'])\n",
        "\n",
        "        labels = ['True Positives', 'False Positives', 'False Negatives']\n",
        "        counts = [tp_count, fp_count, fn_count]\n",
        "        colors = ['green', 'red', 'orange']\n",
        "\n",
        "        plt.pie(counts, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "        plt.title('Classification Results')\n",
        "\n",
        "        # 7. Coverage by source type\n",
        "        plt.subplot(3, 3, 7)\n",
        "        if 'source_type' in self.auto_df.columns:\n",
        "            source_counts = self.auto_df['source_type'].value_counts()\n",
        "            plt.pie(source_counts.values, labels=source_counts.index, autopct='%1.1f%%')\n",
        "            plt.title('Auto KPIs by Source Type')\n",
        "\n",
        "        # 8. Performance metrics summary\n",
        "        plt.subplot(3, 3, 8)\n",
        "        best_metrics = max(self.validation_results['threshold_analysis'],\n",
        "                          key=lambda x: x['f1_score'])\n",
        "\n",
        "        metrics = ['Precision', 'Recall', 'F1 Score']\n",
        "        values = [best_metrics['precision'], best_metrics['recall'], best_metrics['f1_score']]\n",
        "\n",
        "        bars = plt.bar(metrics, values, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
        "        plt.ylabel('Score')\n",
        "        plt.title(f'Best Performance Metrics\\n(Threshold: {best_metrics[\"threshold\"]})')\n",
        "        plt.ylim(0, 1)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar, value in zip(bars, values):\n",
        "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                    f'{value:.3f}', ha='center', va='bottom')\n",
        "\n",
        "        # 9. Match quality distribution\n",
        "        plt.subplot(3, 3, 9)\n",
        "        if self.validation_results['detailed_matches']:\n",
        "            match_similarities = [match['similarity'] for match in self.validation_results['detailed_matches']]\n",
        "            quality_bins = [0.5, 0.7, 0.8, 0.9, 1.0]\n",
        "            quality_labels = ['Medium', 'Good', 'Very Good', 'Excellent']\n",
        "\n",
        "            quality_counts = []\n",
        "            for i in range(len(quality_bins)-1):\n",
        "                count = sum(1 for sim in match_similarities\n",
        "                          if quality_bins[i] <= sim < quality_bins[i+1])\n",
        "                quality_counts.append(count)\n",
        "\n",
        "            plt.bar(quality_labels, quality_counts, color='lightblue', edgecolor='black')\n",
        "            plt.ylabel('Number of Matches')\n",
        "            plt.title('Match Quality Distribution')\n",
        "            plt.xticks(rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save visualization\n",
        "        viz_path = self.output_dir / \"validation_visualizations.png\"\n",
        "        plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        logging.info(f\"Visualizations saved to {viz_path}\")\n",
        "\n",
        "    def generate_detailed_report(self) -> str:\n",
        "        \"\"\"Generate comprehensive validation report\"\"\"\n",
        "        if not self.validation_results:\n",
        "            logging.warning(\"No validation results found. Run evaluation first.\")\n",
        "            return \"\"\n",
        "\n",
        "        # Find best configuration\n",
        "        best_metrics = max(self.validation_results['threshold_analysis'],\n",
        "                          key=lambda x: x['f1_score'])\n",
        "\n",
        "        report = f\"\"\"\n",
        "# KPI Extraction Validation Report\n",
        "Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "## Dataset Overview\n",
        "- **Manual KPIs**: {len(self.manual_df)} annotations\n",
        "- **Auto KPIs**: {len(self.auto_df)} extractions\n",
        "- **Manual file**: {self.manual_path}\n",
        "- **Auto file**: {self.auto_path}\n",
        "\n",
        "## Best Performance Configuration\n",
        "- **Similarity Type**: {best_metrics['similarity_type']}\n",
        "- **Threshold**: {best_metrics['threshold']}\n",
        "- **Precision**: {best_metrics['precision']:.3f}\n",
        "- **Recall**: {best_metrics['recall']:.3f}\n",
        "- **F1 Score**: {best_metrics['f1_score']:.3f}\n",
        "\n",
        "## Detailed Metrics\n",
        "- **True Positives**: {best_metrics['true_positives']}\n",
        "- **False Positives**: {best_metrics['false_positives']}\n",
        "- **False Negatives**: {best_metrics['false_negatives']}\n",
        "- **Match Rate**: {best_metrics['match_rate']:.3f}\n",
        "\n",
        "## Error Analysis\n",
        "\n",
        "### False Positives ({len(self.validation_results['false_positives'])})\n",
        "KPIs extracted automatically but not in manual annotations:\n",
        "\"\"\"\n",
        "\n",
        "        # Add false positives\n",
        "        for i, fp in enumerate(self.validation_results['false_positives'][:10], 1):\n",
        "            report += f\"\\n{i}. **{fp['category']}** | {fp['kpi_text']}\\n\"\n",
        "\n",
        "        if len(self.validation_results['false_positives']) > 10:\n",
        "            report += f\"\\n... and {len(self.validation_results['false_positives']) - 10} more\\n\"\n",
        "\n",
        "        report += f\"\"\"\n",
        "### False Negatives ({len(self.validation_results['false_negatives'])})\n",
        "KPIs in manual annotations but missed by extraction:\n",
        "\"\"\"\n",
        "\n",
        "        # Add false negatives\n",
        "        for i, fn in enumerate(self.validation_results['false_negatives'][:10], 1):\n",
        "            report += f\"\\n{i}. **{fn['category']}** | {fn['kpi_text']}\\n\"\n",
        "\n",
        "        if len(self.validation_results['false_negatives']) > 10:\n",
        "            report += f\"\\n... and {len(self.validation_results['false_negatives']) - 10} more\\n\"\n",
        "\n",
        "        # Category analysis\n",
        "        if 'category_analysis' in self.validation_results:\n",
        "            report += \"\\n## Category-wise Performance\\n\\n\"\n",
        "            report += \"| Category | Manual | Auto | Matches | Precision | Recall | F1 |\\n\"\n",
        "            report += \"|----------|--------|------|---------|-----------|--------|----|\\\\n\"\n",
        "\n",
        "            for category, metrics in self.validation_results['category_analysis'].items():\n",
        "                report += f\"| {category[:20]} | {metrics['manual_count']} | {metrics['auto_count']} | {metrics['matches']} | {metrics['precision']:.3f} | {metrics['recall']:.3f} | {metrics['f1_score']:.3f} |\\n\"\n",
        "\n",
        "        # Theme analysis\n",
        "        if 'theme_analysis' in self.validation_results:\n",
        "            report += \"\\n## Theme-wise Coverage\\n\\n\"\n",
        "            report += \"| Theme | Manual Count | Auto Count | Coverage |\\n\"\n",
        "            report += \"|-------|--------------|------------|----------|\\n\"\n",
        "\n",
        "            for theme, metrics in self.validation_results['theme_analysis'].items():\n",
        "                report += f\"| {theme} | {metrics['manual_count']} | {metrics['auto_count']} | {metrics['coverage']:.3f} |\\n\"\n",
        "\n",
        "        # Recommendations\n",
        "        report += f\"\"\"\n",
        "## Recommendations\n",
        "\n",
        "### Strengths\n",
        "- Overall F1 Score: {best_metrics['f1_score']:.3f}\n",
        "- Precision: {best_metrics['precision']:.3f} (low false positive rate)\n",
        "- Recall: {best_metrics['recall']:.3f} (good coverage)\n",
        "\n",
        "### Areas for Improvement\n",
        "\"\"\"\n",
        "\n",
        "        if best_metrics['precision'] < 0.8:\n",
        "            report += \"- **Precision**: Consider stricter filtering to reduce false positives\\n\"\n",
        "\n",
        "        if best_metrics['recall'] < 0.8:\n",
        "            report += \"- **Recall**: Improve extraction to catch more manual KPIs\\n\"\n",
        "\n",
        "        if best_metrics['f1_score'] < 0.7:\n",
        "            report += \"- **Overall Performance**: Significant room for improvement in both precision and recall\\n\"\n",
        "\n",
        "        # Source-specific recommendations\n",
        "        if 'source_type' in self.auto_df.columns:\n",
        "            text_kpis = len(self.auto_df[self.auto_df['source_type'] == 'text'])\n",
        "            image_kpis = len(self.auto_df[self.auto_df['source_type'] == 'image'])\n",
        "\n",
        "            report += f\"\"\"\n",
        "### Source Type Analysis\n",
        "- **Text/Table KPIs**: {text_kpis}\n",
        "- **Image/Chart KPIs**: {image_kpis}\n",
        "- **Image Coverage**: {image_kpis / (text_kpis + image_kpis) * 100:.1f}%\n",
        "\"\"\"\n",
        "\n",
        "        return report\n",
        "\n",
        "    def save_results(self):\n",
        "        \"\"\"Save all validation results to files\"\"\"\n",
        "        # Save detailed results as JSON\n",
        "        results_path = self.output_dir / \"validation_results.json\"\n",
        "        with open(results_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.validation_results, f, indent=2, ensure_ascii=False, default=str)\n",
        "\n",
        "        # Save matches as Excel\n",
        "        if self.validation_results['detailed_matches']:\n",
        "            matches_df = pd.DataFrame(self.validation_results['detailed_matches'])\n",
        "            matches_path = self.output_dir / \"detailed_matches.xlsx\"\n",
        "            matches_df.to_excel(matches_path, index=False)\n",
        "\n",
        "        # Save false positives and negatives\n",
        "        fp_df = pd.DataFrame(self.validation_results['false_positives'])\n",
        "        fn_df = pd.DataFrame(self.validation_results['false_negatives'])\n",
        "\n",
        "        with pd.ExcelWriter(self.output_dir / \"error_analysis.xlsx\") as writer:\n",
        "            fp_df.to_excel(writer, sheet_name='False_Positives', index=False)\n",
        "            fn_df.to_excel(writer, sheet_name='False_Negatives', index=False)\n",
        "\n",
        "        # Save report\n",
        "        report = self.generate_detailed_report()\n",
        "        report_path = self.output_dir / \"validation_report.md\"\n",
        "        with open(report_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(report)\n",
        "\n",
        "        # Save metrics summary\n",
        "        threshold_df = pd.DataFrame(self.validation_results['threshold_analysis'])\n",
        "        threshold_df.to_excel(self.output_dir / \"threshold_analysis.xlsx\", index=False)\n",
        "\n",
        "        logging.info(f\"All validation results saved to {self.output_dir}\")\n",
        "\n",
        "        return {\n",
        "            'results_json': results_path,\n",
        "            'matches_excel': self.output_dir / \"detailed_matches.xlsx\",\n",
        "            'error_analysis': self.output_dir / \"error_analysis.xlsx\",\n",
        "            'report_markdown': report_path,\n",
        "            'threshold_analysis': self.output_dir / \"threshold_analysis.xlsx\",\n",
        "            'visualizations': self.output_dir / \"validation_visualizations.png\"\n",
        "        }\n",
        "\n",
        "    def run_full_validation(self) -> Dict[str, any]:\n",
        "        \"\"\"Run complete validation pipeline\"\"\"\n",
        "        logging.info(\"Starting full validation pipeline...\")\n",
        "\n",
        "        # Step 1: Run comprehensive evaluation\n",
        "        self.run_comprehensive_evaluation()\n",
        "\n",
        "        # Step 2: Generate visualizations\n",
        "        self.generate_visualizations()\n",
        "\n",
        "        # Step 3: Save all results\n",
        "        saved_files = self.save_results()\n",
        "\n",
        "        # Step 4: Print summary\n",
        "        best_metrics = max(self.validation_results['threshold_analysis'],\n",
        "                          key=lambda x: x['f1_score'])\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"KPI EXTRACTION VALIDATION SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"üìä Dataset: {len(self.manual_df)} manual vs {len(self.auto_df)} auto KPIs\")\n",
        "        print(f\"üéØ Best F1 Score: {best_metrics['f1_score']:.3f}\")\n",
        "        print(f\"üìà Precision: {best_metrics['precision']:.3f}\")\n",
        "        print(f\"üìâ Recall: {best_metrics['recall']:.3f}\")\n",
        "        print(f\"‚úÖ True Positives: {best_metrics['true_positives']}\")\n",
        "        print(f\"‚ùå False Positives: {best_metrics['false_positives']}\")\n",
        "        print(f\"‚ö†Ô∏è  False Negatives: {best_metrics['false_negatives']}\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"üìÅ Results saved to: {self.output_dir}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        return {\n",
        "            'validation_results': self.validation_results,\n",
        "            'saved_files': saved_files,\n",
        "            'best_metrics': best_metrics\n",
        "        }\n"
      ],
      "metadata": {
        "id": "UfLJWwsd4nDX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ ÂÖºÂÆπÊÄßÂáΩÊï∞ ============\n",
        "def extract_kpi_from_chunk(chunk: str) -> List[Dict]:\n",
        "    \"\"\"Backward compatibility function\"\"\"\n",
        "    return extract_kpi_from_chunk_universal(chunk)\n",
        "\n",
        "def process_sustainability_report(pdf_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Backward compatibility function for text-only processing\"\"\"\n",
        "    return process_text_only()\n",
        "\n",
        "def process_sustainability_report_with_images(pdf_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Backward compatibility function for full processing\"\"\"\n",
        "    return process_sustainability_report_with_enhanced_images(pdf_path)\n"
      ],
      "metadata": {
        "id": "uhyRnexga6fA"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ ‰ΩøÁî®Á§∫‰æã ============\n",
        "def example_usage():\n",
        "    \"\"\"Usage examples\"\"\"\n",
        "    print(\"=== KPI Extraction Tool Usage Examples ===\\n\")\n",
        "\n",
        "    print(\"1. Full extraction (text + images):\")\n",
        "    print(\"   df_results = process_sustainability_report_with_enhanced_images(PDF_PATH)\")\n",
        "    print(\"   save_results(df_results, EXPORT_AUTO_XLSX, PDF_PATH)\\n\")\n",
        "\n",
        "    print(\"2. Text-only extraction:\")\n",
        "    print(\"   df_results = process_text_only()\")\n",
        "    print(\"   # Results automatically saved\\n\")\n",
        "\n",
        "    print(\"3. Simple run:\")\n",
        "    print(\"   run_kpi_extraction()  # Complete pipeline with validation\\n\")\n",
        "\n",
        "    print(\"4. Debug single component:\")\n",
        "    print(\"   test_text_extraction_only()  # Test first 3 chunks\")\n",
        "    print(\"   debug_single_image_analysis('path/to/image.jpg')\\n\")\n",
        "\n",
        "    print(\"5. Install dependencies:\")\n",
        "    print(\"   install_dependencies()  # Install all required packages\\n\")\n"
      ],
      "metadata": {
        "id": "fldjMEkaa6g1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MetadataValidationExtension:\n",
        "    \"\"\"ÂÖÉÊï∞ÊçÆÈ™åËØÅÊâ©Â±ï - È™åËØÅKPIÊñáÊú¨‰ª•Â§ñÁöÑÂ≠óÊÆµÂáÜÁ°ÆÊÄß\"\"\"\n",
        "\n",
        "    def __init__(self, validation_pipeline: KPIValidationPipeline):\n",
        "        \"\"\"\n",
        "        Âü∫‰∫éÁé∞ÊúâÁöÑKPIValidationPipelineÊâ©Â±ïÂÖÉÊï∞ÊçÆÈ™åËØÅÂäüËÉΩ\n",
        "\n",
        "        Args:\n",
        "            validation_pipeline: Áé∞ÊúâÁöÑÈ™åËØÅÁÆ°ÈÅìÂÆû‰æã\n",
        "        \"\"\"\n",
        "        self.main_validator = validation_pipeline\n",
        "        self.metadata_results = {}\n",
        "\n",
        "        # ÈúÄË¶ÅÈ™åËØÅÁöÑÂÖÉÊï∞ÊçÆÂ≠óÊÆµÈÖçÁΩÆ\n",
        "        self.metadata_fields = {\n",
        "            # ÊñáÊ°£Áõ∏ÂÖ≥Â≠óÊÆµ\n",
        "            'document_fields': {\n",
        "                'PDF file name': {'weight': 0.05, 'type': 'exact'},\n",
        "                'Title of the report': {'weight': 0.05, 'type': 'exact'},\n",
        "                'Absolute Page Number': {'weight': 0.15, 'type': 'exact'},  # ÊÇ®ÊèêÂà∞ÁöÑÈóÆÈ¢òÂ≠óÊÆµ\n",
        "                'Impacted Stakeholder': {'weight': 0.10, 'type': 'similarity'}\n",
        "            },\n",
        "\n",
        "            # KPIÂàÜÁ±ªÂ≠óÊÆµ\n",
        "            'classification_fields': {\n",
        "                'kpi_theme': {'weight': 0.15, 'type': 'exact'},  # ÊÇ®ÊèêÂà∞ÁöÑÈóÆÈ¢òÂ≠óÊÆµ\n",
        "                'kpi_category': {'weight': 0.15, 'type': 'similarity'}\n",
        "            },\n",
        "\n",
        "            # Êï∞ÂÄºÁõ∏ÂÖ≥Â≠óÊÆµ\n",
        "            'quantitative_fields': {\n",
        "                'quantitative_value': {'weight': 0.20, 'type': 'numerical'},  # ÊÇ®ÊèêÂà∞ÁöÑÈóÆÈ¢òÂ≠óÊÆµ\n",
        "                'unit': {'weight': 0.10, 'type': 'similarity'},\n",
        "                'time_period': {'weight': 0.05, 'type': 'similarity'}\n",
        "            },\n",
        "\n",
        "            # ÂÖ∂‰ªñÂàÜÊûêÂ≠óÊÆµ\n",
        "            'analysis_fields': {\n",
        "                'target_or_actual': {'weight': 0.05, 'type': 'exact'},\n",
        "                'source_type': {'weight': 0.05, 'type': 'exact'},\n",
        "                'chart_type': {'weight': 0.03, 'type': 'similarity'},\n",
        "                'estimation_confidence': {'weight': 0.02, 'type': 'exact'},\n",
        "                'chart_title': {'weight': 0.03, 'type': 'similarity'},\n",
        "                'data_source': {'weight': 0.02, 'type': 'similarity'},\n",
        "                'image_type': {'weight': 0.02, 'type': 'exact'}\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # ÈóÆÈ¢òÂ≠óÊÆµÊ†áËÆ∞ÔºàÊÇ®ÁâπÂà´ÂÖ≥Ê≥®ÁöÑÂ≠óÊÆµÔºâ\n",
        "        self.problematic_fields = {\n",
        "            'Absolute Page Number': 'page_number_issues',\n",
        "            'kpi_theme': 'theme_classification_issues',\n",
        "            'quantitative_value': 'value_extraction_issues'\n",
        "        }\n",
        "\n",
        "        logging.info(\"ÂÖÉÊï∞ÊçÆÈ™åËØÅÊâ©Â±ïÂàùÂßãÂåñÂÆåÊàê\")\n",
        "\n",
        "    def validate_single_field(self, manual_value, auto_value, field_name: str, validation_type: str) -> Dict[str, any]:\n",
        "        \"\"\"\n",
        "        È™åËØÅÂçï‰∏™Â≠óÊÆµÁöÑÂáÜÁ°ÆÊÄß\n",
        "\n",
        "        Args:\n",
        "            manual_value: ÊâãÂä®Ê†áÊ≥®ÂÄº\n",
        "            auto_value: Ëá™Âä®ÊèêÂèñÂÄº\n",
        "            field_name: Â≠óÊÆµÂêçÁß∞\n",
        "            validation_type: È™åËØÅÁ±ªÂûã ('exact', 'similarity', 'numerical')\n",
        "\n",
        "        Returns:\n",
        "            È™åËØÅÁªìÊûúÂ≠óÂÖ∏\n",
        "        \"\"\"\n",
        "        result = {\n",
        "            'field_name': field_name,\n",
        "            'manual_value': manual_value,\n",
        "            'auto_value': auto_value,\n",
        "            'validation_type': validation_type,\n",
        "            'is_correct': False,\n",
        "            'score': 0.0,\n",
        "            'error_type': None,\n",
        "            'notes': \"\"\n",
        "        }\n",
        "\n",
        "        # Â§ÑÁêÜÁ©∫ÂÄºÊÉÖÂÜµ\n",
        "        manual_clean = str(manual_value).strip() if pd.notna(manual_value) else \"\"\n",
        "        auto_clean = str(auto_value).strip() if pd.notna(auto_value) else \"\"\n",
        "\n",
        "        if not manual_clean and not auto_clean:\n",
        "            result.update({'is_correct': True, 'score': 1.0, 'notes': 'Both values empty'})\n",
        "            return result\n",
        "        elif not manual_clean or not auto_clean:\n",
        "            result.update({'error_type': 'missing_value', 'notes': 'One value is missing'})\n",
        "            return result\n",
        "\n",
        "        # Ê†πÊçÆÈ™åËØÅÁ±ªÂûãËøõË°åÊØîËæÉ\n",
        "        if validation_type == 'exact':\n",
        "            is_match = manual_clean.lower() == auto_clean.lower()\n",
        "            result.update({\n",
        "                'is_correct': is_match,\n",
        "                'score': 1.0 if is_match else 0.0,\n",
        "                'error_type': None if is_match else 'exact_mismatch'\n",
        "            })\n",
        "\n",
        "        elif validation_type == 'similarity':\n",
        "            # ‰ΩøÁî®‰∏ªÈ™åËØÅÂô®ÁöÑÊñáÊú¨Áõ∏‰ººÂ∫¶ÁÆóÊ≥ï\n",
        "            similarity_scores = self.main_validator.calculate_text_similarity(manual_clean, auto_clean)\n",
        "            similarity = similarity_scores['combined']\n",
        "\n",
        "            # ÂØπ‰∫éÂÖÉÊï∞ÊçÆÔºå‰ΩøÁî®Êõ¥È´òÁöÑÁõ∏‰ººÂ∫¶ÈòàÂÄº\n",
        "            threshold = 0.8\n",
        "            is_match = similarity >= threshold\n",
        "\n",
        "            result.update({\n",
        "                'is_correct': is_match,\n",
        "                'score': similarity,\n",
        "                'similarity_details': similarity_scores,\n",
        "                'error_type': None if is_match else 'similarity_mismatch',\n",
        "                'notes': f'Similarity: {similarity:.3f}'\n",
        "            })\n",
        "\n",
        "        elif validation_type == 'numerical':\n",
        "            try:\n",
        "                # Ê†áÂáÜÂåñÊï∞ÂÄºÊ†ºÂºè\n",
        "                manual_num = self._normalize_numerical_value(manual_clean)\n",
        "                auto_num = self._normalize_numerical_value(auto_clean)\n",
        "\n",
        "                if manual_num is not None and auto_num is not None:\n",
        "                    # ÂÖÅËÆ∏Â∞èÂπÖÂ∫¶Â∑ÆÂºÇÔºàÈÄÇÂ∫îÊèêÂèñ‰∏≠ÁöÑËàçÂÖ•ËØØÂ∑ÆÔºâ\n",
        "                    tolerance = 0.01 if abs(manual_num) < 10 else abs(manual_num) * 0.001\n",
        "                    is_match = abs(manual_num - auto_num) <= tolerance\n",
        "\n",
        "                    result.update({\n",
        "                        'is_correct': is_match,\n",
        "                        'score': 1.0 if is_match else max(0.0, 1.0 - abs(manual_num - auto_num) / max(abs(manual_num), 1)),\n",
        "                        'error_type': None if is_match else 'numerical_mismatch',\n",
        "                        'manual_parsed': manual_num,\n",
        "                        'auto_parsed': auto_num,\n",
        "                        'notes': f'Manual: {manual_num}, Auto: {auto_num}'\n",
        "                    })\n",
        "                else:\n",
        "                    result.update({\n",
        "                        'error_type': 'numerical_parse_error',\n",
        "                        'notes': f'Failed to parse numerical values: \"{manual_clean}\" vs \"{auto_clean}\"'\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                result.update({\n",
        "                    'error_type': 'numerical_validation_error',\n",
        "                    'notes': f'Numerical validation failed: {str(e)}'\n",
        "                })\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _normalize_numerical_value(self, value_str: str) -> float:\n",
        "        \"\"\"Ê†áÂáÜÂåñÊï∞ÂÄºÂ≠óÁ¨¶‰∏≤‰∏∫float\"\"\"\n",
        "        if not value_str:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # ÁßªÈô§Â∏∏ËßÅÁöÑÈùûÊï∞Â≠óÂ≠óÁ¨¶Ôºå‰øùÁïôÊï∞Â≠ó„ÄÅÂ∞èÊï∞ÁÇπ„ÄÅË¥üÂè∑\n",
        "            cleaned = re.sub(r'[^\\d\\.\\-\\+]', '', str(value_str))\n",
        "            if cleaned:\n",
        "                return float(cleaned)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Â∞ùËØïÊèêÂèñÁ¨¨‰∏Ä‰∏™Êï∞Â≠ó\n",
        "        numbers = re.findall(r'-?\\d+\\.?\\d*', str(value_str))\n",
        "        if numbers:\n",
        "            try:\n",
        "                return float(numbers[0])\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return None\n",
        "\n",
        "    def validate_metadata_for_matched_pairs(self) -> Dict[str, any]:\n",
        "        \"\"\"\n",
        "        ÂØπÂ∑≤ÂåπÈÖçÁöÑKPIÂØπËøõË°åÂÖÉÊï∞ÊçÆÈ™åËØÅ\n",
        "\n",
        "        Returns:\n",
        "            ÂÖÉÊï∞ÊçÆÈ™åËØÅÁªìÊûú\n",
        "        \"\"\"\n",
        "        logging.info(\"ÂºÄÂßãÂÖÉÊï∞ÊçÆÈ™åËØÅ...\")\n",
        "\n",
        "        # Ëé∑Âèñ‰∏ªÈ™åËØÅÂô®ÁöÑÂåπÈÖçÁªìÊûú\n",
        "        if not hasattr(self.main_validator, 'validation_results') or not self.main_validator.validation_results:\n",
        "            logging.error(\"‰∏ªÈ™åËØÅÂô®Â∞öÊú™ËøêË°åÔºåËØ∑ÂÖàËøêË°åÊñáÊú¨È™åËØÅ\")\n",
        "            return {}\n",
        "\n",
        "        matched_pairs = self.main_validator.validation_results.get('detailed_matches', [])\n",
        "        if not matched_pairs:\n",
        "            logging.warning(\"Ê≤°ÊúâÊâæÂà∞ÂåπÈÖçÁöÑKPIÂØπÔºåÊó†Ê≥ïËøõË°åÂÖÉÊï∞ÊçÆÈ™åËØÅ\")\n",
        "            return {}\n",
        "\n",
        "        metadata_results = {\n",
        "            'total_pairs': len(matched_pairs),\n",
        "            'field_results': {},\n",
        "            'overall_scores': {},\n",
        "            'problematic_field_analysis': {},\n",
        "            'detailed_results': []\n",
        "        }\n",
        "\n",
        "        # ÂàùÂßãÂåñÂ≠óÊÆµÁªìÊûúÁªüËÆ°\n",
        "        all_fields = {}\n",
        "        for category, fields in self.metadata_fields.items():\n",
        "            all_fields.update(fields)\n",
        "\n",
        "        for field_name in all_fields.keys():\n",
        "            metadata_results['field_results'][field_name] = {\n",
        "                'total_comparisons': 0,\n",
        "                'correct_count': 0,\n",
        "                'total_score': 0.0,\n",
        "                'error_types': {},\n",
        "                'examples': {'correct': [], 'incorrect': []}\n",
        "            }\n",
        "\n",
        "        # ÂØπÊØè‰∏™ÂåπÈÖçÂØπËøõË°åÂÖÉÊï∞ÊçÆÈ™åËØÅ\n",
        "        for pair_idx, match_pair in enumerate(matched_pairs):\n",
        "            manual_idx = match_pair['manual_idx']\n",
        "            auto_idx = match_pair['auto_idx']\n",
        "            text_similarity = match_pair['similarity']\n",
        "\n",
        "            # Ëé∑ÂèñÂÆåÊï¥ÁöÑKPIËÆ∞ÂΩï\n",
        "            manual_kpi = self.main_validator.manual_df.iloc[manual_idx]\n",
        "            auto_kpi = self.main_validator.auto_df.iloc[auto_idx]\n",
        "\n",
        "            pair_results = {\n",
        "                'pair_index': pair_idx,\n",
        "                'manual_idx': manual_idx,\n",
        "                'auto_idx': auto_idx,\n",
        "                'text_similarity': text_similarity,\n",
        "                'manual_text': match_pair['manual_text'],\n",
        "                'auto_text': match_pair['auto_text'],\n",
        "                'field_validations': {},\n",
        "                'pair_metadata_score': 0.0\n",
        "            }\n",
        "\n",
        "            total_weight = 0.0\n",
        "            weighted_score = 0.0\n",
        "\n",
        "            # È™åËØÅÊØè‰∏™ÂÖÉÊï∞ÊçÆÂ≠óÊÆµ\n",
        "            for field_name, field_config in all_fields.items():\n",
        "                manual_value = manual_kpi.get(field_name, '')\n",
        "                auto_value = auto_kpi.get(field_name, '')\n",
        "\n",
        "                field_result = self.validate_single_field(\n",
        "                    manual_value, auto_value, field_name, field_config['type']\n",
        "                )\n",
        "\n",
        "                pair_results['field_validations'][field_name] = field_result\n",
        "\n",
        "                # Êõ¥Êñ∞Â≠óÊÆµÁªüËÆ°\n",
        "                field_stats = metadata_results['field_results'][field_name]\n",
        "                field_stats['total_comparisons'] += 1\n",
        "                field_stats['total_score'] += field_result['score']\n",
        "\n",
        "                if field_result['is_correct']:\n",
        "                    field_stats['correct_count'] += 1\n",
        "                    if len(field_stats['examples']['correct']) < 3:\n",
        "                        field_stats['examples']['correct'].append({\n",
        "                            'manual': manual_value,\n",
        "                            'auto': auto_value,\n",
        "                            'pair_idx': pair_idx\n",
        "                        })\n",
        "                else:\n",
        "                    error_type = field_result.get('error_type', 'unknown')\n",
        "                    field_stats['error_types'][error_type] = field_stats['error_types'].get(error_type, 0) + 1\n",
        "\n",
        "                    if len(field_stats['examples']['incorrect']) < 3:\n",
        "                        field_stats['examples']['incorrect'].append({\n",
        "                            'manual': manual_value,\n",
        "                            'auto': auto_value,\n",
        "                            'error_type': error_type,\n",
        "                            'pair_idx': pair_idx,\n",
        "                            'notes': field_result.get('notes', '')\n",
        "                        })\n",
        "\n",
        "                # ËÆ°ÁÆóÂä†ÊùÉÂàÜÊï∞\n",
        "                weight = field_config['weight']\n",
        "                weighted_score += field_result['score'] * weight\n",
        "                total_weight += weight\n",
        "\n",
        "            # ËÆ°ÁÆóËØ•ÂØπKPIÁöÑÂÖÉÊï∞ÊçÆÊÄªÂàÜ\n",
        "            if total_weight > 0:\n",
        "                pair_results['pair_metadata_score'] = weighted_score / total_weight\n",
        "\n",
        "            metadata_results['detailed_results'].append(pair_results)\n",
        "\n",
        "        # ËÆ°ÁÆóÊï¥‰ΩìÂàÜÊï∞\n",
        "        self._calculate_overall_metadata_scores(metadata_results, all_fields)\n",
        "\n",
        "        # ÂàÜÊûêÈóÆÈ¢òÂ≠óÊÆµ\n",
        "        self._analyze_problematic_fields(metadata_results)\n",
        "\n",
        "        self.metadata_results = metadata_results\n",
        "        logging.info(f\"ÂÖÉÊï∞ÊçÆÈ™åËØÅÂÆåÊàêÔºåÂÖ±È™åËØÅ {len(matched_pairs)} ÂØπKPI\")\n",
        "\n",
        "        return metadata_results\n",
        "\n",
        "    def _calculate_overall_metadata_scores(self, metadata_results: Dict, all_fields: Dict):\n",
        "        \"\"\"ËÆ°ÁÆóÊï¥‰ΩìÂÖÉÊï∞ÊçÆÂàÜÊï∞\"\"\"\n",
        "        overall_scores = metadata_results['overall_scores']\n",
        "\n",
        "        # ÊåâÁ±ªÂà´ËÆ°ÁÆóÂàÜÊï∞\n",
        "        for category, fields in self.metadata_fields.items():\n",
        "            category_score = 0.0\n",
        "            category_weight = 0.0\n",
        "\n",
        "            for field_name, field_config in fields.items():\n",
        "                field_stats = metadata_results['field_results'][field_name]\n",
        "                if field_stats['total_comparisons'] > 0:\n",
        "                    field_accuracy = field_stats['correct_count'] / field_stats['total_comparisons']\n",
        "                    field_avg_score = field_stats['total_score'] / field_stats['total_comparisons']\n",
        "\n",
        "                    # ‰ΩøÁî®Âπ≥ÂùáÂàÜÊï∞ËÄå‰∏çÊòØÁÆÄÂçïÁöÑÊ≠£Á°ÆÁéá\n",
        "                    category_score += field_avg_score * field_config['weight']\n",
        "                    category_weight += field_config['weight']\n",
        "\n",
        "            if category_weight > 0:\n",
        "                overall_scores[category] = category_score / category_weight\n",
        "            else:\n",
        "                overall_scores[category] = 0.0\n",
        "\n",
        "        # ËÆ°ÁÆóÊÄª‰ΩìÂÖÉÊï∞ÊçÆÂàÜÊï∞\n",
        "        total_score = 0.0\n",
        "        total_weight = 0.0\n",
        "\n",
        "        for field_name, field_config in all_fields.items():\n",
        "            field_stats = metadata_results['field_results'][field_name]\n",
        "            if field_stats['total_comparisons'] > 0:\n",
        "                field_avg_score = field_stats['total_score'] / field_stats['total_comparisons']\n",
        "                total_score += field_avg_score * field_config['weight']\n",
        "                total_weight += field_config['weight']\n",
        "\n",
        "        if total_weight > 0:\n",
        "            overall_scores['overall_metadata_score'] = total_score / total_weight\n",
        "        else:\n",
        "            overall_scores['overall_metadata_score'] = 0.0\n",
        "\n",
        "    def _analyze_problematic_fields(self, metadata_results: Dict):\n",
        "        \"\"\"ÂàÜÊûêÈóÆÈ¢òÂ≠óÊÆµÁöÑËØ¶ÁªÜÊÉÖÂÜµ\"\"\"\n",
        "        problematic_analysis = metadata_results['problematic_field_analysis']\n",
        "\n",
        "        for field_name, issue_type in self.problematic_fields.items():\n",
        "            if field_name in metadata_results['field_results']:\n",
        "                field_stats = metadata_results['field_results'][field_name]\n",
        "\n",
        "                if field_stats['total_comparisons'] > 0:\n",
        "                    accuracy = field_stats['correct_count'] / field_stats['total_comparisons']\n",
        "                    avg_score = field_stats['total_score'] / field_stats['total_comparisons']\n",
        "\n",
        "                    problematic_analysis[field_name] = {\n",
        "                        'issue_type': issue_type,\n",
        "                        'accuracy': accuracy,\n",
        "                        'average_score': avg_score,\n",
        "                        'total_errors': field_stats['total_comparisons'] - field_stats['correct_count'],\n",
        "                        'error_breakdown': field_stats['error_types'],\n",
        "                        'severity': self._assess_field_severity(accuracy, field_name),\n",
        "                        'improvement_priority': self._get_improvement_priority(field_name, accuracy)\n",
        "                    }\n",
        "\n",
        "    def _assess_field_severity(self, accuracy: float, field_name: str) -> str:\n",
        "        \"\"\"ËØÑ‰º∞Â≠óÊÆµÈóÆÈ¢òÁöÑ‰∏•ÈáçÁ®ãÂ∫¶\"\"\"\n",
        "        if accuracy >= 0.9:\n",
        "            return \"low\"\n",
        "        elif accuracy >= 0.7:\n",
        "            return \"medium\"\n",
        "        elif accuracy >= 0.5:\n",
        "            return \"high\"\n",
        "        else:\n",
        "            return \"critical\"\n",
        "\n",
        "    def _get_improvement_priority(self, field_name: str, accuracy: float) -> str:\n",
        "        \"\"\"Ëé∑ÂèñÊîπËøõ‰ºòÂÖàÁ∫ß\"\"\"\n",
        "        field_importance = {\n",
        "            'Absolute Page Number': 'medium',  # ÊÇ®ËØ¥‰ºöÂçáÁ∫ß\n",
        "            'kpi_theme': 'high',  # ÈáçË¶ÅÂàÜÁ±ªÂ≠óÊÆµ\n",
        "            'quantitative_value': 'critical'  # ÊúÄÈáçË¶ÅÁöÑÊï∞ÂÄºÂ≠óÊÆµ\n",
        "        }\n",
        "\n",
        "        importance = field_importance.get(field_name, 'low')\n",
        "\n",
        "        if accuracy < 0.5 and importance in ['high', 'critical']:\n",
        "            return 'urgent'\n",
        "        elif accuracy < 0.7 and importance == 'critical':\n",
        "            return 'high'\n",
        "        elif accuracy < 0.8 and importance in ['high', 'critical']:\n",
        "            return 'medium'\n",
        "        else:\n",
        "            return 'low'\n",
        "\n",
        "    def generate_metadata_report(self) -> str:\n",
        "        \"\"\"ÁîüÊàêÂÖÉÊï∞ÊçÆÈ™åËØÅÊä•Âëä\"\"\"\n",
        "        if not self.metadata_results:\n",
        "            return \"No metadata validation results available. Please run validate_metadata_for_matched_pairs() first.\"\n",
        "\n",
        "        results = self.metadata_results\n",
        "        overall_scores = results['overall_scores']\n",
        "\n",
        "        report = f\"\"\"\n",
        "# ÂÖÉÊï∞ÊçÆÈ™åËØÅÊä•Âëä (KPIÊñáÊú¨È™åËØÅÁã¨Á´ãÊä•Âëä)\n",
        "\n",
        "## È™åËØÅÊ¶ÇËßà\n",
        "- **È™åËØÅKPIÂØπÊï∞**: {results['total_pairs']}\n",
        "- **ÊÄª‰ΩìÂÖÉÊï∞ÊçÆÂàÜÊï∞**: {overall_scores.get('overall_metadata_score', 0):.3f}\n",
        "- **È™åËØÅÊó∂Èó¥**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "## ÂàÜÁ±ªÂæóÂàÜ\n",
        "\n",
        "### ÊñáÊ°£Áõ∏ÂÖ≥Â≠óÊÆµ ({overall_scores.get('document_fields', 0):.3f})\n",
        "\"\"\"\n",
        "\n",
        "        # Ê∑ªÂä†ÂêÑÁ±ªÂà´ËØ¶ÁªÜÂàÜÊï∞\n",
        "        for category, category_score in overall_scores.items():\n",
        "            if category.endswith('_fields'):\n",
        "                category_name = category.replace('_', ' ').title()\n",
        "                report += f\"- **{category_name}**: {category_score:.3f}\\n\"\n",
        "\n",
        "        report += \"\\n## Â≠óÊÆµËØ¶ÁªÜÂàÜÊûê\\n\\n\"\n",
        "        report += \"| Â≠óÊÆµÂêç | ÂáÜÁ°ÆÁéá | Âπ≥ÂùáÂàÜÊï∞ | ÈîôËØØÊï∞ | ‰∏ªË¶ÅÈîôËØØÁ±ªÂûã |\\n\"\n",
        "        report += \"|--------|--------|----------|--------|-------------|\\n\"\n",
        "\n",
        "        # ÊåâÂáÜÁ°ÆÁéáÊéíÂ∫èÊòæÁ§∫Â≠óÊÆµ\n",
        "        field_items = list(results['field_results'].items())\n",
        "        field_items.sort(key=lambda x: x[1]['correct_count'] / max(x[1]['total_comparisons'], 1))\n",
        "\n",
        "        for field_name, field_stats in field_items:\n",
        "            if field_stats['total_comparisons'] > 0:\n",
        "                accuracy = field_stats['correct_count'] / field_stats['total_comparisons']\n",
        "                avg_score = field_stats['total_score'] / field_stats['total_comparisons']\n",
        "                error_count = field_stats['total_comparisons'] - field_stats['correct_count']\n",
        "\n",
        "                main_error = max(field_stats['error_types'].items(),\n",
        "                               key=lambda x: x[1], default=('none', 0))[0]\n",
        "\n",
        "                report += f\"| {field_name} | {accuracy:.3f} | {avg_score:.3f} | {error_count} | {main_error} |\\n\"\n",
        "\n",
        "        # ÈóÆÈ¢òÂ≠óÊÆµÈáçÁÇπÂàÜÊûê\n",
        "        if results['problematic_field_analysis']:\n",
        "            report += \"\\n## ÈáçÁÇπÂÖ≥Ê≥®Â≠óÊÆµÂàÜÊûê\\n\\n\"\n",
        "\n",
        "            for field_name, analysis in results['problematic_field_analysis'].items():\n",
        "                report += f\"### {field_name} ({analysis['issue_type']})\\n\"\n",
        "                report += f\"- **ÂáÜÁ°ÆÁéá**: {analysis['accuracy']:.3f}\\n\"\n",
        "                report += f\"- **Âπ≥ÂùáÂàÜÊï∞**: {analysis['average_score']:.3f}\\n\"\n",
        "                report += f\"- **ÈîôËØØÊï∞**: {analysis['total_errors']}\\n\"\n",
        "                report += f\"- **‰∏•ÈáçÁ®ãÂ∫¶**: {analysis['severity']}\\n\"\n",
        "                report += f\"- **ÊîπËøõ‰ºòÂÖàÁ∫ß**: {analysis['improvement_priority']}\\n\"\n",
        "\n",
        "                if analysis['error_breakdown']:\n",
        "                    report += \"- **ÈîôËØØÁ±ªÂûãÂàÜÂ∏É**:\\n\"\n",
        "                    for error_type, count in analysis['error_breakdown'].items():\n",
        "                        report += f\"  - {error_type}: {count}\\n\"\n",
        "                report += \"\\n\"\n",
        "\n",
        "        # ÊîπËøõÂª∫ËÆÆ\n",
        "        report += \"## ÊîπËøõÂª∫ËÆÆ\\n\\n\"\n",
        "\n",
        "        critical_fields = [name for name, analysis in results['problematic_field_analysis'].items()\n",
        "                          if analysis['improvement_priority'] in ['urgent', 'high']]\n",
        "\n",
        "        if critical_fields:\n",
        "            report += \"### ‰ºòÂÖàÊîπËøõÂ≠óÊÆµ:\\n\"\n",
        "            for field in critical_fields:\n",
        "                analysis = results['problematic_field_analysis'][field]\n",
        "                report += f\"- **{field}**: {analysis['issue_type']} (‰ºòÂÖàÁ∫ß: {analysis['improvement_priority']})\\n\"\n",
        "\n",
        "        report += \"\\n### ÂÖ∑‰ΩìÂª∫ËÆÆ:\\n\"\n",
        "        if 'Absolute Page Number' in results['problematic_field_analysis']:\n",
        "            report += \"- **È°µÁ†ÅÊèêÂèñ**: ËÄÉËôëÊîπËøõPDFÈ°µÁ†ÅËØÜÂà´ÁÆóÊ≥ï\\n\"\n",
        "        if 'kpi_theme' in results['problematic_field_analysis']:\n",
        "            report += \"- **‰∏ªÈ¢òÂàÜÁ±ª**: ‰ºòÂåñKPI‰∏ªÈ¢òÂàÜÁ±ªÈÄªËæë\\n\"\n",
        "        if 'quantitative_value' in results['problematic_field_analysis']:\n",
        "            report += \"- **Êï∞ÂÄºÊèêÂèñ**: Âä†Âº∫Êï∞ÂÄºËØÜÂà´ÂíåÊ†áÂáÜÂåñÂ§ÑÁêÜ\\n\"\n",
        "\n",
        "        return report\n",
        "\n",
        "    def save_metadata_validation_results(self, output_dir: str = None):\n",
        "        \"\"\"‰øùÂ≠òÂÖÉÊï∞ÊçÆÈ™åËØÅÁªìÊûú\"\"\"\n",
        "        if not self.metadata_results:\n",
        "            logging.warning(\"No metadata results to save\")\n",
        "            return {}\n",
        "\n",
        "        if output_dir is None:\n",
        "            output_dir = self.main_validator.output_dir / \"metadata_validation\"\n",
        "\n",
        "        output_path = Path(output_dir)\n",
        "        output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        saved_files = {}\n",
        "\n",
        "        # 1. ‰øùÂ≠òËØ¶ÁªÜÁªìÊûúJSON\n",
        "        results_file = output_path / \"metadata_validation_results.json\"\n",
        "        with open(results_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.metadata_results, f, indent=2, ensure_ascii=False, default=str)\n",
        "        saved_files['detailed_json'] = results_file\n",
        "\n",
        "        # 2. ‰øùÂ≠òÂ≠óÊÆµÂàÜÊûêExcel\n",
        "        field_analysis_data = []\n",
        "        for field_name, field_stats in self.metadata_results['field_results'].items():\n",
        "            if field_stats['total_comparisons'] > 0:\n",
        "                accuracy = field_stats['correct_count'] / field_stats['total_comparisons']\n",
        "                avg_score = field_stats['total_score'] / field_stats['total_comparisons']\n",
        "\n",
        "                field_analysis_data.append({\n",
        "                    'Field Name': field_name,\n",
        "                    'Total Comparisons': field_stats['total_comparisons'],\n",
        "                    'Correct Count': field_stats['correct_count'],\n",
        "                    'Accuracy': accuracy,\n",
        "                    'Average Score': avg_score,\n",
        "                    'Error Count': field_stats['total_comparisons'] - field_stats['correct_count'],\n",
        "                    'Main Error Type': max(field_stats['error_types'].items(),\n",
        "                                         key=lambda x: x[1], default=('none', 0))[0]\n",
        "                })\n",
        "\n",
        "        field_df = pd.DataFrame(field_analysis_data)\n",
        "        field_df = field_df.sort_values('Accuracy')\n",
        "\n",
        "        excel_file = output_path / \"metadata_field_analysis.xlsx\"\n",
        "        with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n",
        "            field_df.to_excel(writer, sheet_name='Field Analysis', index=False)\n",
        "\n",
        "            # ÈóÆÈ¢òÂ≠óÊÆµËØ¶ÁªÜÂàÜÊûê\n",
        "            if self.metadata_results['problematic_field_analysis']:\n",
        "                prob_data = []\n",
        "                for field_name, analysis in self.metadata_results['problematic_field_analysis'].items():\n",
        "                    prob_data.append({\n",
        "                        'Field Name': field_name,\n",
        "                        'Issue Type': analysis['issue_type'],\n",
        "                        'Accuracy': analysis['accuracy'],\n",
        "                        'Average Score': analysis['average_score'],\n",
        "                        'Total Errors': analysis['total_errors'],\n",
        "                        'Severity': analysis['severity'],\n",
        "                        'Improvement Priority': analysis['improvement_priority']\n",
        "                    })\n",
        "\n",
        "                prob_df = pd.DataFrame(prob_data)\n",
        "                prob_df.to_excel(writer, sheet_name='Problematic Fields', index=False)\n",
        "\n",
        "        saved_files['field_analysis_excel'] = excel_file\n",
        "\n",
        "        # 3. ‰øùÂ≠òÂÖÉÊï∞ÊçÆÊä•Âëä\n",
        "        report = self.generate_metadata_report()\n",
        "        report_file = output_path / \"metadata_validation_report.md\"\n",
        "        with open(report_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(report)\n",
        "        saved_files['report_markdown'] = report_file\n",
        "\n",
        "        logging.info(f\"ÂÖÉÊï∞ÊçÆÈ™åËØÅÁªìÊûúÂ∑≤‰øùÂ≠òÂà∞: {output_path}\")\n",
        "        return saved_files\n",
        "\n",
        "# ÈõÜÊàêÂáΩÊï∞ÔºöÂú®‰∏ªÈ™åËØÅÊµÅÁ®ã‰∏≠Ê∑ªÂä†ÂÖÉÊï∞ÊçÆÈ™åËØÅ\n",
        "def run_complete_validation_with_metadata(df_auto: pd.DataFrame, manual_xlsx_path: str,\n",
        "                                         output_dir: str = \"complete_validation\") -> Dict[str, any]:\n",
        "    \"\"\"\n",
        "    ËøêË°åÂÆåÊï¥È™åËØÅÔºàÊñáÊú¨È™åËØÅ + ÂÖÉÊï∞ÊçÆÈ™åËØÅÔºâ\n",
        "\n",
        "    Args:\n",
        "        df_auto: Ëá™Âä®ÊèêÂèñÁöÑKPI DataFrame\n",
        "        manual_xlsx_path: ÊâãÂä®Ê†áÊ≥®Êñá‰ª∂Ë∑ØÂæÑ\n",
        "        output_dir: ËæìÂá∫ÁõÆÂΩï\n",
        "\n",
        "    Returns:\n",
        "        ÂÆåÊï¥È™åËØÅÁªìÊûú\n",
        "    \"\"\"\n",
        "    logging.info(\"ÂºÄÂßãËøêË°åÂÆåÊï¥È™åËØÅÔºàÊñáÊú¨ + ÂÖÉÊï∞ÊçÆÔºâ...\")\n",
        "\n",
        "    # Step 1: ËøêË°åÊñáÊú¨È™åËØÅ\n",
        "    print(\"üîç Step 1: ËøêË°åÊñáÊú¨È™åËØÅ...\")\n",
        "    text_validation_results = enhanced_compare_with_manual_kpis(\n",
        "        df_auto, manual_xlsx_path, f\"{output_dir}/text_validation\"\n",
        "    )\n",
        "\n",
        "    if not text_validation_results:\n",
        "        logging.error(\"ÊñáÊú¨È™åËØÅÂ§±Ë¥•ÔºåÊó†Ê≥ïÁªßÁª≠ÂÖÉÊï∞ÊçÆÈ™åËØÅ\")\n",
        "        return {}\n",
        "\n",
        "    # Step 2: ËøêË°åÂÖÉÊï∞ÊçÆÈ™åËØÅ\n",
        "    print(\"üìä Step 2: ËøêË°åÂÖÉÊï∞ÊçÆÈ™åËØÅ...\")\n",
        "\n",
        "    # Ëé∑ÂèñÊñáÊú¨È™åËØÅÂô®ÂÆû‰æã\n",
        "    temp_auto_path = Path(f\"{output_dir}/text_validation\") / \"temp_auto_kpis.xlsx\"\n",
        "    if not temp_auto_path.exists():\n",
        "        temp_auto_path = Path(output_dir) / \"temp_auto_kpis.xlsx\"\n",
        "        df_auto.to_excel(temp_auto_path, index=False)\n",
        "\n",
        "    main_validator = KPIValidationPipeline(\n",
        "        manual_excel_path=manual_xlsx_path,\n",
        "        auto_excel_path=str(temp_auto_path),\n",
        "        output_dir=f\"{output_dir}/text_validation\"\n",
        "    )\n",
        "\n",
        "    # Á°Æ‰øùÊñáÊú¨È™åËØÅÂ∑≤ËøêË°å\n",
        "    if not hasattr(main_validator, 'validation_results') or not main_validator.validation_results:\n",
        "        main_validator.run_comprehensive_evaluation()\n",
        "\n",
        "    # ÂàõÂª∫ÂÖÉÊï∞ÊçÆÈ™åËØÅÊâ©Â±ï\n",
        "    metadata_validator = MetadataValidationExtension(main_validator)\n",
        "    metadata_results = metadata_validator.validate_metadata_for_matched_pairs()\n",
        "\n",
        "    # ‰øùÂ≠òÂÖÉÊï∞ÊçÆÈ™åËØÅÁªìÊûú\n",
        "    metadata_saved_files = metadata_validator.save_metadata_validation_results(f\"{output_dir}/metadata_validation\")\n",
        "\n",
        "    # Step 3: ÁîüÊàêÁªºÂêàÊä•Âëä\n",
        "    print(\"üìã Step 3: ÁîüÊàêÁªºÂêàÊä•Âëä...\")\n",
        "    complete_results = {\n",
        "        'text_validation': text_validation_results,\n",
        "        'metadata_validation': metadata_results,\n",
        "        'saved_files': {\n",
        "            'text_validation_files': text_validation_results.get('saved_files', {}),\n",
        "            'metadata_validation_files': metadata_saved_files\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # ÊâìÂç∞ÊëòË¶Å\n",
        "    text_best = text_validation_results.get('best_metrics', {})\n",
        "    metadata_overall = metadata_results.get('overall_scores', {})\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"ÂÆåÊï¥È™åËØÅÁªìÊûúÊëòË¶Å\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"üìù ÊñáÊú¨È™åËØÅ F1 ÂàÜÊï∞: {text_best.get('f1_score', 0):.3f}\")\n",
        "    print(f\"üìä ÂÖÉÊï∞ÊçÆÊÄª‰ΩìÂàÜÊï∞: {metadata_overall.get('overall_metadata_score', 0):.3f}\")\n",
        "    print(f\"üéØ È™åËØÅKPIÂØπÊï∞: {metadata_results.get('total_pairs', 0)}\")\n",
        "\n",
        "    print(f\"\\nüìÇ ÂÖÉÊï∞ÊçÆÂàÜÁ±ªÂæóÂàÜ:\")\n",
        "    for category, score in metadata_overall.items():\n",
        "        if category.endswith('_fields'):\n",
        "            category_name = category.replace('_', ' ').title()\n",
        "            print(f\"   {category_name}: {score:.3f}\")\n",
        "\n",
        "    # ÊòæÁ§∫ÈóÆÈ¢òÂ≠óÊÆµ\n",
        "    if metadata_results.get('problematic_field_analysis'):\n",
        "        print(f\"\\n‚ö†Ô∏è  ÈúÄË¶ÅÂÖ≥Ê≥®ÁöÑÂ≠óÊÆµ:\")\n",
        "        for field_name, analysis in metadata_results['problematic_field_analysis'].items():\n",
        "            print(f\"   {field_name}: {analysis['accuracy']:.3f} (‰ºòÂÖàÁ∫ß: {analysis['improvement_priority']})\")\n",
        "\n",
        "    print(f\"\\nüìÅ ÂÆåÊï¥ÁªìÊûú‰øùÂ≠òÂú®: {output_dir}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂\n",
        "    if temp_auto_path.exists():\n",
        "        temp_auto_path.unlink()\n",
        "\n",
        "    return complete_results\n",
        "\n",
        "# ‰æøÊç∑‰ΩøÁî®ÂáΩÊï∞\n",
        "def add_metadata_validation_to_existing_pipeline():\n",
        "    \"\"\"‰∏∫Áé∞ÊúâÈ™åËØÅÊµÅÁ®ãÊ∑ªÂä†ÂÖÉÊï∞ÊçÆÈ™åËØÅÁöÑÁ§∫‰æãÂáΩÊï∞\"\"\"\n",
        "\n",
        "    def enhanced_main_with_metadata():\n",
        "        \"\"\"Â¢ûÂº∫ÁâàmainÂáΩÊï∞ÔºåÂåÖÂê´ÂÖÉÊï∞ÊçÆÈ™åËØÅ\"\"\"\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format=\"%(asctime)s - %(levelname)s: %(message)s\",\n",
        "            datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            if not os.path.exists(PDF_PATH):\n",
        "                logging.error(f\"PDF file not found: {PDF_PATH}\")\n",
        "                return\n",
        "\n",
        "            # ÂéüÊúâÁöÑKPIÊèêÂèñÊµÅÁ®ã\n",
        "            df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
        "            save_results(df_auto, EXPORT_AUTO_XLSX, PDF_PATH)\n",
        "            logging.info(f\"KPI extraction completed: {len(df_auto)} KPIs extracted\")\n",
        "\n",
        "            # Â¢ûÂº∫ÁöÑÈ™åËØÅÊµÅÁ®ãÔºàÊñáÊú¨ + ÂÖÉÊï∞ÊçÆÔºâ\n",
        "            if MANUAL_XLSX and Path(MANUAL_XLSX).exists():\n",
        "                print(\"\\nüîç Running complete validation (text + metadata)...\")\n",
        "                complete_validation_results = run_complete_validation_with_metadata(\n",
        "                    df_auto, MANUAL_XLSX, \"complete_validation\"\n",
        "                )\n",
        "\n",
        "                if complete_validation_results:\n",
        "                    print(\"‚úÖ Complete validation finished with detailed analysis!\")\n",
        "                    print(f\"üìÅ Detailed results saved to: complete_validation/\")\n",
        "                else:\n",
        "                    print(\"‚ö†Ô∏è Validation encountered issues\")\n",
        "            else:\n",
        "                logging.info(\"Manual KPI file not found, skipping validation.\")\n",
        "\n",
        "            # ÊòæÁ§∫ÊèêÂèñÊëòË¶ÅÔºà‰øùÊåÅÂéüÊúâÈÄªËæëÔºâ\n",
        "            if not df_auto.empty:\n",
        "                print(f\"\\n=== Extraction Summary ===\")\n",
        "                print(f\"Total KPIs extracted: {len(df_auto)}\")\n",
        "\n",
        "                if 'source_type' in df_auto.columns:\n",
        "                    source_counts = df_auto['source_type'].value_counts()\n",
        "                    print(f\"From text/tables: {source_counts.get('text', 0)}\")\n",
        "                    print(f\"From images/charts: {source_counts.get('image', 0)}\")\n",
        "\n",
        "                if 'kpi_theme' in df_auto.columns:\n",
        "                    theme_counts = df_auto['kpi_theme'].value_counts()\n",
        "                    print(f\"\\nKPI Distribution by Theme:\")\n",
        "                    for theme, count in theme_counts.items():\n",
        "                        print(f\"  {theme}: {count}\")\n",
        "            else:\n",
        "                print(\"\\nNo KPIs were extracted from the document.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error in enhanced main execution: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "    return enhanced_main_with_metadata\n"
      ],
      "metadata": {
        "id": "HfzJhUgy3OtK"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchKPIProcessor:\n",
        "    \"\"\"ÊâπÈáèKPIÂ§ÑÁêÜÂô® - ÊîØÊåÅÂ§ö‰∏™PDFÂíåManualÊñá‰ª∂\"\"\"\n",
        "\n",
        "    def __init__(self, base_output_dir: str = \"batch_kpi_results\"):\n",
        "        \"\"\"\n",
        "        ÂàùÂßãÂåñÊâπÈáèÂ§ÑÁêÜÂô®\n",
        "\n",
        "        Args:\n",
        "            base_output_dir: ÊâπÈáèÂ§ÑÁêÜÁªìÊûúÁöÑÂü∫Á°ÄÁõÆÂΩï\n",
        "        \"\"\"\n",
        "        self.base_output_dir = Path(base_output_dir)\n",
        "        self.base_output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Â≠òÂÇ®ÊâÄÊúâÊñá‰ª∂ÈÖçÂØπ\n",
        "        self.file_pairs = []\n",
        "        self.batch_results = []\n",
        "\n",
        "        # ÂàõÂª∫Êó∂Èó¥Êà≥Áî®‰∫éÊú¨Ê¨°ÊâπÈáèÂ§ÑÁêÜ\n",
        "        self.batch_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        self.current_batch_dir = self.base_output_dir / f\"batch_{self.batch_timestamp}\"\n",
        "        self.current_batch_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        logging.info(f\"ÊâπÈáèÂ§ÑÁêÜÂô®ÂàùÂßãÂåñÂÆåÊàêÔºåÁªìÊûú‰øùÂ≠òÂà∞: {self.current_batch_dir}\")\n",
        "\n",
        "    def add_file_pair(self, pdf_path: str, manual_path: str, document_name: str = None):\n",
        "        \"\"\"\n",
        "        Ê∑ªÂä†‰∏ÄÂØπPDFÂíåManualÊñá‰ª∂\n",
        "\n",
        "        Args:\n",
        "            pdf_path: PDFÊñá‰ª∂Ë∑ØÂæÑ\n",
        "            manual_path: ManualÊ†áÊ≥®Êñá‰ª∂Ë∑ØÂæÑ\n",
        "            document_name: ÊñáÊ°£ÂêçÁß∞ÔºàÂèØÈÄâÔºåÈªòËÆ§‰ΩøÁî®PDFÊñá‰ª∂ÂêçÔºâ\n",
        "        \"\"\"\n",
        "        pdf_path = Path(pdf_path)\n",
        "        manual_path = Path(manual_path)\n",
        "\n",
        "        # È™åËØÅÊñá‰ª∂Â≠òÂú®\n",
        "        if not pdf_path.exists():\n",
        "            logging.error(f\"PDFÊñá‰ª∂‰∏çÂ≠òÂú®: {pdf_path}\")\n",
        "            return False\n",
        "\n",
        "        if not manual_path.exists():\n",
        "            logging.error(f\"ManualÊñá‰ª∂‰∏çÂ≠òÂú®: {manual_path}\")\n",
        "            return False\n",
        "\n",
        "        # Ëá™Âä®ÁîüÊàêÊñáÊ°£ÂêçÁß∞\n",
        "        if document_name is None:\n",
        "            document_name = pdf_path.stem\n",
        "\n",
        "        file_pair = {\n",
        "            'pdf_path': str(pdf_path),\n",
        "            'manual_path': str(manual_path),\n",
        "            'document_name': document_name,\n",
        "            'doc_id': len(self.file_pairs) + 1\n",
        "        }\n",
        "\n",
        "        self.file_pairs.append(file_pair)\n",
        "        logging.info(f\"Ê∑ªÂä†Êñá‰ª∂ÂØπ {len(self.file_pairs)}: {document_name}\")\n",
        "        return True\n",
        "\n",
        "    def add_multiple_pairs_from_directory(self, pdf_dir: str, manual_dir: str,\n",
        "                                         pdf_pattern: str = \"*.pdf\",\n",
        "                                         manual_pattern: str = \"*.xlsx\"):\n",
        "        \"\"\"\n",
        "        ‰ªéÁõÆÂΩïÊâπÈáèÊ∑ªÂä†Êñá‰ª∂ÂØπÔºàÊåâÊñá‰ª∂ÂêçÂåπÈÖçÔºâ\n",
        "\n",
        "        Args:\n",
        "            pdf_dir: PDFÊñá‰ª∂ÁõÆÂΩï\n",
        "            manual_dir: ManualÊñá‰ª∂ÁõÆÂΩï\n",
        "            pdf_pattern: PDFÊñá‰ª∂ÂåπÈÖçÊ®°Âºè\n",
        "            manual_pattern: ManualÊñá‰ª∂ÂåπÈÖçÊ®°Âºè\n",
        "        \"\"\"\n",
        "        pdf_dir = Path(pdf_dir)\n",
        "        manual_dir = Path(manual_dir)\n",
        "\n",
        "        if not pdf_dir.exists() or not manual_dir.exists():\n",
        "            logging.error(f\"ÁõÆÂΩï‰∏çÂ≠òÂú®: {pdf_dir} Êàñ {manual_dir}\")\n",
        "            return 0\n",
        "\n",
        "        # Ëé∑ÂèñÊâÄÊúâPDFÊñá‰ª∂\n",
        "        pdf_files = list(pdf_dir.glob(pdf_pattern))\n",
        "        added_count = 0\n",
        "\n",
        "        for pdf_file in pdf_files:\n",
        "            # Â∞ùËØïÊâæÂà∞ÂØπÂ∫îÁöÑManualÊñá‰ª∂\n",
        "            base_name = pdf_file.stem\n",
        "\n",
        "            # Â∞ùËØïÂ§öÁßçÂåπÈÖçÊ®°Âºè\n",
        "            possible_manual_names = [\n",
        "                f\"{base_name}.xlsx\",\n",
        "                f\"{base_name}_manual.xlsx\",\n",
        "                f\"manual_{base_name}.xlsx\",\n",
        "                f\"{base_name}.xls\"\n",
        "            ]\n",
        "\n",
        "            manual_file = None\n",
        "            for manual_name in possible_manual_names:\n",
        "                potential_manual = manual_dir / manual_name\n",
        "                if potential_manual.exists():\n",
        "                    manual_file = potential_manual\n",
        "                    break\n",
        "\n",
        "            if manual_file:\n",
        "                if self.add_file_pair(str(pdf_file), str(manual_file), base_name):\n",
        "                    added_count += 1\n",
        "            else:\n",
        "                logging.warning(f\"Êú™ÊâæÂà∞ {base_name} ÂØπÂ∫îÁöÑManualÊñá‰ª∂\")\n",
        "\n",
        "        logging.info(f\"‰ªéÁõÆÂΩïÊâπÈáèÊ∑ªÂä†‰∫Ü {added_count} ‰∏™Êñá‰ª∂ÂØπ\")\n",
        "        return added_count\n",
        "\n",
        "    def list_file_pairs(self):\n",
        "        \"\"\"ÊòæÁ§∫ÊâÄÊúâÂ∑≤Ê∑ªÂä†ÁöÑÊñá‰ª∂ÂØπ\"\"\"\n",
        "        if not self.file_pairs:\n",
        "            print(\"‚ùå Ê≤°ÊúâÊ∑ªÂä†‰ªª‰ΩïÊñá‰ª∂ÂØπ\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\nüìã Â∑≤Ê∑ªÂä†ÁöÑÊñá‰ª∂ÂØπ (ÂÖ± {len(self.file_pairs)} ÂØπ):\")\n",
        "        print(\"-\" * 80)\n",
        "        for pair in self.file_pairs:\n",
        "            print(f\"{pair['doc_id']:2d}. ÊñáÊ°£: {pair['document_name']}\")\n",
        "            print(f\"    PDF:    {pair['pdf_path']}\")\n",
        "            print(f\"    Manual: {pair['manual_path']}\")\n",
        "            print()\n",
        "\n",
        "    def process_single_document(self, file_pair: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        Â§ÑÁêÜÂçï‰∏™ÊñáÊ°£ÔºàPDF + ManualÔºâ\n",
        "\n",
        "        Args:\n",
        "            file_pair: Êñá‰ª∂ÂØπ‰ø°ÊÅØ\n",
        "\n",
        "        Returns:\n",
        "            Â§ÑÁêÜÁªìÊûúÂ≠óÂÖ∏\n",
        "        \"\"\"\n",
        "        doc_name = file_pair['document_name']\n",
        "        pdf_path = file_pair['pdf_path']\n",
        "        manual_path = file_pair['manual_path']\n",
        "        doc_id = file_pair['doc_id']\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"üìÑ Â§ÑÁêÜÊñáÊ°£ {doc_id}/{len(self.file_pairs)}: {doc_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # ‰∏∫ÊØè‰∏™ÊñáÊ°£ÂàõÂª∫Áã¨Á´ãÁõÆÂΩï\n",
        "        doc_output_dir = self.current_batch_dir / f\"doc_{doc_id}_{doc_name}\"\n",
        "        doc_output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        start_time = time.time()\n",
        "        result = {\n",
        "            'doc_id': doc_id,\n",
        "            'document_name': doc_name,\n",
        "            'pdf_path': pdf_path,\n",
        "            'manual_path': manual_path,\n",
        "            'output_dir': str(doc_output_dir),\n",
        "            'start_time': datetime.now().isoformat(),\n",
        "            'status': 'processing'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Step 1: ‰∏¥Êó∂‰øÆÊîπÂÖ®Â±ÄPDFË∑ØÂæÑ\n",
        "            global PDF_PATH\n",
        "            original_pdf_path = PDF_PATH\n",
        "            PDF_PATH = pdf_path\n",
        "\n",
        "            print(f\"üìä Step 1: ÊèêÂèñKPI from {Path(pdf_path).name}...\")\n",
        "\n",
        "            # Step 2: ËøêË°åKPIÊèêÂèñ\n",
        "            df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
        "\n",
        "            # Step 3: ‰øùÂ≠òËá™Âä®ÊèêÂèñÁªìÊûú\n",
        "            auto_excel_path = doc_output_dir / f\"{doc_name}_auto_kpis.xlsx\"\n",
        "            save_results(df_auto, str(auto_excel_path), PDF_PATH)\n",
        "\n",
        "            print(f\"‚úÖ ÊèêÂèñÂÆåÊàê: {len(df_auto)} KPIs\")\n",
        "\n",
        "            # Step 4: ËøêË°åÈ™åËØÅ\n",
        "            print(f\"üîç Step 2: ËøêË°åÈ™åËØÅ against {Path(manual_path).name}...\")\n",
        "            validation_output_dir = doc_output_dir / \"validation\"\n",
        "            validation_results = enhanced_compare_with_manual_kpis(\n",
        "                df_auto, manual_path, str(validation_output_dir)\n",
        "            )\n",
        "\n",
        "            # Step 5: Êî∂ÈõÜÁªìÊûú\n",
        "            processing_time = time.time() - start_time\n",
        "\n",
        "            result.update({\n",
        "                'status': 'completed',\n",
        "                'processing_time_seconds': processing_time,\n",
        "                'extracted_kpis_count': len(df_auto),\n",
        "                'auto_excel_path': str(auto_excel_path),\n",
        "                'validation_output_dir': str(validation_output_dir),\n",
        "                'end_time': datetime.now().isoformat()\n",
        "            })\n",
        "\n",
        "            # Ê∑ªÂä†È™åËØÅÊåáÊ†á\n",
        "            if validation_results and 'best_metrics' in validation_results:\n",
        "                metrics = validation_results['best_metrics']\n",
        "                result.update({\n",
        "                    'validation_f1_score': metrics.get('f1_score', 0),\n",
        "                    'validation_precision': metrics.get('precision', 0),\n",
        "                    'validation_recall': metrics.get('recall', 0),\n",
        "                    'true_positives': metrics.get('true_positives', 0),\n",
        "                    'false_positives': metrics.get('false_positives', 0),\n",
        "                    'false_negatives': metrics.get('false_negatives', 0)\n",
        "                })\n",
        "\n",
        "                print(f\"üéØ È™åËØÅÂÆåÊàê:\")\n",
        "                print(f\"   F1 Score: {metrics.get('f1_score', 0):.3f}\")\n",
        "                print(f\"   Precision: {metrics.get('precision', 0):.3f}\")\n",
        "                print(f\"   Recall: {metrics.get('recall', 0):.3f}\")\n",
        "\n",
        "            print(f\"‚è±Ô∏è  Â§ÑÁêÜËÄóÊó∂: {processing_time:.1f}Áßí\")\n",
        "            print(f\"üìÅ ÁªìÊûú‰øùÂ≠òÂà∞: {doc_output_dir}\")\n",
        "\n",
        "            # ÊÅ¢Â§çÂéüÂßãPDFË∑ØÂæÑ\n",
        "            PDF_PATH = original_pdf_path\n",
        "\n",
        "        except Exception as e:\n",
        "            processing_time = time.time() - start_time\n",
        "            error_msg = str(e)\n",
        "\n",
        "            result.update({\n",
        "                'status': 'failed',\n",
        "                'processing_time_seconds': processing_time,\n",
        "                'error_message': error_msg,\n",
        "                'end_time': datetime.now().isoformat()\n",
        "            })\n",
        "\n",
        "            print(f\"‚ùå Â§ÑÁêÜÂ§±Ë¥•: {error_msg}\")\n",
        "\n",
        "            # ÊÅ¢Â§çÂéüÂßãPDFË∑ØÂæÑ\n",
        "            PDF_PATH = original_pdf_path\n",
        "\n",
        "            # ‰øùÂ≠òÈîôËØØÊó•Âøó\n",
        "            error_log_path = doc_output_dir / \"error_log.txt\"\n",
        "            with open(error_log_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(f\"ÊñáÊ°£: {doc_name}\\n\")\n",
        "                f.write(f\"ÈîôËØØÊó∂Èó¥: {datetime.now()}\\n\")\n",
        "                f.write(f\"ÈîôËØØ‰ø°ÊÅØ: {error_msg}\\n\")\n",
        "                f.write(f\"PDFË∑ØÂæÑ: {pdf_path}\\n\")\n",
        "                f.write(f\"ManualË∑ØÂæÑ: {manual_path}\\n\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def run_batch_processing(self, max_workers: int = 1):\n",
        "        \"\"\"\n",
        "        ËøêË°åÊâπÈáèÂ§ÑÁêÜ\n",
        "\n",
        "        Args:\n",
        "            max_workers: ÊúÄÂ§ßÂπ∂ÂèëÂ§ÑÁêÜÊï∞ÔºàÂª∫ËÆÆ‰øùÊåÅ‰∏∫1ÔºåÈÅøÂÖçAPIÈôêÂà∂Ôºâ\n",
        "        \"\"\"\n",
        "        if not self.file_pairs:\n",
        "            print(\"‚ùå Ê≤°ÊúâË¶ÅÂ§ÑÁêÜÁöÑÊñá‰ª∂ÂØπ\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\nüöÄ ÂºÄÂßãÊâπÈáèÂ§ÑÁêÜ {len(self.file_pairs)} ‰∏™ÊñáÊ°£...\")\n",
        "        print(f\"üìÅ ÁªìÊûúÂ∞Ü‰øùÂ≠òÂà∞: {self.current_batch_dir}\")\n",
        "\n",
        "        batch_start_time = time.time()\n",
        "\n",
        "        # Â§ÑÁêÜÊØè‰∏™ÊñáÊ°£\n",
        "        for file_pair in self.file_pairs:\n",
        "            result = self.process_single_document(file_pair)\n",
        "            self.batch_results.append(result)\n",
        "\n",
        "            # ÂÆûÊó∂‰øùÂ≠òËøõÂ∫¶ÔºàÈò≤Ê≠¢‰∏≠Êñ≠‰∏¢Â§±ÁªìÊûúÔºâ\n",
        "            self.save_batch_progress()\n",
        "\n",
        "        # ÁîüÊàêÊúÄÁªàÊä•Âëä\n",
        "        batch_total_time = time.time() - batch_start_time\n",
        "        self.generate_batch_summary(batch_total_time)\n",
        "\n",
        "        print(f\"\\nüéâ ÊâπÈáèÂ§ÑÁêÜÂÆåÊàê!\")\n",
        "        print(f\"‚è±Ô∏è  ÊÄªËÄóÊó∂: {batch_total_time:.1f}Áßí ({batch_total_time/60:.1f}ÂàÜÈíü)\")\n",
        "        print(f\"üìä Â§ÑÁêÜÁªüËÆ°: {self.get_batch_statistics()}\")\n",
        "        print(f\"üìÅ ÂÆåÊï¥ÁªìÊûúÊü•Áúã: {self.current_batch_dir}\")\n",
        "\n",
        "    def save_batch_progress(self):\n",
        "        \"\"\"‰øùÂ≠òÊâπÈáèÂ§ÑÁêÜËøõÂ∫¶\"\"\"\n",
        "        progress_file = self.current_batch_dir / \"batch_progress.json\"\n",
        "        with open(progress_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump({\n",
        "                'batch_timestamp': self.batch_timestamp,\n",
        "                'file_pairs': self.file_pairs,\n",
        "                'results': self.batch_results,\n",
        "                'last_updated': datetime.now().isoformat()\n",
        "            }, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    def get_batch_statistics(self) -> str:\n",
        "        \"\"\"Ëé∑ÂèñÊâπÈáèÂ§ÑÁêÜÁªüËÆ°‰ø°ÊÅØ\"\"\"\n",
        "        if not self.batch_results:\n",
        "            return \"Êó†ÁªìÊûú\"\n",
        "\n",
        "        total = len(self.batch_results)\n",
        "        completed = len([r for r in self.batch_results if r['status'] == 'completed'])\n",
        "        failed = len([r for r in self.batch_results if r['status'] == 'failed'])\n",
        "\n",
        "        # ËÆ°ÁÆóÂπ≥ÂùáÈ™åËØÅÊåáÊ†á\n",
        "        completed_results = [r for r in self.batch_results if r['status'] == 'completed']\n",
        "        if completed_results:\n",
        "            avg_f1 = sum(r.get('validation_f1_score', 0) for r in completed_results) / len(completed_results)\n",
        "            avg_precision = sum(r.get('validation_precision', 0) for r in completed_results) / len(completed_results)\n",
        "            avg_recall = sum(r.get('validation_recall', 0) for r in completed_results) / len(completed_results)\n",
        "            total_kpis = sum(r.get('extracted_kpis_count', 0) for r in completed_results)\n",
        "        else:\n",
        "            avg_f1 = avg_precision = avg_recall = total_kpis = 0\n",
        "\n",
        "        return f\"\"\"\n",
        "        ÊàêÂäü: {completed}/{total} ({completed/total*100:.1f}%)\n",
        "        Â§±Ë¥•: {failed}/{total} ({failed/total*100:.1f}%)\n",
        "        ÊÄªKPIÊï∞: {total_kpis}\n",
        "        Âπ≥ÂùáF1: {avg_f1:.3f}\n",
        "        Âπ≥ÂùáÁ≤æÁ°ÆÁéá: {avg_precision:.3f}\n",
        "        Âπ≥ÂùáÂè¨ÂõûÁéá: {avg_recall:.3f}\n",
        "        \"\"\"\n",
        "\n",
        "    def generate_batch_summary(self, total_time: float):\n",
        "        \"\"\"ÁîüÊàêÊâπÈáèÂ§ÑÁêÜÊ±áÊÄªÊä•Âëä\"\"\"\n",
        "        # 1. ‰øùÂ≠òËØ¶ÁªÜÁªìÊûúÂà∞Excel\n",
        "        results_df = pd.DataFrame(self.batch_results)\n",
        "        excel_path = self.current_batch_dir / \"batch_summary.xlsx\"\n",
        "\n",
        "        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
        "            # ‰∏ªË¶ÅÁªìÊûú\n",
        "            results_df.to_excel(writer, sheet_name='Â§ÑÁêÜÁªìÊûú', index=False)\n",
        "\n",
        "            # È™åËØÅÊåáÊ†áÊ±áÊÄª\n",
        "            if not results_df.empty:\n",
        "                completed_df = results_df[results_df['status'] == 'completed']\n",
        "                if not completed_df.empty:\n",
        "                    validation_columns = ['document_name', 'extracted_kpis_count',\n",
        "                                        'validation_f1_score', 'validation_precision',\n",
        "                                        'validation_recall', 'true_positives',\n",
        "                                        'false_positives', 'false_negatives']\n",
        "\n",
        "                    validation_df = completed_df[validation_columns].copy()\n",
        "                    validation_df.to_excel(writer, sheet_name='È™åËØÅÊåáÊ†á', index=False)\n",
        "\n",
        "        # 2. ÁîüÊàêMarkdownÊä•Âëä\n",
        "        report_path = self.current_batch_dir / \"batch_report.md\"\n",
        "        with open(report_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"\"\"# ÊâπÈáèKPIÊèêÂèñ‰∏éÈ™åËØÅÊä•Âëä\n",
        "\n",
        "## Â§ÑÁêÜÊ¶ÇËßà\n",
        "- **Â§ÑÁêÜÊó∂Èó¥**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "- **ÊâπÊ¨°ID**: {self.batch_timestamp}\n",
        "- **ÊÄªÊñáÊ°£Êï∞**: {len(self.file_pairs)}\n",
        "- **ÊÄªËÄóÊó∂**: {total_time:.1f}Áßí ({total_time/60:.1f}ÂàÜÈíü)\n",
        "\n",
        "## Â§ÑÁêÜÁªüËÆ°\n",
        "{self.get_batch_statistics()}\n",
        "\n",
        "## ËØ¶ÁªÜÁªìÊûú\n",
        "\n",
        "| ÊñáÊ°£ID | ÊñáÊ°£ÂêçÁß∞ | Áä∂ÊÄÅ | KPIÊï∞Èáè | F1ÂàÜÊï∞ | Á≤æÁ°ÆÁéá | Âè¨ÂõûÁéá | Â§ÑÁêÜÊó∂Èó¥(Áßí) |\n",
        "|--------|----------|------|---------|--------|--------|--------|-------------|\n",
        "\"\"\")\n",
        "\n",
        "            for result in self.batch_results:\n",
        "                f.write(f\"| {result['doc_id']} | {result['document_name']} | {result['status']} | \"\n",
        "                       f\"{result.get('extracted_kpis_count', 'N/A')} | \"\n",
        "                       f\"{result.get('validation_f1_score', 0):.3f} | \"\n",
        "                       f\"{result.get('validation_precision', 0):.3f} | \"\n",
        "                       f\"{result.get('validation_recall', 0):.3f} | \"\n",
        "                       f\"{result.get('processing_time_seconds', 0):.1f} |\\n\")\n",
        "\n",
        "            if any(r['status'] == 'failed' for r in self.batch_results):\n",
        "                f.write(f\"\\n## Â§±Ë¥•ÁöÑÊñáÊ°£\\n\")\n",
        "                for result in self.batch_results:\n",
        "                    if result['status'] == 'failed':\n",
        "                        f.write(f\"- **{result['document_name']}**: {result.get('error_message', 'Êú™Áü•ÈîôËØØ')}\\n\")\n",
        "\n",
        "        print(f\"üìã ÊâπÈáèÂ§ÑÁêÜÊä•ÂëäÁîüÊàê: {report_path}\")\n",
        "        print(f\"üìä ËØ¶ÁªÜÁªìÊûúExcel: {excel_path}\")"
      ],
      "metadata": {
        "id": "sJ5B5aGfIZsZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‰øÆÊîπÊâπÈáèÂ§ÑÁêÜÂô®ÔºåÊòæÁ§∫ËØ¶ÁªÜÁöÑÂçïÊñáÊ°£È™åËØÅ‰ø°ÊÅØ\n",
        "class BatchKPIProcessorWithDetailedOutput(BatchKPIProcessor):\n",
        "    \"\"\"\n",
        "    Êâ©Â±ïÊâπÈáèÂ§ÑÁêÜÂô®ÔºåÊîØÊåÅËØ¶ÁªÜÁöÑÈ™åËØÅËæìÂá∫\n",
        "    \"\"\"\n",
        "\n",
        "    def process_single_document(self, file_pair: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        Â§ÑÁêÜÂçï‰∏™ÊñáÊ°£ÔºåÊòæÁ§∫ËØ¶ÁªÜÁöÑÈ™åËØÅ‰ø°ÊÅØ\n",
        "        \"\"\"\n",
        "        doc_name = file_pair['document_name']\n",
        "        pdf_path = file_pair['pdf_path']\n",
        "        manual_path = file_pair['manual_path']\n",
        "        doc_id = file_pair['doc_id']\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"üìÑ Processing Document {doc_id}/{len(self.file_pairs)}: {doc_name}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        # ‰∏∫ÊØè‰∏™ÊñáÊ°£ÂàõÂª∫Áã¨Á´ãÁõÆÂΩï\n",
        "        doc_output_dir = self.current_batch_dir / f\"doc_{doc_id}_{doc_name}\"\n",
        "        doc_output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        start_time = time.time()\n",
        "        result = {\n",
        "            'doc_id': doc_id,\n",
        "            'document_name': doc_name,\n",
        "            'pdf_path': pdf_path,\n",
        "            'manual_path': manual_path,\n",
        "            'output_dir': str(doc_output_dir),\n",
        "            'start_time': datetime.now().isoformat(),\n",
        "            'status': 'processing'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Step 1: ‰∏¥Êó∂‰øÆÊîπÂÖ®Â±ÄPDFË∑ØÂæÑ\n",
        "            global PDF_PATH\n",
        "            original_pdf_path = PDF_PATH\n",
        "            PDF_PATH = pdf_path\n",
        "\n",
        "            print(f\"üìä Step 1: Extracting KPIs from {Path(pdf_path).name}...\")\n",
        "\n",
        "            # Step 2: ËøêË°åKPIÊèêÂèñ\n",
        "            df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
        "\n",
        "            # Step 3: ‰øùÂ≠òËá™Âä®ÊèêÂèñÁªìÊûú\n",
        "            auto_excel_path = doc_output_dir / f\"{doc_name}_auto_kpis.xlsx\"\n",
        "            save_results(df_auto, str(auto_excel_path), PDF_PATH)\n",
        "\n",
        "            print(f\"‚úÖ Extraction completed: {len(df_auto)} KPIs\")\n",
        "\n",
        "            # Step 4: ËøêË°åËØ¶ÁªÜÈ™åËØÅ\n",
        "            print(f\"\\nüîç Step 2: Running comprehensive validation against {Path(manual_path).name}...\")\n",
        "            validation_output_dir = doc_output_dir / \"validation\"\n",
        "            validation_start_time = time.time()\n",
        "\n",
        "            complete_validation_results = enhanced_compare_with_manual_kpis(\n",
        "                df_auto, manual_path, str(validation_output_dir)\n",
        "            )\n",
        "\n",
        "            validation_time = time.time() - validation_start_time\n",
        "\n",
        "            # Step 5: Êî∂ÈõÜÁªìÊûú\n",
        "            processing_time = time.time() - start_time\n",
        "\n",
        "            result.update({\n",
        "                'status': 'completed',\n",
        "                'processing_time_seconds': processing_time,\n",
        "                'validation_time_seconds': validation_time,\n",
        "                'extracted_kpis_count': len(df_auto),\n",
        "                'auto_excel_path': str(auto_excel_path),\n",
        "                'validation_output_dir': str(validation_output_dir),\n",
        "                'end_time': datetime.now().isoformat()\n",
        "            })\n",
        "\n",
        "            # Ê∑ªÂä†ËØ¶ÁªÜÁöÑÈ™åËØÅÊåáÊ†á\n",
        "            if complete_validation_results:\n",
        "                # ÊñáÊú¨È™åËØÅÊåáÊ†á\n",
        "                if 'text_validation' in complete_validation_results:\n",
        "                    text_metrics = complete_validation_results['text_validation'].get('best_metrics', {})\n",
        "                    result.update({\n",
        "                        'text_validation_f1_score': text_metrics.get('f1_score', 0),\n",
        "                        'text_validation_precision': text_metrics.get('precision', 0),\n",
        "                        'text_validation_recall': text_metrics.get('recall', 0),\n",
        "                        'true_positives': text_metrics.get('true_positives', 0),\n",
        "                        'false_positives': text_metrics.get('false_positives', 0),\n",
        "                        'false_negatives': text_metrics.get('false_negatives', 0),\n",
        "                        'manual_kpis_count': text_metrics.get('total_manual', 0),\n",
        "                        'auto_kpis_count': text_metrics.get('total_auto', 0)\n",
        "                    })\n",
        "\n",
        "                # ÂÖÉÊï∞ÊçÆÈ™åËØÅÊåáÊ†á\n",
        "                if 'metadata_validation' in complete_validation_results:\n",
        "                    metadata_scores = complete_validation_results['metadata_validation'].get('overall_scores', {})\n",
        "                    result.update({\n",
        "                        'metadata_overall_score': metadata_scores.get('overall_metadata_score', 0),\n",
        "                        'document_fields_score': metadata_scores.get('document_fields', 0),\n",
        "                        'classification_fields_score': metadata_scores.get('classification_fields', 0),\n",
        "                        'quantitative_fields_score': metadata_scores.get('quantitative_fields', 0),\n",
        "                        'analysis_fields_score': metadata_scores.get('analysis_fields', 0)\n",
        "                    })\n",
        "\n",
        "                    # Ê∑ªÂä†ÈóÆÈ¢òÂ≠óÊÆµÂàÜÊûê\n",
        "                    problematic_fields = complete_validation_results['metadata_validation'].get('problematic_field_analysis', {})\n",
        "                    for field_name in ['Absolute Page Number', 'kpi_theme', 'quantitative_value']:\n",
        "                        if field_name in problematic_fields:\n",
        "                            field_key = field_name.lower().replace(' ', '_')\n",
        "                            result[f'{field_key}_accuracy'] = problematic_fields[field_name]['accuracy']\n",
        "                            result[f'{field_key}_priority'] = problematic_fields[field_name]['improvement_priority']\n",
        "\n",
        "                # ÊòæÁ§∫ÊúÄÁªàÊëòË¶Å\n",
        "                print(f\"\\nüéØ Document {doc_id} Verification Completed:\")\n",
        "                print(f\"   üìä Dataset: {result.get('manual_kpis_count', 0)} manual vs {result.get('auto_kpis_count', 0)} auto KPIs\")\n",
        "                print(f\"   üéØ Text F1 Score: {result.get('text_validation_f1_score', 0):.3f}\")\n",
        "                print(f\"   üìà Text Precision: {result.get('text_validation_precision', 0):.3f}\")\n",
        "                print(f\"   üìâ Text Recall: {result.get('text_validation_recall', 0):.3f}\")\n",
        "                print(f\"   ‚úÖ True Positives: {result.get('true_positives', 0)}\")\n",
        "                print(f\"   ‚ùå False Positives: {result.get('false_positives', 0)}\")\n",
        "                print(f\"   ‚ö†Ô∏è  False Negatives: {result.get('false_negatives', 0)}\")\n",
        "                print(f\"   üìä Metadata Overall Score: {result.get('metadata_overall_score', 0):.3f}\")\n",
        "\n",
        "            print(f\"‚è±Ô∏è  Processing time: {processing_time:.1f}Áßí (validation: {validation_time:.1f}Áßí)\")\n",
        "            print(f\"üìÅ Results saved to: {doc_output_dir}\")\n",
        "\n",
        "            # ÊÅ¢Â§çÂéüÂßãPDFË∑ØÂæÑ\n",
        "            PDF_PATH = original_pdf_path\n",
        "\n",
        "        except Exception as e:\n",
        "            processing_time = time.time() - start_time\n",
        "            error_msg = str(e)\n",
        "\n",
        "            result.update({\n",
        "                'status': 'failed',\n",
        "                'processing_time_seconds': processing_time,\n",
        "                'error_message': error_msg,\n",
        "                'end_time': datetime.now().isoformat()\n",
        "            })\n",
        "\n",
        "            print(f\"‚ùå Processing failed: {error_msg}\")\n",
        "\n",
        "            # ÊÅ¢Â§çÂéüÂßãPDFË∑ØÂæÑ\n",
        "            PDF_PATH = original_pdf_path\n",
        "\n",
        "            # ‰øùÂ≠òÈîôËØØÊó•Âøó\n",
        "            error_log_path = doc_output_dir / \"error_log.txt\"\n",
        "            with open(error_log_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(f\"Document: {doc_name}\\n\")\n",
        "                f.write(f\"Error time: {datetime.now()}\\n\")\n",
        "                f.write(f\"Error message: {error_msg}\\n\")\n",
        "                f.write(f\"PDF Path: {pdf_path}\\n\")\n",
        "                f.write(f\"Manual Path: {manual_path}\\n\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def generate_batch_summary(self, total_time: float):\n",
        "        \"\"\"ÁîüÊàêËØ¶ÁªÜÁöÑÊâπÈáèÂ§ÑÁêÜÊ±áÊÄªÊä•Âëä\"\"\"\n",
        "        # Ë∞ÉÁî®Áà∂Á±ªÊñπÊ≥ïÁîüÊàêÂü∫Á°ÄÊä•Âëä\n",
        "        super().generate_batch_summary(total_time)\n",
        "\n",
        "        # ÁîüÊàêËØ¶ÁªÜÁöÑÊâπÈáèÊëòË¶Å\n",
        "        completed_results = [r for r in self.batch_results if r['status'] == 'completed']\n",
        "\n",
        "        if completed_results:\n",
        "            print(f\"\\n{'='*80}\")\n",
        "            print(\"BATCH PROCESSING DETAILED SUMMARY\")\n",
        "            print(f\"{'='*80}\")\n",
        "\n",
        "            # ËÆ°ÁÆóÊâπÈáèÁªüËÆ°\n",
        "            total_manual_kpis = sum(r.get('manual_kpis_count', 0) for r in completed_results)\n",
        "            total_auto_kpis = sum(r.get('auto_kpis_count', 0) for r in completed_results)\n",
        "            total_true_positives = sum(r.get('true_positives', 0) for r in completed_results)\n",
        "            total_false_positives = sum(r.get('false_positives', 0) for r in completed_results)\n",
        "            total_false_negatives = sum(r.get('false_negatives', 0) for r in completed_results)\n",
        "\n",
        "            # ËÆ°ÁÆóÂπ≥ÂùáÊåáÊ†á\n",
        "            avg_f1 = sum(r.get('text_validation_f1_score', 0) for r in completed_results) / len(completed_results)\n",
        "            avg_precision = sum(r.get('text_validation_precision', 0) for r in completed_results) / len(completed_results)\n",
        "            avg_recall = sum(r.get('text_validation_recall', 0) for r in completed_results) / len(completed_results)\n",
        "            avg_metadata_score = sum(r.get('metadata_overall_score', 0) for r in completed_results) / len(completed_results)\n",
        "\n",
        "            print(f\"üìä Batch Dataset Summary:\")\n",
        "            print(f\"   Total Documents Processed: {len(completed_results)}\")\n",
        "            print(f\"   Total Manual KPIs: {total_manual_kpis}\")\n",
        "            print(f\"   Total Auto KPIs: {total_auto_kpis}\")\n",
        "            print(f\"   Total True Positives: {total_true_positives}\")\n",
        "            print(f\"   Total False Positives: {total_false_positives}\")\n",
        "            print(f\"   Total False Negatives: {total_false_negatives}\")\n",
        "\n",
        "            print(f\"\\nüéØ Batch Average Performance:\")\n",
        "            print(f\"   Average Text F1 Score: {avg_f1:.3f}\")\n",
        "            print(f\"   Average Text Precision: {avg_precision:.3f}\")\n",
        "            print(f\"   Average Text Recall: {avg_recall:.3f}\")\n",
        "            print(f\"   Average Metadata Overall Score: {avg_metadata_score:.3f}\")\n",
        "\n",
        "            # ÊòæÁ§∫ÂêÑÊñáÊ°£Ë°®Áé∞\n",
        "            print(f\"\\nüìã Individual Document Performance:\")\n",
        "            print(\"=\"*80)\n",
        "\n",
        "            for result in completed_results:\n",
        "                doc_name = result['document_name'][:30]  # ÈôêÂà∂ÈïøÂ∫¶\n",
        "                f1 = result.get('text_validation_f1_score', 0)\n",
        "                precision = result.get('text_validation_precision', 0)\n",
        "                recall = result.get('text_validation_recall', 0)\n",
        "                metadata = result.get('metadata_overall_score', 0)\n",
        "                tp = result.get('true_positives', 0)\n",
        "                fp = result.get('false_positives', 0)\n",
        "                fn = result.get('false_negatives', 0)\n",
        "\n",
        "                print(f\"{doc_name:<30} | F1: {f1:.3f} | P: {precision:.3f} | R: {recall:.3f} | Meta: {metadata:.3f} | TP:{tp:2d} FP:{fp:2d} FN:{fn:2d}\")\n",
        "\n",
        "            print(\"=\"*80)\n",
        "            print(f\"üìÅ Detailed results saved to: {self.current_batch_dir}\")\n",
        "            print(\"=\"*80)\n",
        "\n",
        "# ÂàõÂª∫ËØ¶ÁªÜËæìÂá∫ÁöÑÊâπÈáèÂ§ÑÁêÜÂô®\n",
        "def create_batch_processor_with_detailed_output():\n",
        "    \"\"\"ÂàõÂª∫ÊîØÊåÅËØ¶ÁªÜËæìÂá∫ÁöÑÊâπÈáèÂ§ÑÁêÜÂô®\"\"\"\n",
        "    return BatchKPIProcessorWithDetailedOutput()\n",
        "\n",
        "# ‰øÆÊîπÈõÜÊàêÊâßË°åÂáΩÊï∞\n",
        "def integrated_main_execution_detailed():\n",
        "    \"\"\"ÈõÜÊàêÁöÑ‰∏ªÊâßË°åÂáΩÊï∞ - ËØ¶ÁªÜËæìÂá∫ÁâàÊú¨\"\"\"\n",
        "    print(\"üöÄ Enhanced KPI Extraction System - Detailed Output\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"1. Single PDF processing (detailed text + metadata validation)\")\n",
        "    print(\"2. Batch PDF processing (detailed text + metadata validation)\")\n",
        "    print(\"3. Quick batch from directories (detailed output)\")\n",
        "    print(\"4. Quick metadata validation test\")\n",
        "    print(\"5. View usage examples\")\n",
        "\n",
        "    try:\n",
        "        choice = input(\"Please select processing mode (1-5): \")\n",
        "\n",
        "        if choice == '1':\n",
        "            # Âçï‰∏™PDFÂ§ÑÁêÜÔºåËØ¶ÁªÜËæìÂá∫\n",
        "            enhanced_main_with_detailed_output()\n",
        "\n",
        "        elif choice == '2':\n",
        "            # ÊâπÈáèÂ§ÑÁêÜÔºåËØ¶ÁªÜËæìÂá∫\n",
        "            processor = create_batch_processor_with_detailed_output()\n",
        "\n",
        "            # ËÆ©Áî®Êà∑ÊâãÂä®Ê∑ªÂä†Êñá‰ª∂ÂØπ\n",
        "            while True:\n",
        "                pdf_path = input(\"Enter PDF file path (press Enter to finish): \").strip()\n",
        "                if not pdf_path:\n",
        "                    break\n",
        "                manual_path = input(\"Enter corresponding Manual file path: \").strip()\n",
        "                doc_name = input(\"Document name (press Enter for default): \").strip() or None\n",
        "\n",
        "                processor.add_file_pair(pdf_path, manual_path, doc_name)\n",
        "\n",
        "            if processor.file_pairs:\n",
        "                processor.list_file_pairs()\n",
        "                confirm = input(f\"\\nStart detailed processing of these {len(processor.file_pairs)} documents? (y/n): \")\n",
        "                if confirm.lower() == 'y':\n",
        "                    processor.run_batch_processing()\n",
        "                else:\n",
        "                    print(\"Batch processing cancelled.\")\n",
        "            else:\n",
        "                print(\"‚ùå No file pairs added.\")\n",
        "\n",
        "        elif choice == '3':\n",
        "            # Âø´ÈÄüÁõÆÂΩïÊâπÈáèÂ§ÑÁêÜÔºåËØ¶ÁªÜËæìÂá∫\n",
        "            pdf_dir = input(\"PDF files directory path: \").strip()\n",
        "            manual_dir = input(\"Manual files directory path: \").strip()\n",
        "\n",
        "            processor = create_batch_processor_with_detailed_output()\n",
        "            added_count = processor.add_multiple_pairs_from_directory(pdf_dir, manual_dir)\n",
        "\n",
        "            if added_count == 0:\n",
        "                print(\"‚ùå No matching PDF and Manual file pairs found.\")\n",
        "                return None\n",
        "\n",
        "            processor.list_file_pairs()\n",
        "            response = input(f\"\\nStart detailed processing of these {added_count} documents? (y/n): \")\n",
        "            if response.lower() == 'y':\n",
        "                processor.run_batch_processing()\n",
        "            else:\n",
        "                print(\"Batch processing cancelled.\")\n",
        "\n",
        "        elif choice == '4':\n",
        "            # Âø´ÈÄüÂÖÉÊï∞ÊçÆÈ™åËØÅÊµãËØï\n",
        "            quick_test_metadata_validation()\n",
        "\n",
        "        elif choice == '5':\n",
        "            # ÊòæÁ§∫‰ΩøÁî®Á§∫‰æã\n",
        "            metadata_validation_usage_examples()\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid selection, running default single PDF processing\")\n",
        "            enhanced_main_with_detailed_output()\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nUser cancelled operation\")\n",
        "    except Exception as e:\n",
        "        print(f\"Execution error: {e}\")\n",
        "        print(\"Running default single PDF processing\")\n",
        "        enhanced_main_with_detailed_output()"
      ],
      "metadata": {
        "id": "fP6Nh5cT_rRu"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FileUploadManager:\n",
        "    \"\"\"Êñá‰ª∂‰∏ä‰º†ÂíåÁÆ°ÁêÜÂô®\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.uploaded_files = []\n",
        "        self.pdf_files = []\n",
        "        self.manual_files = []\n",
        "        self.file_pairs = []\n",
        "\n",
        "        # ÂàõÂª∫Â∑•‰ΩúÁõÆÂΩï\n",
        "        self.work_dir = Path(\"/content/kpi_files\")\n",
        "        self.work_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        print(f\"üìÅ Â∑•‰ΩúÁõÆÂΩï: {self.work_dir}\")\n",
        "\n",
        "    def upload_files_directly(self):\n",
        "        \"\"\"Áõ¥Êé•‰∏ä‰º†Êñá‰ª∂Âà∞Colab\"\"\"\n",
        "        print(\"üì§ ËØ∑ÈÄâÊã©Ë¶Å‰∏ä‰º†ÁöÑÊñá‰ª∂...\")\n",
        "        print(\"ÂèØ‰ª•ÂêåÊó∂ÈÄâÊã©Â§ö‰∏™PDFÂíåExcelÊñá‰ª∂\")\n",
        "\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        for filename, content in uploaded.items():\n",
        "            file_path = self.work_dir / filename\n",
        "            with open(file_path, 'wb') as f:\n",
        "                f.write(content)\n",
        "\n",
        "            self.uploaded_files.append(str(file_path))\n",
        "            print(f\"‚úÖ Â∑≤‰∏ä‰º†: {filename}\")\n",
        "\n",
        "        self._categorize_files()\n",
        "        return len(uploaded)\n",
        "\n",
        "    def mount_google_drive(self):\n",
        "        \"\"\"ÊåÇËΩΩGoogle Drive\"\"\"\n",
        "        try:\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"‚úÖ Google DriveÂ∑≤ÊåÇËΩΩ\")\n",
        "            print(\"üìÅ ‰Ω†ÁöÑÊñá‰ª∂Âú®: /content/drive/MyDrive/\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå DriveÊåÇËΩΩÂ§±Ë¥•: {e}\")\n",
        "            return False\n",
        "\n",
        "    def scan_drive_directory(self, drive_path: str):\n",
        "        \"\"\"Êâ´ÊèèDriveÁõÆÂΩï‰∏≠ÁöÑÊñá‰ª∂\"\"\"\n",
        "        drive_path = Path(drive_path)\n",
        "\n",
        "        if not drive_path.exists():\n",
        "            print(f\"‚ùå ÁõÆÂΩï‰∏çÂ≠òÂú®: {drive_path}\")\n",
        "            return 0\n",
        "\n",
        "        # Êâ´ÊèèPDFÊñá‰ª∂\n",
        "        pdf_files = list(drive_path.glob(\"*.pdf\"))\n",
        "        excel_files = list(drive_path.glob(\"*.xlsx\")) + list(drive_path.glob(\"*.xls\"))\n",
        "\n",
        "        print(f\"üìä ÂèëÁé∞Êñá‰ª∂:\")\n",
        "        print(f\"   PDFÊñá‰ª∂: {len(pdf_files)}‰∏™\")\n",
        "        print(f\"   ExcelÊñá‰ª∂: {len(excel_files)}‰∏™\")\n",
        "\n",
        "        # Â§çÂà∂Âà∞Â∑•‰ΩúÁõÆÂΩï\n",
        "        for pdf_file in pdf_files:\n",
        "            dest = self.work_dir / pdf_file.name\n",
        "            shutil.copy2(pdf_file, dest)\n",
        "            self.uploaded_files.append(str(dest))\n",
        "            print(f\"üìÑ Â§çÂà∂PDF: {pdf_file.name}\")\n",
        "\n",
        "        for excel_file in excel_files:\n",
        "            dest = self.work_dir / excel_file.name\n",
        "            shutil.copy2(excel_file, dest)\n",
        "            self.uploaded_files.append(str(dest))\n",
        "            print(f\"üìä Â§çÂà∂Excel: {excel_file.name}\")\n",
        "\n",
        "        self._categorize_files()\n",
        "        return len(pdf_files) + len(excel_files)\n",
        "\n",
        "    def _categorize_files(self):\n",
        "        \"\"\"ÂàÜÁ±ªÊñá‰ª∂\"\"\"\n",
        "        self.pdf_files = []\n",
        "        self.manual_files = []\n",
        "\n",
        "        for file_path in self.uploaded_files:\n",
        "            path = Path(file_path)\n",
        "            if path.suffix.lower() == '.pdf':\n",
        "                self.pdf_files.append(file_path)\n",
        "            elif path.suffix.lower() in ['.xlsx', '.xls']:\n",
        "                self.manual_files.append(file_path)\n",
        "\n",
        "        print(f\"\\nüìã Êñá‰ª∂ÂàÜÁ±ªÂÆåÊàê:\")\n",
        "        print(f\"   PDFÊñá‰ª∂: {len(self.pdf_files)}‰∏™\")\n",
        "        print(f\"   ManualÊñá‰ª∂: {len(self.manual_files)}‰∏™\")\n",
        "\n",
        "    def auto_match_files(self) -> List[Tuple[str, str, str]]:\n",
        "        \"\"\"Ëá™Âä®ÂåπÈÖçPDFÂíåManualÊñá‰ª∂\"\"\"\n",
        "        matches = []\n",
        "\n",
        "        for pdf_path in self.pdf_files:\n",
        "            pdf_name = Path(pdf_path).stem\n",
        "\n",
        "            # Â∞ùËØïÂ§öÁßçÂåπÈÖçÊ®°Âºè\n",
        "            potential_matches = []\n",
        "\n",
        "            for manual_path in self.manual_files:\n",
        "                manual_name = Path(manual_path).stem\n",
        "\n",
        "                # ÂåπÈÖçÊ®°Âºè1: ÂÆåÂÖ®Áõ∏Âêå\n",
        "                if pdf_name.lower() == manual_name.lower():\n",
        "                    potential_matches.append((manual_path, 1.0, \"ÂÆåÂÖ®ÂåπÈÖç\"))\n",
        "\n",
        "                # ÂåπÈÖçÊ®°Âºè2: PDFÂêçÁß∞_manual\n",
        "                elif manual_name.lower() == f\"{pdf_name.lower()}_manual\":\n",
        "                    potential_matches.append((manual_path, 0.9, \"ÂêéÁºÄÂåπÈÖç\"))\n",
        "\n",
        "                # ÂåπÈÖçÊ®°Âºè3: manual_PDFÂêçÁß∞\n",
        "                elif manual_name.lower() == f\"manual_{pdf_name.lower()}\":\n",
        "                    potential_matches.append((manual_path, 0.9, \"ÂâçÁºÄÂåπÈÖç\"))\n",
        "\n",
        "                # ÂåπÈÖçÊ®°Âºè4: ÂåÖÂê´ÂÖ≥Á≥ª\n",
        "                elif pdf_name.lower() in manual_name.lower() or manual_name.lower() in pdf_name.lower():\n",
        "                    potential_matches.append((manual_path, 0.7, \"ÈÉ®ÂàÜÂåπÈÖç\"))\n",
        "\n",
        "            # ÈÄâÊã©ÊúÄ‰Ω≥ÂåπÈÖç\n",
        "            if potential_matches:\n",
        "                best_match = max(potential_matches, key=lambda x: x[1])\n",
        "                matches.append((pdf_path, best_match[0], pdf_name))\n",
        "                print(f\"‚úÖ ÂåπÈÖç: {pdf_name} ‚Üí {Path(best_match[0]).name} ({best_match[2]})\")\n",
        "            else:\n",
        "                print(f\"‚ùå Êú™ÊâæÂà∞ÂåπÈÖç: {pdf_name}\")\n",
        "\n",
        "        self.file_pairs = matches\n",
        "        return matches\n",
        "\n",
        "    def manual_pair_files(self):\n",
        "        \"\"\"ÊâãÂä®ÈÖçÂØπÊñá‰ª∂\"\"\"\n",
        "        print(\"\\nüîß ÊâãÂä®Êñá‰ª∂ÈÖçÂØπ\")\n",
        "        print(\"=\" * 40)\n",
        "\n",
        "        self.file_pairs = []\n",
        "\n",
        "        print(\"ÂèØÁî®ÁöÑPDFÊñá‰ª∂:\")\n",
        "        for i, pdf_path in enumerate(self.pdf_files, 1):\n",
        "            print(f\"  {i}. {Path(pdf_path).name}\")\n",
        "\n",
        "        print(\"\\nÂèØÁî®ÁöÑManualÊñá‰ª∂:\")\n",
        "        for i, manual_path in enumerate(self.manual_files, 1):\n",
        "            print(f\"  {i}. {Path(manual_path).name}\")\n",
        "\n",
        "        for pdf_path in self.pdf_files:\n",
        "            pdf_name = Path(pdf_path).name\n",
        "            print(f\"\\n‰∏∫PDFÊñá‰ª∂ '{pdf_name}' ÈÄâÊã©ManualÊñá‰ª∂:\")\n",
        "\n",
        "            for i, manual_path in enumerate(self.manual_files, 1):\n",
        "                print(f\"  {i}. {Path(manual_path).name}\")\n",
        "\n",
        "            try:\n",
        "                choice = int(input(\"ËØ∑ËæìÂÖ•ManualÊñá‰ª∂ÁºñÂè∑ (0Ë∑≥Ëøá): \"))\n",
        "                if choice > 0 and choice <= len(self.manual_files):\n",
        "                    manual_path = self.manual_files[choice - 1]\n",
        "                    doc_name = Path(pdf_path).stem\n",
        "                    self.file_pairs.append((pdf_path, manual_path, doc_name))\n",
        "                    print(f\"‚úÖ ÈÖçÂØπÊàêÂäü: {pdf_name} ‚Üí {Path(manual_path).name}\")\n",
        "                else:\n",
        "                    print(f\"‚è≠Ô∏è Ë∑≥Ëøá: {pdf_name}\")\n",
        "            except ValueError:\n",
        "                print(f\"‚è≠Ô∏è ËæìÂÖ•Êó†ÊïàÔºåË∑≥Ëøá: {pdf_name}\")\n",
        "\n",
        "    def validate_manual_files(self) -> Dict[str, bool]:\n",
        "        \"\"\"È™åËØÅManualÊñá‰ª∂Ê†ºÂºè\"\"\"\n",
        "        validation_results = {}\n",
        "\n",
        "        print(\"\\nüîç È™åËØÅManualÊñá‰ª∂Ê†ºÂºè...\")\n",
        "\n",
        "        for manual_path in self.manual_files:\n",
        "            file_name = Path(manual_path).name\n",
        "            try:\n",
        "                df = pd.read_excel(manual_path)\n",
        "\n",
        "                # Ê£ÄÊü•ÂøÖÈúÄÂàó\n",
        "                required_columns = ['kpi_text']\n",
        "                missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "\n",
        "                if missing_columns:\n",
        "                    print(f\"‚ùå {file_name}: Áº∫Â∞ëÂàó {missing_columns}\")\n",
        "                    validation_results[manual_path] = False\n",
        "                else:\n",
        "                    # Ê£ÄÊü•Êï∞ÊçÆ\n",
        "                    non_empty_rows = df['kpi_text'].notna().sum()\n",
        "                    print(f\"‚úÖ {file_name}: {len(df)}Ë°åÊï∞ÊçÆ, {non_empty_rows}‰∏™ÊúâÊïàKPI\")\n",
        "                    validation_results[manual_path] = True\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå {file_name}: ËØªÂèñÂ§±Ë¥• - {e}\")\n",
        "                validation_results[manual_path] = False\n",
        "\n",
        "        return validation_results\n",
        "\n",
        "    def show_file_summary(self):\n",
        "        \"\"\"ÊòæÁ§∫Êñá‰ª∂Ê±áÊÄª\"\"\"\n",
        "        print(f\"\\nüìä Êñá‰ª∂‰∏ä‰º†Ê±áÊÄª\")\n",
        "        print(\"=\" * 40)\n",
        "        print(f\"ÊÄªÊñá‰ª∂Êï∞: {len(self.uploaded_files)}\")\n",
        "        print(f\"PDFÊñá‰ª∂: {len(self.pdf_files)}\")\n",
        "        print(f\"ManualÊñá‰ª∂: {len(self.manual_files)}\")\n",
        "        print(f\"ÈÖçÂØπÊñá‰ª∂: {len(self.file_pairs)}\")\n",
        "\n",
        "        if self.file_pairs:\n",
        "            print(f\"\\nüìã ÈÖçÂØπÁªìÊûú:\")\n",
        "            for i, (pdf_path, manual_path, doc_name) in enumerate(self.file_pairs, 1):\n",
        "                print(f\"  {i}. {doc_name}\")\n",
        "                print(f\"     PDF: {Path(pdf_path).name}\")\n",
        "                print(f\"     Manual: {Path(manual_path).name}\")\n",
        "\n",
        "    def get_file_pairs_for_batch_processing(self) -> List[Tuple[str, str, str]]:\n",
        "        \"\"\"Ëé∑ÂèñÁî®‰∫éÊâπÈáèÂ§ÑÁêÜÁöÑÊñá‰ª∂ÂØπ\"\"\"\n",
        "        return self.file_pairs\n",
        "\n",
        "    def create_batch_processor_from_uploads(self):\n",
        "        \"\"\"‰ªé‰∏ä‰º†ÁöÑÊñá‰ª∂ÂàõÂª∫ÊâπÈáèÂ§ÑÁêÜÂô®\"\"\"\n",
        "        if not self.file_pairs:\n",
        "            print(\"‚ùå Ê≤°ÊúâÂèØÁî®ÁöÑÊñá‰ª∂ÂØπ\")\n",
        "            return None\n",
        "\n",
        "        # ÂØºÂÖ•ÊâπÈáèÂ§ÑÁêÜÂô®\n",
        "        from your_main_script import create_batch_processor  # ÈúÄË¶ÅÊõøÊç¢‰∏∫ÂÆûÈôÖÁöÑÂØºÂÖ•\n",
        "\n",
        "        processor = create_batch_processor()\n",
        "\n",
        "        for pdf_path, manual_path, doc_name in self.file_pairs:\n",
        "            processor.add_file_pair(pdf_path, manual_path, doc_name)\n",
        "\n",
        "        return processor"
      ],
      "metadata": {
        "id": "InVqb0qIB7a0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ ‰æøÊç∑‰ΩøÁî®ÂáΩÊï∞ ============\n",
        "\n",
        "def create_batch_processor():\n",
        "    \"\"\"A convenience function to create a batch KPI processor\"\"\"\n",
        "    return BatchKPIProcessor()\n",
        "\n",
        "def quick_batch_from_directories(pdf_dir: str, manual_dir: str):\n",
        "    \"\"\"Quickly create a batch process from two directories\"\"\"\n",
        "    processor = BatchKPIProcessor()\n",
        "\n",
        "    # Ê∑ªÂä†Êñá‰ª∂ÂØπ\n",
        "    added_count = processor.add_multiple_pairs_from_directory(pdf_dir, manual_dir)\n",
        "\n",
        "    if added_count == 0:\n",
        "        print(\"‚ùå No matching PDF and Manual file pairs found.\")\n",
        "        return None\n",
        "\n",
        "    # ÊòæÁ§∫Êñá‰ª∂ÂàóË°®\n",
        "    processor.list_file_pairs()\n",
        "\n",
        "    # ËØ¢ÈóÆÊòØÂê¶ÁªßÁª≠\n",
        "    response = input(f\"\\nStart processing these {added_count} documents? (y/n): \")\n",
        "    if response.lower() == 'y':\n",
        "        processor.run_batch_processing()\n",
        "        return processor\n",
        "    else:\n",
        "        print(\"Batch processing cancelled.\")\n",
        "        return processor\n",
        "\n",
        "def manual_batch_setup():\n",
        "    \"\"\"Interactive function for manually configuring a batch process\"\"\"\n",
        "    processor = BatchKPIProcessor()\n",
        "\n",
        "    print(\"üìã Manual Batch Processing Setup\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    while True:\n",
        "        print(f\"\\nCurrently added {len(processor.file_pairs)} file pair(s)\")\n",
        "        print(\"1. Add a single file pair\")\n",
        "        print(\"2. Add multiple file pairs from directories\")\n",
        "        print(\"3. View added file pairs\")\n",
        "        print(\"4. Start batch processing\")\n",
        "        print(\"5. Exit\")\n",
        "\n",
        "        choice = input(\"Select an option (1-5): \")\n",
        "\n",
        "        if choice == '1':\n",
        "            pdf_path = input(\"PDF file path: \")\n",
        "            manual_path = input(\"Manual file path: \")\n",
        "            doc_name = input(\"Document name (press Enter for default): \").strip()\n",
        "\n",
        "            if not doc_name:\n",
        "                doc_name = None\n",
        "\n",
        "            processor.add_file_pair(pdf_path, manual_path, doc_name)\n",
        "\n",
        "        elif choice == '2':\n",
        "            pdf_dir = input(\"PDF directory: \")\n",
        "            manual_dir = input(\"Manual directory: \")\n",
        "            added = processor.add_multiple_pairs_from_directory(pdf_dir, manual_dir)\n",
        "            print(f\"{added} file pair(s) added.\")\n",
        "\n",
        "        elif choice == '3':\n",
        "            processor.list_file_pairs()\n",
        "\n",
        "        elif choice == '4':\n",
        "            if processor.file_pairs:\n",
        "                processor.run_batch_processing()\n",
        "                break\n",
        "            else:\n",
        "                print(\"‚ùå No file pairs added.\")\n",
        "\n",
        "        elif choice == '5':\n",
        "            print(\"Exiting batch setup.\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Invalid selection. Please try again.\")\n",
        "\n",
        "    return processor"
      ],
      "metadata": {
        "id": "Mc6cffCKCHDk"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced comparison function for the main code\n",
        "def enhanced_compare_with_manual_kpis(df_auto: pd.DataFrame, manual_xlsx_path: str,\n",
        "                                     output_dir: str = \"validation_results\") -> Dict[str, any]:\n",
        "    \"\"\"\n",
        "    Â¢ûÂº∫ÁâàÊØîËæÉÂáΩÊï∞ - ÊòæÁ§∫ËØ¶ÁªÜÁöÑÊñáÊú¨È™åËØÅÁªìÊûú + ÂÖÉÊï∞ÊçÆÈ™åËØÅ\n",
        "    \"\"\"\n",
        "    if not Path(manual_xlsx_path).exists():\n",
        "        logging.warning(f\"Manual KPI file not found: {manual_xlsx_path}\")\n",
        "        return {}\n",
        "\n",
        "    # Save auto KPIs to temporary file for validation pipeline\n",
        "    temp_auto_path = Path(output_dir) / \"temp_auto_kpis.xlsx\"\n",
        "    temp_auto_path.parent.mkdir(exist_ok=True)\n",
        "    df_auto.to_excel(temp_auto_path, index=False)\n",
        "\n",
        "    try:\n",
        "        # Step 1: ËøêË°åËØ¶ÁªÜÁöÑÊñáÊú¨È™åËØÅ\n",
        "        print(f\"\\nüîç Running detailed text validation against {Path(manual_xlsx_path).name}...\")\n",
        "\n",
        "        validator = KPIValidationPipeline(\n",
        "            manual_excel_path=manual_xlsx_path,\n",
        "            auto_excel_path=str(temp_auto_path),\n",
        "            output_dir=output_dir\n",
        "        )\n",
        "\n",
        "        # Run full validation\n",
        "        text_validation_results = validator.run_full_validation()\n",
        "\n",
        "        # ÊòæÁ§∫ËØ¶ÁªÜÁöÑÊñáÊú¨È™åËØÅÊëòË¶Å\n",
        "        if text_validation_results and 'best_metrics' in text_validation_results:\n",
        "            best_metrics = text_validation_results['best_metrics']\n",
        "\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"KPI EXTRACTION VALIDATION SUMMARY\")\n",
        "            print(\"=\"*60)\n",
        "            print(f\"üìä Dataset: {len(validator.manual_df)} manual vs {len(validator.auto_df)} auto KPIs\")\n",
        "            print(f\"üéØ Best F1 Score: {best_metrics['f1_score']:.3f}\")\n",
        "            print(f\"üìà Precision: {best_metrics['precision']:.3f}\")\n",
        "            print(f\"üìâ Recall: {best_metrics['recall']:.3f}\")\n",
        "            print(f\"‚úÖ True Positives: {best_metrics['true_positives']}\")\n",
        "            print(f\"‚ùå False Positives: {best_metrics['false_positives']}\")\n",
        "            print(f\"‚ö†Ô∏è  False Negatives: {best_metrics['false_negatives']}\")\n",
        "            print(\"=\"*60)\n",
        "            print(f\"üìÅ Results saved to: {output_dir}\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "        # Step 2: ËøêË°åÂÖÉÊï∞ÊçÆÈ™åËØÅ\n",
        "        print(f\"\\nüìä Running metadata validation...\")\n",
        "\n",
        "        metadata_validator = MetadataValidationExtension(validator)\n",
        "        metadata_results = metadata_validator.validate_metadata_for_matched_pairs()\n",
        "\n",
        "        # ‰øùÂ≠òÂÖÉÊï∞ÊçÆÈ™åËØÅÁªìÊûú\n",
        "        metadata_saved_files = metadata_validator.save_metadata_validation_results(f\"{output_dir}/metadata_validation\")\n",
        "\n",
        "        # ÊòæÁ§∫ÂÖÉÊï∞ÊçÆÈ™åËØÅÊëòË¶Å\n",
        "        if metadata_results:\n",
        "            overall_scores = metadata_results.get('overall_scores', {})\n",
        "\n",
        "            print(f\"\\nüìã METADATA VALIDATION SUMMARY\")\n",
        "            print(\"=\"*60)\n",
        "            print(f\"üìä Metadata Overall Score: {overall_scores.get('overall_metadata_score', 0):.3f}\")\n",
        "            print(f\"üéØ Validated KPI Pairs: {metadata_results.get('total_pairs', 0)}\")\n",
        "\n",
        "            print(f\"\\nüìÇ Metadata Category Scores:\")\n",
        "            for category, score in overall_scores.items():\n",
        "                if category.endswith('_fields'):\n",
        "                    category_name = category.replace('_', ' ').title()\n",
        "                    print(f\"   {category_name}: {score:.3f}\")\n",
        "\n",
        "            # ÊòæÁ§∫ÈóÆÈ¢òÂ≠óÊÆµ\n",
        "            if metadata_results.get('problematic_field_analysis'):\n",
        "                print(f\"\\n‚ö†Ô∏è  Fields Needing Attention:\")\n",
        "                for field_name, analysis in metadata_results['problematic_field_analysis'].items():\n",
        "                    priority_emoji = \"üî¥\" if analysis['improvement_priority'] == 'high' else \"üü°\" if analysis['improvement_priority'] == 'medium' else \"üü¢\"\n",
        "                    print(f\"   {priority_emoji} {field_name}: {analysis['accuracy']:.3f} ({analysis['improvement_priority']} priority)\")\n",
        "\n",
        "            print(\"=\"*60)\n",
        "            print(f\"üìÅ Metadata results saved to: {output_dir}/metadata_validation\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "        # ÁªÑÂêàÁªìÊûú\n",
        "        complete_results = {\n",
        "            'text_validation': text_validation_results,\n",
        "            'metadata_validation': metadata_results,\n",
        "            'saved_files': {\n",
        "                'text_validation_files': text_validation_results.get('saved_files', {}),\n",
        "                'metadata_validation_files': metadata_saved_files\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Clean up temporary file\n",
        "        if temp_auto_path.exists():\n",
        "            temp_auto_path.unlink()\n",
        "\n",
        "        return complete_results\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Enhanced validation failed: {e}\")\n",
        "        if temp_auto_path.exists():\n",
        "            temp_auto_path.unlink()\n",
        "        return {}\n",
        "\n",
        "\n",
        "# Integration function for the main pipeline\n",
        "def run_kpi_extraction_with_validation():\n",
        "    \"\"\"Run KPI extraction with comprehensive validation\"\"\"\n",
        "    print(\"üöÄ Starting KPI extraction with automated validation...\")\n",
        "\n",
        "    # Validate environment\n",
        "    if not validate_environment():\n",
        "        print(\"Please fix the environment issues before running.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Step 1: Run KPI extraction\n",
        "        print(\"\\nüìä Step 1: Extracting KPIs...\")\n",
        "        df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
        "\n",
        "        # Save auto results\n",
        "        save_results(df_auto, EXPORT_AUTO_XLSX, PDF_PATH)\n",
        "        print(f\"‚úÖ Extracted {len(df_auto)} KPIs and saved to {EXPORT_AUTO_XLSX}\")\n",
        "\n",
        "        # Step 2: Run validation if manual file exists\n",
        "        if MANUAL_XLSX and Path(MANUAL_XLSX).exists():\n",
        "            print(f\"\\nüîç Step 2: Running validation against {MANUAL_XLSX}...\")\n",
        "            validation_results = enhanced_compare_with_manual_kpis(\n",
        "                df_auto, MANUAL_XLSX, \"validation_results\"\n",
        "            )\n",
        "\n",
        "            if validation_results:\n",
        "                best_metrics = validation_results['best_metrics']\n",
        "                print(f\"\\nüéØ Validation completed!\")\n",
        "                print(f\"   F1 Score: {best_metrics['f1_score']:.3f}\")\n",
        "                print(f\"   Precision: {best_metrics['precision']:.3f}\")\n",
        "                print(f\"   Recall: {best_metrics['recall']:.3f}\")\n",
        "\n",
        "                return {\n",
        "                    'extracted_kpis': df_auto,\n",
        "                    'validation_results': validation_results\n",
        "                }\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è Validation failed, but extraction completed successfully\")\n",
        "                return {'extracted_kpis': df_auto}\n",
        "        else:\n",
        "            print(f\"\\n‚ö†Ô∏è Manual KPI file not found ({MANUAL_XLSX}), skipping validation\")\n",
        "            return {'extracted_kpis': df_auto}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Pipeline failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "\n",
        "# Batch validation function for multiple documents\n",
        "def run_batch_validation(pdf_list: List[str], manual_list: List[str],\n",
        "                        output_base_dir: str = \"batch_validation\"):\n",
        "    \"\"\"\n",
        "    Run validation across multiple PDF documents\n",
        "\n",
        "    Args:\n",
        "        pdf_list: List of PDF file paths\n",
        "        manual_list: List of corresponding manual annotation files\n",
        "        output_base_dir: Base directory for validation results\n",
        "    \"\"\"\n",
        "    batch_results = []\n",
        "\n",
        "    for i, (pdf_path, manual_path) in enumerate(zip(pdf_list, manual_list)):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing document {i+1}/{len(pdf_list)}: {Path(pdf_path).name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        try:\n",
        "            # Set up paths for this document\n",
        "            doc_name = Path(pdf_path).stem\n",
        "            doc_output_dir = Path(output_base_dir) / doc_name\n",
        "            doc_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            # Extract KPIs\n",
        "            global PDF_PATH\n",
        "            original_pdf_path = PDF_PATH\n",
        "            PDF_PATH = pdf_path\n",
        "\n",
        "            df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
        "            auto_excel_path = doc_output_dir / f\"{doc_name}_auto_kpis.xlsx\"\n",
        "            save_results(df_auto, str(auto_excel_path), PDF_PATH)\n",
        "\n",
        "            # Run validation\n",
        "            validation_results = enhanced_compare_with_manual_kpis(\n",
        "                df_auto, manual_path, str(doc_output_dir / \"validation\")\n",
        "            )\n",
        "\n",
        "            # Store results\n",
        "            doc_result = {\n",
        "                'document': doc_name,\n",
        "                'pdf_path': pdf_path,\n",
        "                'manual_path': manual_path,\n",
        "                'extracted_kpis': len(df_auto),\n",
        "                'validation_results': validation_results.get('best_metrics', {}),\n",
        "                'output_dir': str(doc_output_dir)\n",
        "            }\n",
        "            batch_results.append(doc_result)\n",
        "\n",
        "            # Restore original PDF path\n",
        "            PDF_PATH = original_pdf_path\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to process {doc_name}: {e}\")\n",
        "            batch_results.append({\n",
        "                'document': doc_name,\n",
        "                'pdf_path': pdf_path,\n",
        "                'manual_path': manual_path,\n",
        "                'error': str(e)\n",
        "            })\n",
        "\n",
        "    # Generate batch summary\n",
        "    batch_summary_path = Path(output_base_dir) / \"batch_summary.xlsx\"\n",
        "    batch_df = pd.DataFrame(batch_results)\n",
        "    batch_df.to_excel(batch_summary_path, index=False)\n",
        "\n",
        "    print(f\"\\nüéâ Batch validation completed!\")\n",
        "    print(f\"üìä Processed {len(pdf_list)} documents\")\n",
        "    print(f\"üìÅ Results saved to {output_base_dir}\")\n",
        "    print(f\"üìã Summary available at {batch_summary_path}\")\n",
        "\n",
        "    return batch_results\n",
        "\n",
        "\n",
        "# Quick validation function for testing\n",
        "def quick_validation_test(manual_xlsx: str = None, auto_xlsx: str = None):\n",
        "    \"\"\"Quick validation test with existing files\"\"\"\n",
        "    manual_file = manual_xlsx or MANUAL_XLSX\n",
        "    auto_file = auto_xlsx or EXPORT_AUTO_XLSX\n",
        "\n",
        "    if not Path(manual_file).exists():\n",
        "        print(f\"‚ùå Manual file not found: {manual_file}\")\n",
        "        return None\n",
        "\n",
        "    if not Path(auto_file).exists():\n",
        "        print(f\"‚ùå Auto file not found: {auto_file}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"üîç Quick validation test:\")\n",
        "    print(f\"  Manual: {manual_file}\")\n",
        "    print(f\"  Auto: {auto_file}\")\n",
        "\n",
        "    try:\n",
        "        validator = KPIValidationPipeline(\n",
        "            manual_excel_path=manual_file,\n",
        "            auto_excel_path=auto_file,\n",
        "            output_dir=\"quick_validation\"\n",
        "        )\n",
        "\n",
        "        results = validator.run_full_validation()\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Quick validation failed: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Performance benchmarking function\n",
        "def benchmark_extraction_methods():\n",
        "    \"\"\"Benchmark different extraction methods with validation\"\"\"\n",
        "    methods = {\n",
        "        'text_only': process_text_only,\n",
        "        'with_images': process_sustainability_report_with_enhanced_images,\n",
        "        'optimized': process_sustainability_report_OPTIMIZED\n",
        "    }\n",
        "\n",
        "    benchmark_results = {}\n",
        "\n",
        "    for method_name, method_func in methods.items():\n",
        "        print(f\"\\nüß™ Benchmarking {method_name}...\")\n",
        "\n",
        "        try:\n",
        "            import time\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Run extraction\n",
        "            df_result = method_func(PDF_PATH)\n",
        "            extraction_time = time.time() - start_time\n",
        "\n",
        "            # Save results\n",
        "            method_output = f\"{method_name}_{EXPORT_AUTO_XLSX}\"\n",
        "            save_results(df_result, method_output, PDF_PATH)\n",
        "\n",
        "            # Run validation if manual file exists\n",
        "            validation_metrics = {}\n",
        "            if MANUAL_XLSX and Path(MANUAL_XLSX).exists():\n",
        "                validation_results = enhanced_compare_with_manual_kpis(\n",
        "                    df_result, MANUAL_XLSX, f\"benchmark_{method_name}\"\n",
        "                )\n",
        "                if validation_results:\n",
        "                    validation_metrics = validation_results['best_metrics']\n",
        "\n",
        "            benchmark_results[method_name] = {\n",
        "                'extraction_time': extraction_time,\n",
        "                'kpi_count': len(df_result),\n",
        "                'kpis_per_second': len(df_result) / extraction_time,\n",
        "                'validation_metrics': validation_metrics\n",
        "            }\n",
        "\n",
        "            print(f\"‚úÖ {method_name}: {len(df_result)} KPIs in {extraction_time:.1f}s\")\n",
        "            if validation_metrics:\n",
        "                print(f\"   F1: {validation_metrics.get('f1_score', 0):.3f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå {method_name} failed: {e}\")\n",
        "            benchmark_results[method_name] = {'error': str(e)}\n",
        "\n",
        "    # Save benchmark results\n",
        "    benchmark_df = pd.DataFrame(benchmark_results).T\n",
        "    benchmark_df.to_excel(\"extraction_benchmark.xlsx\")\n",
        "\n",
        "    print(f\"\\nüèÜ Benchmark completed!\")\n",
        "    print(f\"üìä Results saved to extraction_benchmark.xlsx\")\n",
        "\n",
        "    return benchmark_results\n",
        "\n",
        "\n",
        "# Usage examples and documentation\n",
        "def validation_usage_examples():\n",
        "    \"\"\"Show usage examples for the validation pipeline\"\"\"\n",
        "    print(\"\"\"\n",
        "# KPI Validation Pipeline Usage Examples\n",
        "\n",
        "## 1. Basic validation with existing files\n",
        "```python\n",
        "validator = KPIValidationPipeline(\n",
        "    manual_excel_path=\"manual_kpis.xlsx\",\n",
        "    auto_excel_path=\"auto_kpis.xlsx\"\n",
        ")\n",
        "results = validator.run_full_validation()\n",
        "```\n",
        "\n",
        "## 2. Integrated extraction + validation\n",
        "```python\n",
        "results = run_kpi_extraction_with_validation()\n",
        "```\n",
        "\n",
        "## 3. Quick validation test\n",
        "```python\n",
        "results = quick_validation_test(\"manual.xlsx\", \"auto.xlsx\")\n",
        "```\n",
        "\n",
        "## 4. Batch validation for multiple documents\n",
        "```python\n",
        "pdf_files = [\"doc1.pdf\", \"doc2.pdf\", \"doc3.pdf\"]\n",
        "manual_files = [\"manual1.xlsx\", \"manual2.xlsx\", \"manual3.xlsx\"]\n",
        "batch_results = run_batch_validation(pdf_files, manual_files)\n",
        "```\n",
        "\n",
        "## 5. Benchmark different extraction methods\n",
        "```python\n",
        "benchmark_results = benchmark_extraction_methods()\n",
        "```\n",
        "\n",
        "## 6. Custom threshold analysis\n",
        "```python\n",
        "validator = KPIValidationPipeline(\"manual.xlsx\", \"auto.xlsx\")\n",
        "validator.run_comprehensive_evaluation()\n",
        "\n",
        "# Check performance at different thresholds\n",
        "for threshold in [0.5, 0.7, 0.9]:\n",
        "    metrics = validator.calculate_metrics_at_threshold(threshold)\n",
        "    print(f\"Threshold {threshold}: F1={metrics['f1_score']:.3f}\")\n",
        "```\n",
        "\n",
        "## Output Files Generated:\n",
        "- validation_results.json - Complete results in JSON format\n",
        "- detailed_matches.xlsx - All matched KPIs with similarity scores\n",
        "- error_analysis.xlsx - False positives and false negatives\n",
        "- validation_report.md - Human-readable report\n",
        "- threshold_analysis.xlsx - Performance across different thresholds\n",
        "- validation_visualizations.png - Comprehensive charts and graphs\n",
        "\n",
        "## Key Metrics Explained:\n",
        "- **Precision**: % of auto KPIs that match manual annotations\n",
        "- **Recall**: % of manual KPIs found by automatic extraction\n",
        "- **F1 Score**: Harmonic mean of precision and recall\n",
        "- **True Positives**: Correctly identified KPIs\n",
        "- **False Positives**: Auto KPIs not in manual annotations\n",
        "- **False Negatives**: Manual KPIs missed by extraction\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "vv0x-kG2_odA"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‰øÆÊîπ‰∏ªÂáΩÊï∞ÔºåÊòæÁ§∫ÂÆåÊï¥ÁöÑËØ¶ÁªÜ‰ø°ÊÅØ\n",
        "def enhanced_main_with_detailed_output():\n",
        "    \"\"\"Â¢ûÂº∫ÁâàmainÂáΩÊï∞ÔºåÊòæÁ§∫ËØ¶ÁªÜÁöÑÈ™åËØÅËæìÂá∫\"\"\"\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format=\"%(asctime)s - %(levelname)s: %(message)s\",\n",
        "        datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(PDF_PATH):\n",
        "            logging.error(f\"PDF file not found: {PDF_PATH}\")\n",
        "            return\n",
        "\n",
        "        # Step 1: KPIÊèêÂèñ\n",
        "        print(f\"\\nüöÄ Starting KPI extraction from {Path(PDF_PATH).name}...\")\n",
        "        df_auto = process_sustainability_report_with_enhanced_images(PDF_PATH)\n",
        "        save_results(df_auto, EXPORT_AUTO_XLSX, PDF_PATH)\n",
        "        print(f\"‚úÖ KPI extraction completed: {len(df_auto)} KPIs extracted\")\n",
        "\n",
        "        # Step 2: ËØ¶ÁªÜÈ™åËØÅÔºàÂ¶ÇÊûúÊúâmanualÊñá‰ª∂Ôºâ\n",
        "        if MANUAL_XLSX and Path(MANUAL_XLSX).exists():\n",
        "            validation_start_time = time.time()\n",
        "\n",
        "            print(f\"\\nüîç Starting comprehensive validation against {Path(MANUAL_XLSX).name}...\")\n",
        "            complete_validation_results = enhanced_compare_with_manual_kpis(\n",
        "                df_auto, MANUAL_XLSX, \"comprehensive_validation\"\n",
        "            )\n",
        "\n",
        "            validation_time = time.time() - validation_start_time\n",
        "\n",
        "            if complete_validation_results:\n",
        "                # ÊòæÁ§∫ÁªºÂêàÊëòË¶Å\n",
        "                text_metrics = complete_validation_results['text_validation'].get('best_metrics', {})\n",
        "                metadata_scores = complete_validation_results['metadata_validation'].get('overall_scores', {})\n",
        "\n",
        "                print(f\"\\nüéØ Verification completed:\")\n",
        "                print(f\"   Text F1 Score: {text_metrics.get('f1_score', 0):.3f}\")\n",
        "                print(f\"   Text Precision: {text_metrics.get('precision', 0):.3f}\")\n",
        "                print(f\"   Text Recall: {text_metrics.get('recall', 0):.3f}\")\n",
        "                print(f\"   Metadata Overall Score: {metadata_scores.get('overall_metadata_score', 0):.3f}\")\n",
        "                print(f\"‚è±Ô∏è  Validation time: {validation_time:.1f}Áßí\")\n",
        "                print(f\"üìÅ Complete results saved to: comprehensive_validation/\")\n",
        "\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è Validation encountered issues, but extraction completed successfully\")\n",
        "        else:\n",
        "            logging.info(\"Manual KPI file not found, skipping validation.\")\n",
        "\n",
        "        # Step 3: ÊòæÁ§∫ÊèêÂèñÊëòË¶Å\n",
        "        if not df_auto.empty:\n",
        "            print(f\"\\n=== EXTRACTION SUMMARY ===\")\n",
        "            print(f\"Total KPIs extracted: {len(df_auto)}\")\n",
        "\n",
        "            if 'source_type' in df_auto.columns:\n",
        "                source_counts = df_auto['source_type'].value_counts()\n",
        "                print(f\"From text/tables: {source_counts.get('text', 0)}\")\n",
        "                print(f\"From images/charts: {source_counts.get('image', 0)}\")\n",
        "\n",
        "            if 'kpi_theme' in df_auto.columns:\n",
        "                theme_counts = df_auto['kpi_theme'].value_counts()\n",
        "                print(f\"\\nKPI Distribution by Theme:\")\n",
        "                for theme, count in theme_counts.items():\n",
        "                    print(f\"  {theme}: {count}\")\n",
        "        else:\n",
        "            print(\"\\nNo KPIs were extracted from the document.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in enhanced main execution: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "cZqmV6ik8iHV"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ÂàõÂª∫ËØ¶ÁªÜËæìÂá∫ÁöÑÊâπÈáèÂ§ÑÁêÜÂô®\n",
        "def create_batch_processor_with_detailed_output():\n",
        "    \"\"\"ÂàõÂª∫ÊîØÊåÅËØ¶ÁªÜËæìÂá∫ÁöÑÊâπÈáèÂ§ÑÁêÜÂô®\"\"\"\n",
        "    return BatchKPIProcessorWithDetailedOutput()\n",
        "\n",
        "# ‰øÆÊîπÈõÜÊàêÊâßË°åÂáΩÊï∞\n",
        "def integrated_main_execution_detailed():\n",
        "    \"\"\"ÈõÜÊàêÁöÑ‰∏ªÊâßË°åÂáΩÊï∞ - ËØ¶ÁªÜËæìÂá∫ÁâàÊú¨\"\"\"\n",
        "    print(\"üöÄ Enhanced KPI Extraction System - Detailed Output\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"1. Single PDF processing (detailed text + metadata validation)\")\n",
        "    print(\"2. Batch PDF processing (detailed text + metadata validation)\")\n",
        "    print(\"3. Quick batch from directories (detailed output)\")\n",
        "    print(\"4. Quick metadata validation test\")\n",
        "    print(\"5. View usage examples\")\n",
        "\n",
        "    try:\n",
        "        choice = input(\"Please select processing mode (1-5): \")\n",
        "\n",
        "        if choice == '1':\n",
        "            # Âçï‰∏™PDFÂ§ÑÁêÜÔºåËØ¶ÁªÜËæìÂá∫\n",
        "            enhanced_main_with_detailed_output()\n",
        "\n",
        "        elif choice == '2':\n",
        "            # ÊâπÈáèÂ§ÑÁêÜÔºåËØ¶ÁªÜËæìÂá∫\n",
        "            processor = create_batch_processor_with_detailed_output()\n",
        "\n",
        "            # ËÆ©Áî®Êà∑ÊâãÂä®Ê∑ªÂä†Êñá‰ª∂ÂØπ\n",
        "            while True:\n",
        "                pdf_path = input(\"Enter PDF file path (press Enter to finish): \").strip()\n",
        "                if not pdf_path:\n",
        "                    break\n",
        "                manual_path = input(\"Enter corresponding Manual file path: \").strip()\n",
        "                doc_name = input(\"Document name (press Enter for default): \").strip() or None\n",
        "\n",
        "                processor.add_file_pair(pdf_path, manual_path, doc_name)\n",
        "\n",
        "            if processor.file_pairs:\n",
        "                processor.list_file_pairs()\n",
        "                confirm = input(f\"\\nStart detailed processing of these {len(processor.file_pairs)} documents? (y/n): \")\n",
        "                if confirm.lower() == 'y':\n",
        "                    processor.run_batch_processing()\n",
        "                else:\n",
        "                    print(\"Batch processing cancelled.\")\n",
        "            else:\n",
        "                print(\"‚ùå No file pairs added.\")\n",
        "\n",
        "        elif choice == '3':\n",
        "            # Âø´ÈÄüÁõÆÂΩïÊâπÈáèÂ§ÑÁêÜÔºåËØ¶ÁªÜËæìÂá∫\n",
        "            pdf_dir = input(\"PDF files directory path: \").strip()\n",
        "            manual_dir = input(\"Manual files directory path: \").strip()\n",
        "\n",
        "            processor = create_batch_processor_with_detailed_output()\n",
        "            added_count = processor.add_multiple_pairs_from_directory(pdf_dir, manual_dir)\n",
        "\n",
        "            if added_count == 0:\n",
        "                print(\"‚ùå No matching PDF and Manual file pairs found.\")\n",
        "                return None\n",
        "\n",
        "            processor.list_file_pairs()\n",
        "            response = input(f\"\\nStart detailed processing of these {added_count} documents? (y/n): \")\n",
        "            if response.lower() == 'y':\n",
        "                processor.run_batch_processing()\n",
        "            else:\n",
        "                print(\"Batch processing cancelled.\")\n",
        "\n",
        "        elif choice == '4':\n",
        "            # Âø´ÈÄüÂÖÉÊï∞ÊçÆÈ™åËØÅÊµãËØï\n",
        "            quick_test_metadata_validation()\n",
        "\n",
        "        elif choice == '5':\n",
        "            # ÊòæÁ§∫‰ΩøÁî®Á§∫‰æã\n",
        "            metadata_validation_usage_examples()\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid selection, running default single PDF processing\")\n",
        "            enhanced_main_with_detailed_output()\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nUser cancelled operation\")\n",
        "    except Exception as e:\n",
        "        print(f\"Execution error: {e}\")\n",
        "        print(\"Running default single PDF processing\")\n",
        "        enhanced_main_with_detailed_output()\n",
        "\n",
        "# Êõ¥Êñ∞‰∏ªÊâßË°åÈÉ®ÂàÜ\n",
        "if __name__ == \"__main__\":\n",
        "    # ‰ΩøÁî®ËØ¶ÁªÜËæìÂá∫ÁöÑÈõÜÊàêÊâßË°åÂáΩÊï∞\n",
        "    integrated_main_execution_detailed()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbBRhgS1ARfC",
        "outputId": "e31986cf-493e-4f2f-f27b-9056f29c8968"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Enhanced KPI Extraction System - Detailed Output\n",
            "============================================================\n",
            "1. Single PDF processing (detailed text + metadata validation)\n",
            "2. Batch PDF processing (detailed text + metadata validation)\n",
            "3. Quick batch from directories (detailed output)\n",
            "4. Quick metadata validation test\n",
            "5. View usage examples\n",
            "Please select processing mode (1-5): 2\n",
            "Enter PDF file path (press Enter to finish): /content/NewNew.pdf\n",
            "Enter corresponding Manual file path: /content/NewNew.xlsx\n",
            "Document name (press Enter for default): NewNew\n",
            "Enter PDF file path (press Enter to finish): /content/fujit.pdf\n",
            "Enter corresponding Manual file path: /content/fujit.xlsx\n",
            "Document name (press Enter for default): fujit\n",
            "Enter PDF file path (press Enter to finish): /content/Cineplex.pdf\n",
            "Enter corresponding Manual file path: /content/Cineplex.xlsx\n",
            "Document name (press Enter for default): Cineplex\n",
            "Enter PDF file path (press Enter to finish): \n",
            "\n",
            "üìã Â∑≤Ê∑ªÂä†ÁöÑÊñá‰ª∂ÂØπ (ÂÖ± 3 ÂØπ):\n",
            "--------------------------------------------------------------------------------\n",
            " 1. ÊñáÊ°£: NewNew\n",
            "    PDF:    /content/NewNew.pdf\n",
            "    Manual: /content/NewNew.xlsx\n",
            "\n",
            " 2. ÊñáÊ°£: fujit\n",
            "    PDF:    /content/fujit.pdf\n",
            "    Manual: /content/fujit.xlsx\n",
            "\n",
            " 3. ÊñáÊ°£: Cineplex\n",
            "    PDF:    /content/Cineplex.pdf\n",
            "    Manual: /content/Cineplex.xlsx\n",
            "\n",
            "\n",
            "Start detailed processing of these 3 documents? (y/n): y\n",
            "\n",
            "üöÄ ÂºÄÂßãÊâπÈáèÂ§ÑÁêÜ 3 ‰∏™ÊñáÊ°£...\n",
            "üìÅ ÁªìÊûúÂ∞Ü‰øùÂ≠òÂà∞: batch_kpi_results/batch_20250731_110839\n",
            "\n",
            "================================================================================\n",
            "üìÑ Processing Document 1/3: NewNew\n",
            "================================================================================\n",
            "üìä Step 1: Extracting KPIs from NewNew.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Extraction completed: 55 KPIs\n",
            "\n",
            "üîç Step 2: Running comprehensive validation against NewNew.xlsx...\n",
            "\n",
            "üîç Running detailed text validation against NewNew.xlsx...\n",
            "\n",
            "============================================================\n",
            "KPI EXTRACTION VALIDATION SUMMARY\n",
            "============================================================\n",
            "üìä Dataset: 46 manual vs 55 auto KPIs\n",
            "üéØ Best F1 Score: 0.911\n",
            "üìà Precision: 0.836\n",
            "üìâ Recall: 1.000\n",
            "‚úÖ True Positives: 46\n",
            "‚ùå False Positives: 9\n",
            "‚ö†Ô∏è  False Negatives: 0\n",
            "============================================================\n",
            "üìÅ Results saved to: batch_kpi_results/batch_20250731_110839/doc_1_NewNew/validation\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "KPI EXTRACTION VALIDATION SUMMARY\n",
            "============================================================\n",
            "üìä Dataset: 46 manual vs 55 auto KPIs\n",
            "üéØ Best F1 Score: 0.911\n",
            "üìà Precision: 0.836\n",
            "üìâ Recall: 1.000\n",
            "‚úÖ True Positives: 46\n",
            "‚ùå False Positives: 9\n",
            "‚ö†Ô∏è  False Negatives: 0\n",
            "============================================================\n",
            "üìÅ Results saved to: batch_kpi_results/batch_20250731_110839/doc_1_NewNew/validation\n",
            "============================================================\n",
            "\n",
            "üìä Running metadata validation...\n",
            "\n",
            "üìã METADATA VALIDATION SUMMARY\n",
            "============================================================\n",
            "üìä Metadata Overall Score: 0.925\n",
            "üéØ Validated KPI Pairs: 46\n",
            "\n",
            "üìÇ Metadata Category Scores:\n",
            "   Document Fields: 1.000\n",
            "   Classification Fields: 1.000\n",
            "   Quantitative Fields: 0.748\n",
            "   Analysis Fields: 0.986\n",
            "\n",
            "‚ö†Ô∏è  Fields Needing Attention:\n",
            "   üü¢ Absolute Page Number: 1.000 (low priority)\n",
            "   üü¢ kpi_theme: 1.000 (low priority)\n",
            "   üî¥ quantitative_value: 0.696 (high priority)\n",
            "============================================================\n",
            "üìÅ Metadata results saved to: batch_kpi_results/batch_20250731_110839/doc_1_NewNew/validation/metadata_validation\n",
            "============================================================\n",
            "\n",
            "üéØ Document 1 Verification Completed:\n",
            "   üìä Dataset: 46 manual vs 55 auto KPIs\n",
            "   üéØ Text F1 Score: 0.911\n",
            "   üìà Text Precision: 0.836\n",
            "   üìâ Text Recall: 1.000\n",
            "   ‚úÖ True Positives: 46\n",
            "   ‚ùå False Positives: 9\n",
            "   ‚ö†Ô∏è  False Negatives: 0\n",
            "   üìä Metadata Overall Score: 0.925\n",
            "‚è±Ô∏è  Processing time: 522.2Áßí (validation: 127.5Áßí)\n",
            "üìÅ Results saved to: batch_kpi_results/batch_20250731_110839/doc_1_NewNew\n",
            "\n",
            "================================================================================\n",
            "üìÑ Processing Document 2/3: fujit\n",
            "================================================================================\n",
            "üìä Step 1: Extracting KPIs from fujit.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:API response not JSON list: Based on the provided text, here are the extracted KPIs:\n",
            "\n",
            "```json\n",
            "[\n",
            "    {\n",
            "        \"kpi_text\": \"In FY...\n",
            "WARNING:root:API response not JSON list: Based on the provided text, here are the extracted KPIs:\n",
            "\n",
            "```json\n",
            "[]\n",
            "```\n",
            "\n",
            "The text provided does not...\n",
            "WARNING:root:API response not JSON list: Based on the provided text, there are no specific quantifiable KPIs that meet the criteria outlined....\n",
            "WARNING:root:API response not JSON list: The provided text does not contain any specific numbers, percentages, or measurable quantities that ...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It prim...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It prim...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains a table with quantifiable performance data related to environmental management sy...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains a table with quantifiable performance data related to environmental management sy...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a page from the Fujitsu Group Environmental Report 2015, focusing on Environme...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a page from an environmental report, primarily containing text and diagrams re...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It prim...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a page from a report, primarily containing text and a table. Here's the analys...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a text-based document from the Fujitsu Group Environmental Report 2015, focusi...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a text-based document from the Fujitsu Group Environmental Report 2015, focusi...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It show...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It show...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a page from a report, but it does not contain any charts or graphs with quanti...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a page from an environmental report, but it does not contain any charts or gra...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image does not contain any charts or graphs with quantifiable performance data. It appears to be...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a page from a report, but it does not contain any charts or graphs with quanti...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It prim...\n",
            "WARNING:root:Image analysis response not JSON list: I'm unable to analyze the image as it doesn't contain any charts or graphs. If you have a different ...\n",
            "WARNING:root:Image analysis response not JSON list: I'm unable to analyze the image as it doesn't contain any charts or graphs. If you have a different ...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It appe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any visible charts or graphs with quantifiable performance data....\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any visible charts or graphs with quantifiable performance data....\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a page from a report and does not contain any charts or graphs with quantifiab...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It prim...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Extraction completed: 31 KPIs\n",
            "\n",
            "üîç Step 2: Running comprehensive validation against fujit.xlsx...\n",
            "\n",
            "üîç Running detailed text validation against fujit.xlsx...\n",
            "\n",
            "============================================================\n",
            "KPI EXTRACTION VALIDATION SUMMARY\n",
            "============================================================\n",
            "üìä Dataset: 12 manual vs 31 auto KPIs\n",
            "üéØ Best F1 Score: 0.326\n",
            "üìà Precision: 0.226\n",
            "üìâ Recall: 0.583\n",
            "‚úÖ True Positives: 7\n",
            "‚ùå False Positives: 24\n",
            "‚ö†Ô∏è  False Negatives: 5\n",
            "============================================================\n",
            "üìÅ Results saved to: batch_kpi_results/batch_20250731_110839/doc_2_fujit/validation\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "KPI EXTRACTION VALIDATION SUMMARY\n",
            "============================================================\n",
            "üìä Dataset: 12 manual vs 31 auto KPIs\n",
            "üéØ Best F1 Score: 0.326\n",
            "üìà Precision: 0.226\n",
            "üìâ Recall: 0.583\n",
            "‚úÖ True Positives: 7\n",
            "‚ùå False Positives: 24\n",
            "‚ö†Ô∏è  False Negatives: 5\n",
            "============================================================\n",
            "üìÅ Results saved to: batch_kpi_results/batch_20250731_110839/doc_2_fujit/validation\n",
            "============================================================\n",
            "\n",
            "üìä Running metadata validation...\n",
            "\n",
            "üìã METADATA VALIDATION SUMMARY\n",
            "============================================================\n",
            "üìä Metadata Overall Score: 0.945\n",
            "üéØ Validated KPI Pairs: 7\n",
            "\n",
            "üìÇ Metadata Category Scores:\n",
            "   Document Fields: 1.000\n",
            "   Classification Fields: 1.000\n",
            "   Quantitative Fields: 0.807\n",
            "   Analysis Fields: 1.000\n",
            "\n",
            "‚ö†Ô∏è  Fields Needing Attention:\n",
            "   üü¢ Absolute Page Number: 1.000 (low priority)\n",
            "   üü¢ kpi_theme: 1.000 (low priority)\n",
            "   üü¢ quantitative_value: 0.286 (urgent priority)\n",
            "============================================================\n",
            "üìÅ Metadata results saved to: batch_kpi_results/batch_20250731_110839/doc_2_fujit/validation/metadata_validation\n",
            "============================================================\n",
            "\n",
            "üéØ Document 2 Verification Completed:\n",
            "   üìä Dataset: 12 manual vs 31 auto KPIs\n",
            "   üéØ Text F1 Score: 0.326\n",
            "   üìà Text Precision: 0.226\n",
            "   üìâ Text Recall: 0.583\n",
            "   ‚úÖ True Positives: 7\n",
            "   ‚ùå False Positives: 24\n",
            "   ‚ö†Ô∏è  False Negatives: 5\n",
            "   üìä Metadata Overall Score: 0.945\n",
            "‚è±Ô∏è  Processing time: 342.5Áßí (validation: 31.8Áßí)\n",
            "üìÅ Results saved to: batch_kpi_results/batch_20250731_110839/doc_2_fujit\n",
            "\n",
            "================================================================================\n",
            "üìÑ Processing Document 3/3: Cineplex\n",
            "================================================================================\n",
            "üìä Step 1: Extracting KPIs from Cineplex.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:API response not JSON list: The provided text does not contain any specific numbers, percentages, or measurable quantities that ...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a logo and does not contain any charts or graphs with quantifiable performance...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a logo and does not contain any charts or graphs with quantifiable performance...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a text document discussing Cineplex's Corporate Social Responsibility approach...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a text document discussing Cineplex's approach to Corporate Social Responsibil...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a text document and does not contain any charts or graphs with quantifiable pe...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided is a text document and does not contain any charts or graphs. Therefore, there ar...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains text with quantifiable performance data related to inclusivity, diversity, and ac...\n",
            "WARNING:root:Image analysis response not JSON list: The image contains text with quantifiable performance data related to inclusivity, diversity, and cu...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It is a...\n",
            "WARNING:root:Image analysis response not JSON list: The image provided does not contain any charts or graphs with quantifiable performance data. It is a...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Extraction completed: 4 KPIs\n",
            "\n",
            "üîç Step 2: Running comprehensive validation against Cineplex.xlsx...\n",
            "\n",
            "üîç Running detailed text validation against Cineplex.xlsx...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:PDF file not found: /content/Test_Unknown_northwest-sustainability-report-2022_fbqow68f-60-74.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "KPI EXTRACTION VALIDATION SUMMARY\n",
            "============================================================\n",
            "üìä Dataset: 6 manual vs 4 auto KPIs\n",
            "üéØ Best F1 Score: 0.800\n",
            "üìà Precision: 1.000\n",
            "üìâ Recall: 0.667\n",
            "‚úÖ True Positives: 4\n",
            "‚ùå False Positives: 0\n",
            "‚ö†Ô∏è  False Negatives: 2\n",
            "============================================================\n",
            "üìÅ Results saved to: batch_kpi_results/batch_20250731_110839/doc_3_Cineplex/validation\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "KPI EXTRACTION VALIDATION SUMMARY\n",
            "============================================================\n",
            "üìä Dataset: 6 manual vs 4 auto KPIs\n",
            "üéØ Best F1 Score: 0.800\n",
            "üìà Precision: 1.000\n",
            "üìâ Recall: 0.667\n",
            "‚úÖ True Positives: 4\n",
            "‚ùå False Positives: 0\n",
            "‚ö†Ô∏è  False Negatives: 2\n",
            "============================================================\n",
            "üìÅ Results saved to: batch_kpi_results/batch_20250731_110839/doc_3_Cineplex/validation\n",
            "============================================================\n",
            "\n",
            "üìä Running metadata validation...\n",
            "\n",
            "üìã METADATA VALIDATION SUMMARY\n",
            "============================================================\n",
            "üìä Metadata Overall Score: 0.835\n",
            "üéØ Validated KPI Pairs: 4\n",
            "\n",
            "üìÇ Metadata Category Scores:\n",
            "   Document Fields: 1.000\n",
            "   Classification Fields: 0.430\n",
            "   Quantitative Fields: 0.914\n",
            "   Analysis Fields: 1.000\n",
            "\n",
            "‚ö†Ô∏è  Fields Needing Attention:\n",
            "   üü¢ Absolute Page Number: 1.000 (low priority)\n",
            "   üü° kpi_theme: 0.500 (medium priority)\n",
            "   üü¢ quantitative_value: 1.000 (low priority)\n",
            "============================================================\n",
            "üìÅ Metadata results saved to: batch_kpi_results/batch_20250731_110839/doc_3_Cineplex/validation/metadata_validation\n",
            "============================================================\n",
            "\n",
            "üéØ Document 3 Verification Completed:\n",
            "   üìä Dataset: 6 manual vs 4 auto KPIs\n",
            "   üéØ Text F1 Score: 0.800\n",
            "   üìà Text Precision: 1.000\n",
            "   üìâ Text Recall: 0.667\n",
            "   ‚úÖ True Positives: 4\n",
            "   ‚ùå False Positives: 0\n",
            "   ‚ö†Ô∏è  False Negatives: 2\n",
            "   üìä Metadata Overall Score: 0.835\n",
            "‚è±Ô∏è  Processing time: 110.8Áßí (validation: 5.8Áßí)\n",
            "üìÅ Results saved to: batch_kpi_results/batch_20250731_110839/doc_3_Cineplex\n",
            "Execution error: \"['validation_f1_score', 'validation_precision', 'validation_recall'] not in index\"\n",
            "Running default single PDF processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oe8TGqRNEk_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ibmTPOJu5b5r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1IoxrHG3joQz9CaGgenAnsjP3sGaEFA9h",
      "authorship_tag": "ABX9TyMe8E15UOSL0zCA5sln0oC1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}